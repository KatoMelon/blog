[{"content":"Hugo 的 LaTex 公式渲染有点毛病，这个页面上的显示效果可能存在问题。\n如果你需要这篇笔记的 PDF 版本，可 点击这里 下载。\n前言【Week1-1】 机器学习（统计学习）的定义 机器学习或统计学习通常被定义为： Machine or statistical learning is usually defined as:\n从原始数据中提取模式以获取知识的能力。 The ability to acquire knowledge by extracting patterns from raw data. (Goodfellow, Bengio, Courville)\n用于建模和理解复杂数据集的一组工具。 A set of tools for modeling and understanding complex datasets. (James, Witten, Hastie, Tibshirani)\n数据的定义 数据是观察或测量的具体化。 Data is the materialisation of an observation or a measurement.\n数据集是按照一组预定义属性描述的项集合格式化的数据。 Datasets are data formatted as collections of items described by a set of pre-defined attributes.\n在机器学习中，我们的数据总是以数据集的形式表示。 In machine learning, our data is always represented as a dataset.\n知识的表示 知识可以表示为 Knowledge can be represented as\n命题（陈述、定律） Proposition (statement, law)\n叙述（描述、故事） Narrative (description, story)\n模型（数学或计算机） Model (mathematical or computer)\n模型描述了属性之间的关系。\nModels describe relationships between attributes.\n数学模型和计算机模型是等价的：\nMathematical and computer models are equivalent:\n数学模型可以实现为计算机程序。\nMathematical models can be implemented as computer programs.\n每个计算机模型都有对应的数学表达式。\nEvery computer model has a corresponding mathematical expression.\n数据科学 科学不在于使用复杂的仪器、数学或技术，而在于评估我们的知识。 Science is not about using sophisticated instrumentation, maths or techniques, science is about evaluating our knowledge.\n我们在此评估中使用数据与公认的知识。 We use data together with accepted knowledge in this evaluation.\n命题本身并无科学或不科学之分，关键在于我们如何评估它们。 Propositions are not scientific or unscientific, but the way we evaluate them.\n如果没有数据，就没有机器学习。 If there is no data, there is no machine learning.\n这并不意味着机器学习仅仅是关于数据的。 This doesn\u0026rsquo;t mean machine learning is all about data.\n具体来说，不存在中立或客观的数据来揭示真相。 Specifically, there is no such thing as neutral or objective data that speaks the truth.\n我们需要遵循严格的方法论。 We need to follow a rigorous methodology.\n机器学习的两种观点 许多机器学习从业者采用数据集优先的观点：我们从数据集开始，然后（制定问题并最终）生成模型。 Many machine learning professionals adopt a dataset-first view: we start with a dataset, then (formulate a problem and finally) produce a model.\n在CBU5201中，我们采用部署优先（问题优先）的方法：我们从问题开始，然后获取数据集，最后生成模型。 In CBU5201 we use a deployment-first (problem-first) approach: we start with a problem, then secure a dataset and finally produce a model.\n因此，我们将机器学习定义为：一套使用数据用于解决科学、工程和商业问题的工具和方法论\u0026hellip; Accordingly, we define machine learning as: A set of tools together with a methodology for solving scientific, engineering and business problems using data\n机器学习的两个阶段 模型可以被构建、销售和部署以提供价值。\nModels can be built, sold, and deployed to deliver value.\n在模型的生命周期中，我们可以区分两个阶段：\nDuring the life of a model, we can distinguish two stages:\n学习阶段：模型被构建。\nLearning stage: The model is built.\n部署阶段：模型被使用。\nDeployment stage: The model is used.\n基本方法论 在机器学习中，我们关注的是找到在部署时有效的模型。因此，除了构建模型外，我们还需要验证其有效性。\nIn machine learning, we are interested in finding models that work during deployment. Hence, in addition to building a model, we need to check it works.\n基本的机器学习方法论包括两个独立的任务：\nBasic machine learning methodologies include two separate tasks:\n训练：使用数据和质量指标创建模型。我们也可以说我们将模型拟合到数据集。\nTraining: A model is created using data and a quality metric. We also say that we fit a model to a dataset.\n测试：使用新的、未见过的数据评估模型在部署时的性能。\nTesting: The performance of the model during deployment is assessed using new, unseen data.\n如果没有严格的方法论，模型很可能几乎没有用处。\nWithout rigorous methodologies, models are very likely to be of little use.\n机器学习的分类 监督学习 Supervised Learning 在监督学习中，我们给定一个新项目，其中一个属性的值对我们来说是未知的。\nIn supervised learning, we are given a new item such that the value of one of its attributes is unknown to us.\n我们的目标是通过从已知项目的集合中学习来估计缺失的值。\nOur goal is to estimate the missing value by learning from a collection of known items.\n挑战在于构建一个模型，该模型将一个属性x（称为预测变量）映射到另一个属性y（我们称之为标签），使用一个带标签的数据集。\nThe challenge is then to build a model that maps one attribute x, known as the predictor, to another attribute y, which we call the label, using a dataset of labelled examples.\n监督学习根据标签的类型进一步分为两类： Supervised learning is further divided into two categories depending on the type of label:\n分类：标签是离散变量。 Classification: The label is a discrete variable. 回归：标签是连续变量。 Regression: The label is a continuous variable. 无监督学习 Unsupervised Learning 在无监督学习中，我们致力于发现数据集的潜在结构。\nIn unsupervised learning, we set out to find the underlying structure of our dataset.\n这有助于我们获得理解、识别异常、压缩数据并减少处理时间。\nThis can be useful to gain understanding, identify anomalies, compress our data and reduce processing time.\n数据集的基础结构可以通过结构分析来研究，其中包括： The underlying structure of a dataset can be studied using structure analysis, which includes:\n聚类分析：专注于数据点的分组。 Cluster analysis: Focuses on groups of data points. 成分分析：识别感兴趣的方向。 Component analysis: Identifies directions of interest. 密度估计技术提供了描述样本在属性空间中分布的统计模型。 Density estimation techniques provide statistical models that describe the distribution of samples in the attribute space. 无监督学习的一些应用实例：\n客户细分。 Customer segmentation.\n社交社区检测。 Social community detection.\n推荐系统。 Recommendation systems.\n进化分析。 Evolutionary analysis.\n回归 Regression I 【Week 1-2】 问题形成 回归是一个监督学习问题：我们的目标是使用剩余属性（预测变量）来预测一个属性（标签）的值。 Regression is a supervised problem: Our goal is to predict the value of one attribute (label) using the remaining attributes (predictors).\n标签是一个连续变量。 The label is a continuous variable.\n我们的任务是找到最佳模型，该模型可以为给定的预测变量集分配唯一的标签。 Our job is then to find the best model that assigns a unique label to a given set of predictors.\n我们使用由标记样本组成的数据集。 We use datasets consisting of labelled samples.\n关联与因果 预测模型有时被通过因果视角解释：预测因子是原因，标签是其结果。然而这是不正确的。 Prediction models are sometimes interpreted through a causal lens: the predictor is the cause, the label its effect. However this is not correct.\n我们构建预测器的能力源于属性之间的关联，而非因果关系。 Our ability to build predictors is due to association between attributes, rather than causation.\n数据集中的两个属性出现关联的原因可能是： Two attributes in a dataset appear associated due to:\n一个属性导致另一个属性（直接或间接）。 If one causes the other (directly or indirectly).\n两者有共同的原因。 When both have a common cause.\n由于我们收集样本的方式（抽样）。 Due to the way we collect samples (sampling).\n关键信息：在机器学习中，我们不构建因果模型！ Take-home message: In machine learning we don\u0026rsquo;t build causal models!\n模型质量指标 To find the best model, we need a notion of model quality.\n为了找到最佳模型，我们需要一个模型质量的概念。\nThe squared error $ e_i^2 = (y_i - \\hat{y}{=tex}_i)^2 $ is a common quantity used in regression to encapsulate the notion of single prediction quality.\n平方误差 $ e_i^2 = (y_i - \\hat{y}{=tex}_i)^2 $ 是回归中常用的量，用于概括单个预测质量的概念。\nTwo quality metrics based on the squared error are the sum of squared errors (SSE) and the mean squared error (MSE), which can be computed using a dataset as:\n基于平方误差的两个质量指标是误差平方和（SSE）和均方误差（MSE），可以使用数据集计算如下：\n$E_{SSE} = e_1^2 + e_2^2 + \\cdots + e_N^2 = \\sum_{i=1}^{N}e_i^2$\n$E_{MSE} = \\frac{1}{N} \\sum_{i=1}^{N}e_i^2$\n误差是自然存在的 在回归问题中，我们需要注意：\nWhen considering a regression problem, we need to be aware:\n选择的预测变量可能不包括所有决定标签的因素。\nThe chosen predictors might not include all factors that determine the label.\n选择的模型可能无法表示响应与预测变量之间的真实关系（模式）。\nThe chosen model might not be able to represent the true relationship between response and predictor (the pattern).\n随机机制（噪音）可能存在。\nRandom mechanisms (noise) might be present.\n在数学上，我们将这种差异表示为：\nMathematically, we represent this discrepancy as:\n$y = \\hat{y} + e = f(x) + e$\n真实标签 y 与模型预测 f (x) 之间总会存在一些误差（误差 e）。\nThere will always be some discrepancy (error e) between the true label y and our model prediction f(x).\n将回归视作优化问题 即：找出最小MSE的模型。\n$f_{best}(x)=\\arg\\min_f\\frac{1}{N}\\sum_{i=1}^N\\left(y_i-f(x_i)\\right)^2$\n回归学习阶段模型 **先验知识：**线性、多项式等类型的模型\nType of model (linear, polynomial, etc).\n**数据：**标记样本（特征和真值标签）\nLabelled samples (predictors and true label).\n**模型：**基于特征预测标签\nPredicts a label based on the predictors.\n简单线性回归 简单线性回归模型通过以下数学表达式定义： In simple linear regression, models are defined by the mathematical expression:\n$f(x)=w_0+wx_1$\n因此，预测标签 $\\hat{y}$ 可以表示为 Hence, the predicted label $\\hat{y}$ can be expressed as\n$\\hat{y_i}f(x_i)=w_0+w_1x_i$\n因此，一个线性模型有两个参数wo（截距）和w1（斜率），需要调整这些参数以达到最高质量。 A linear model has therefore two parameters wo (intercept) and w1 (gradient), which need to be tuned to achieve the highest quality.\n在机器学习中，我们用数据集来调整这些参数。 In machine learning, we use a dataset to tune the parameters.\n我们说我们在训练模型或将模型拟合到训练数据集上。 We say that we train the model or fit the model to the training dataset.\n简单多项式回归 多项式回归模型的一般形式是： The general form of a polynomial regression model is:\n$f(x_i) = w_0 + w_1x_i + w_2x_i^2 + \\cdots + w_Dx_i^D$\n这里，$D$ 是多项式的阶数。 where $D$ is the degree of the polynomial.\n多项式回归定义了一组包含多种模型的族。 Polynomial regression defines a family of families of models.\n对于每个 $D$ 值，我们都有一个不同的族：$D=1$ 对应于线性的族，$D=2$ 对应于二次的，$D=3$ 对应于三次的，以此类推。 For each value of $D$, we have a different family: $D=1$ corresponds to the linear family, $D=2$ to the quadratic, $D=3$ to the cubic, and so on.\n我们称 $D$ 为一个超参数。 We call $D$ a hyperparameter.\n这意味着其设定的值会产生一个不同的族，具有不同的参数合集。 What it means is that setting its value results in a different family, with a different collection of parameters.\n回归 Regression II 【Week 1-3】 多元回归 Multiple Regression 多元回归拥有两个或更多的预测变量。\n数学表示为：\n$\\boldsymbol{x}i=[1,x{i,1},x_{i,2},\\ldots,x_{i,K}]^T$\n$\\hat{y_i}=f(x_i)$\n多元线性回归 $f(x_i)=w^Tx_i=w_0+w_1x_{i,1}+\\cdots+w_Kx_{i,K}$\nwhere $w = [w_0, w_1,\u0026hellip;,w_K]^T$ is the model\u0026rsquo;s parameter vector.\nNote that we can use the same vector notation for simple linear regression models, by defining $\\boldsymbol{w} = [w_0, w_1]^T$ and $\\boldsymbol{x}_i = [1,x_i]^T.$\n训练数据集可以表示为设计矩阵X： In multiple linear regression, the training dataset can be represented by the design matrix X:\n$$\\begin{gathered}\\mathbf{X}\\quad=\\quad[\\boldsymbol{x}1,\\ldots,\\boldsymbol{x}N]^T=\\begin{bmatrix}\\boldsymbol{x}1^T\\\\boldsymbol{x}2^T\\\\vdots\\\\boldsymbol{x}N^T\\end{bmatrix}=\\begin{bmatrix}1\u0026amp;x{1,1}\u0026amp;x{1,2}\u0026amp;\\ldots\u0026amp;x{1,K}\\1\u0026amp;x{2,1}\u0026amp;x{2,2}\u0026amp;\\ldots\u0026amp;x_{2,K}\\\\vdots\u0026amp;\\vdots\u0026amp;\\vdots\u0026amp;\\ddots\u0026amp;\\vdots\\1\u0026amp;x_{N,1}\u0026amp;x_{N,2}\u0026amp;\\ldots\u0026amp;x_{N,K}\\end{bmatrix}\\end{gathered}$$\n和标签矩阵y and the label vector y:\n$$\\boldsymbol{y}\\quad=\\quad[y_1,\\ldots,y_N]^T=\\begin{bmatrix}y_1\\y_2\\\\vdots\\y_N\\end{bmatrix}$$\n最小二乘解 对训练数据集，线性模型的最小MSE的解析解有如下表示：\n$\\boldsymbol{w}_{best}=\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\mathbf{X}^{T}y$\n$(\\mathbf{X}^T\\mathbf{X})^{-1}$ 在X中的所有列线性独立时存在。\n其他回归模型 线性模型和多项式模型并不是唯一可用的选项。 Linear and polynomial models are not the only options available.\n其他可以使用的模型家族包括： Other families of models that can be used include:\n指数模型 Exponential\n正弦模型 Sinusoids\n径向基函数 Radial basis functions\n样条模型 Splines\n数学公式是相同的，只有f(⋅)的表达式发生变化。 The mathematical formulation is identical and only the expression for f(⋅) changes.\n逻辑函数 在某些问题中，标签代表一个比例或概率，即一个介于0和1之间的量。\nIn some problems, the label represents a proportion or a probability, i.e. a quantity between 0 and 1.\n此外，这个量可能会随着预测变量的增加而增加。\nMoreover, this quantity might increase as the predictor increases.\n在这种情况下，逻辑函数可能会很有用。\nIn such cases, the logistic function can be useful.\n$p(d)=\\frac{e^d}{1+e^d}=\\frac{1}{1+e^{-d}}$\n其他质量指标 均方根RMSE【表征预测值误差的标准差】：$E_{RMSE}=\\sqrt{\\frac{1}{N}\\sum e_i^2}$\nMAE【表征预测值的误差的绝对值平均】：$E_{MAE}=\\frac{1}{N}\\sum|e_i|$\nR方【测量响应中可从预测变量预测的方差比例】：$E_R=1-\\frac{\\sum e_i^2}{\\sum(y_i-\\bar{y})^2}\\mathrm{,where~}\\bar{y}=\\frac{1}{N}\\sum y_i$\n灵活性 Flexibility 模型允许我们通过调整其参数生成多种形状。 Models allow us to generate multiple shapes by tuning their parameters.\n我们通过模型的自由度或复杂度来描述其生成不同形状的能力，即其灵活性。 We talk about the degrees of freedom or the complexity of a model to describe its ability to generate different shapes, i.e. its flexibility.\n模型的灵活性与可解释性和准确性相关，并且在两者之间存在权衡。 The flexibility of a model is related to its interpretability and accuracy, and there is a trade-off between the two.\n模型的训练数据集质量与其灵活性相关。 The quality of a model on a training dataset is also related to its flexibility.\n在训练过程中，灵活性较高的模型产生的误差通常较低。 During training, the error produced by flexible models is in general lower.\n可解释性 interpretability 模型的可解释性对于我们人类以定性的方式理解预测器如何映射到标签至关重要。 Model interpretability is crucial for us, as humans, to understand in a qualitative manner how a predictor is mapped to a label.\n不灵活的模型通常产生更简单且更容易解释的解决方案。 Inflexible models produce solutions that are usually simpler and easier to interpret.\n泛化 Generalisation 泛化能力是指我们的模型能够成功地将学习阶段所掌握的知识应用到实际部署中的能力。 Generalisation is the ability of our model to successfully translate what we was learnt during the learning stage to deployment.\n欠拟合与过拟合 通过比较模型在训练和部署期间的表现，我们可以观察到三种不同的行为： By comparing the performance of models during training and deployment, we can observe three different behaviours:\n欠拟合：训练和部署时产生较大的误差。模型无法捕捉到潜在的模式。过于简单的模型会导致欠拟合。 Underfitting: Large training and deployment errors are produced. The model is unable to capture the underlying pattern. Rigid models lead to underfitting. 过拟合：训练时产生较小的误差，部署时产生较大的误差。模型在记忆无关的细节。过于复杂的模型和不足的数据会导致过拟合。 Overfitting: Small errors are produced during training, large errors during deployment. The model is memorising irrelevant details. Too complex models and not enough data lead to overfitting. 恰到好处：训练和部署时产生较低的误差。模型能够重现潜在的模式并忽略无关的细节。 Just right: Low training and deployment errors. The model is capable of reproducing the underlying pattern and ignores irrelevant details. 泛化能力只能通过比较训练和部署性能来评估，而不仅仅是看每个模型对训练数据的拟合程度。| Generalisation can only be assessed by comparing training and deployment performance, not by just looking at how each model fits the training data.\n方法论 【Week 1-4】 从目标采样数据集 数据集需要具有代表性，即提供目标群体的完整画面。\nDatasets are representative, i.e., provide a complete picture of the target population.\\ 采样需模拟部署期间生成样本的机制：样本需要独立提取。\nSampling mimics the mechanism that generates samples during deployment: Samples need to be extracted independently. 样本需要独立且同分布（iid）。\nSamples need to be independent and identically distributed (iid).\n评估部署时性能 每个机器学习项目都需要包含一个在部署期间评估模型性能的策略。\nEvery machine learning project needs to include a strategy to evaluate the performance of a model during deployment.\n在机器学习中，性能评估策略包括：\nIn machine learning, a performance evaluation strategy includes:\n用于量化性能的质量指标。\\\nA quality metric used to quantify the performance.\n如何利用数据来评估模型的性能。\\\nHow data will be used to assess the performance of a model.\n性能评估策略必须在创建模型之前设计，以避免陷入数据陷阱，例如确认偏差。\nThe performance evaluation strategy has to be designed before creating a model to avoid falling into our own data-traps, such as confirmation bias.\n我们使用数据的子集，即测试数据集，来计算测试部署性能，作为真实性能的估计。 We use a subset of data, the test dataset, to compute the test deployment performance as an estimation of the true performance.\n测试数据集是随机抽取的，因此测试性能本身也是随机的，因为不同的数据集通常会产生不同的值。 Test datasets are extracted randomly. Hence, the test performance is itself random, as different datasets generally produce different values.\n模型由不同团队构建，可以根据它们的测试性能进行比较。 Models built by different teams can be compared based on their test performances.\n需要注意的是，测试性能是一个随机量，因此某些模型可能偶然表现得更好！ Caution should be used, as the test performance is a random quantity, hence some models might appear to be superior by chance!\n优化理论 优化使我们能够在所有候选模型中识别出在目标群体上实现最高质量的模型，即最优模型 Optimization allows us to identify among all the candidate models the one that achieves the highest quality on the target population, i.e. the optimal model.\n误差曲面 误差曲面（也称为误差、目标、损失或成本函数）用E(w)表示，它将每个候选模型w映射到其误差。 The error surface (a.k.a. error, objective, loss or cost function) denoted by E(w) maps each candidate model w to its error.\n我们假设可以通过目标群体的理想描述来获得它。 We will assume that we can obtain it using the ideal description of our target population.\n最优模型可以通过最低误差来识别。\nThe optimal model can be identified as the one with the lowest error.\n误差表面的梯度（斜率）在最优模型处为零。\nThe gradient (slope) of the error surface is zero at the optimal model.\n因此，我们可以通过找到梯度为零的位置来寻找最优模型。\nHence, we can look for it by identifying where the gradient is zero.\n梯度下降 Gradient descent 梯度下降是一种数值优化方法，通过迭代更新模型参数，利用误差曲面的梯度进行调整。 Gradient descent is a numerical optimization method where we iteratively update our model parameters using the gradient of the error surface.\n梯度提供了误差增加最快的方向。利用梯度，我们可以创建以下更新规则： The gradient provides the direction along which the error increases the most. Using the gradient, we can create the following update rule:\n$w_\\mathrm{new}=w_\\mathrm{old}-\\epsilon\\nabla E(w_{old})$\n其中$\\epsilon$被称为学习率或步长。 where $\\epsilon$ is known as the learning rate or step size.\n在每次迭代中，我们调整模型的参数w。因此，这个过程也被称为参数调优。 With every iteration, we adjust the parameters w of our model. This is why this process is also known as parameter tuning.\n学习率 学习率$\\epsilon$控制我们在每次梯度下降迭代中改变模型参数w的程度。\nThe learning rate $\\epsilon$ controls how much we change the parameters w of our model in each iteration of gradient descent.\n较小的$\\epsilon$值会导致模型收敛到最优解的速度较慢。\nSmall values of $\\epsilon$ result in slow convergence to the optimal model.\\ 较大的$\\epsilon$值可能会使模型错过最优解。\nLarge values of $\\epsilon$ risk overshooting the optimal model. 可以采用自适应方法，使学习率逐渐减小。\nAdaptive approaches can be implemented, where the value of the learning rate decreases progressively.\n梯度下降开始与停止 梯度下降开始时，我们需要一个初始模型。初始模型的选择可能至关重要。初始参数 w 通常随机选择（但在合理的值范围内）。\nTo start gradient descent, we need an initial model. The choice of the initial model can be crucial. The initial parameters w are usually chosen randomly (but within a sensible range of values).\n一般来说，梯度下降不会达到最优模型，因此需要设计一个停止策略。常见的选择包括：\nIn general, gradient descent will not reach the optimal model, hence it is necessary to design a stopping strategy. Common choices include:\n迭代次数。\nNumber of iterations.\\\n处理时间。\nProcessing time.\n误差值。\nError value.\n误差值的相对变化。\nRelative change of the error value.\n局部解与全局解 误差表面可以是复杂的，并且具有 Error surfaces can be complex and have\n局部最优（在某个区域内误差最小的模型）。 local optima (the model with the lowest error within a region). 全局最优（在所有模型中误差最小的模型）。 Global optima (the model with the lowest error among all the models). 梯度下降可能会陷入局部最优。为了避免这种情况，我们可以从多个初始模型开始重复该过程，并选择最佳结果。 Gradient descent can get stuck in local optima. To avoid them, we can repeat the procedure from several initial models and select the best.\n训练ML模型 现在问题是，误差曲面不是给定的，我们只有数据。\n我们使用数据的一个子集，即训练数据集，来（隐式或显式）重建优化过程中所需的误差表面。我们将其称为经验误差表面。 We use a subset of data, known as the training dataset, to (implicitly or explicitly) reconstruct the error surface needed during optimisation. We will call this the empirical error surface.\n经验误差和真实误差表面通常不同。\nThe empirical and true error surfaces are in general different.\n因此，它们的最优模型可能不同，即训练数据集上的最佳模型可能不是总体上的最佳模型。\nHence, their optimal models might differ, i.e. the best model for the training dataset might not be the best for the population.\n训练集与最小均方 可以在训练集上使用最小均方作为指标来优化模型。\n$\\hat{y}=\\mathrm{X}w$\n得到均方函数\n$\\begin{aligned}E_{MSE}(w)\u0026amp;=\\quad\\frac{1}{N}\\left(y-\\hat{y}\\right)^T\\left(y-\\hat{y}\\right)\\\u0026amp;=\\quad\\frac{1}{N}\\left(y-\\mathrm{X}w\\right)^T\\left(y-\\mathrm{X}w\\right)\\end{aligned}$\n得到均方函数的梯度：\n$\\nabla E_{MSE}(w)=\\frac{-2}{N}\\mathrm{X}^T\\left(y-\\mathrm{X}w\\right)$\n此梯度在$w=\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^Ty.$时为零。\n暴力穷举 通常，我们无法获得解析解。\nIn general, we will not have analytical solutions.\n我们可以通过在训练数据上评估每个模型来重建经验误差表面。\nWe can reconstruct the empirical error surface by evaluating each model on training data.\n这被称为暴力搜索或穷举搜索。\nThis is called brute-force or exhaustive search.\n方法简单，但通常不实用。\nSimple, but often impractical.\n由数据驱动的梯度下降 梯度下降可以通过使用训练数据集来估计梯度来实现。\nGradient descent can be implemented by estimating the gradient using our training dataset.\n在每次迭代中，使用训练数据集的一个子集（批次）来计算误差表面的梯度。\nDuring each iteration, a subset (batch) of the training dataset is used to compute the gradient of the error surface.\n根据每次迭代中使用的数据量，通常（尽管并不十分有用）可以区分以下几种方法： Depending on the amount of data used in each iteration, it is common (although not really useful) to distinguish between:\n批量梯度下降（使用整个训练数据集）。 Batch gradient descent (the whole training dataset is used).\n随机（或在线）梯度下降（使用一个样本）。 Stochastic (or online) gradient descent (one sample is used).\n小批量梯度下降（使用训练数据集中的一小部分）。 Mini-batch gradient descent (a small subset from the training dataset is used).\n更实用的方法是讨论批量大小（一个介于1和训练数据集大小之间的数字）。无论批量大小的值如何，我们都会使用随机梯度下降这个术语来指代这种方法。 It is more useful to talk about the batch size (a number between 1 and the size of the training dataset). Irrespective of the value of the batch size, we will use the term stochastic gradient descent for this approach.\n小批量会产生经验误差表面梯度的噪声版本，这有助于逃离局部最小值。 Small batches produce noisy versions of the gradient of the empirical error surface, which can help to escape local minima.\n其他基于梯度的优化方法 随机梯度下降是最常用的优化算法，尽管有时可能较慢。\nStochastic gradient descent is the most used optimisation algorithm, although it can sometimes be slow.\n其他流行的基于梯度的优化算法包括：\nOther popular gradient-based optimisation algorithms include:\n动量法定义了更新步骤的速度（方向和大小），这取决于过去的梯度。\nMomentum defines a velocity (direction and speed) for the update step, which depends on past gradients.\\ RMSProp通过使用过去的梯度来缩放学习率。\nRMSProp adapts the learning rate by scaling them using the past gradients.\\ Adam结合了动量法和RMSProp的一些特性。\nAdam combines some features from the Momentum and RMSProp approaches. 经验误差曲面和过拟合 经验误差曲面和真实误差曲面通常是不同的。\nThe empirical and true error surfaces are in general different.\n当使用小数据集和复杂模型时，两者之间的差异可能非常大，导致训练出的模型在经验误差曲面上表现很好，但在真实误差曲面上表现很差。\nWhen small datasets and complex models are used, the differences between the two can be very large, resulting in trained models that work very well for the empirical error surface but very poorly for the true error surface.\n这当然是从另一个角度来看过拟合问题。\nThis is, of course, another way of looking at overfitting.\n通过增加训练数据集的规模，经验误差曲面会接近真实误差曲面，从而降低过拟合的风险。\nBy increasing the size of the training dataset, empirical error surfaces become closer to the true error surface and the risk of overfitting decreases.\n切勿使用相同的数据进行测试和训练模型。\nNever use the same data for testing and training a model.\n测试数据集需要保持不可访问，以避免在训练过程中（有意或无意地）使用它。\nThe test dataset needs to remain inaccessible to avoid using it (inadvertently or not) during training.\n正则化 Regularisation 正则化通过添加一个约束模型参数取值的项来修改经验误差表面。 Regularisation modifies the empirical error surface by adding a term that constrains the values that the model parameters can take on.\n$E_R(w)=E(w)+\\lambda w^Tw$\n例如，MSE的正则化：\n$E_{MSE+R}=\\frac{1}{N}\\sum_{i=1}^Ne_i^2+\\lambda\\sum_{i=1}^Kw_k^2$\n解为：$w=\\left(\\mathbf{X}^T\\mathbf{X}+N\\lambda\\mathbf{I}\\right)^{-1}\\mathbf{X}^Ty$\n随着正则化强度的增加，所得解的复杂度降低，过拟合的风险也随之减小。 As the regularization strength increases, the complexity of the resulting solution decreases, and so does the risk of overfitting.\n（训练期间）成本与质量 正则化提供了一个例子，我们在训练期间使用了一种质量概念（EMSE+R），而在部署期间使用了不同的质量概念（EMSE）。这听起来是不是有点奇怪？ Regularisation provides an example where we use a notion of quality during training (EMSE+R) that is different from the notion of quality during deployment (EMSE). Doesn\u0026rsquo;t it sound strange?\n我们的目标始终是生成一个在部署期间达到最高质量的模型。我们如何实现它，则是另一个问题。 Our goal is always to produce a model that achieves the highest quality during deployment. How we achieve it, is a different question.\n诸如过拟合等因素可能导致模型在训练期间表现最优，但在部署期间却并非如此。一个设计良好的训练期间质量概念可以生成在部署期间表现更好的模型。 Factors such as overfitting might result in models that are optimal during training, but not deployment. A well-designed notion of quality during training can produce models that perform better during deployment.\n我们通常将训练期间的质量概念称为成本或目标函数，以区别于目标质量指标。 We usually call our notion of quality during training cost or objective function, to distinguish it from the target quality metric.\n验证模型 验证方法允许我们使用数据来评估和选择不同的模型家族。用于验证的相同数据随后可以用于训练最终模型。\nValidation methods allow us to use data for assessing and selecting different families of models. The same data used for validation can then be used to train a final model.\n验证涉及每个模型族的一轮或多轮训练和性能估计，随后进行性能平均。\nValidation involves one or more training and performance estimation rounds per model family followed by performance averaging.\n验证集方法是最简单的方法。它将可用数据集随机分为训练集和验证集（或保留集）。 The validation set approach is the simplest method. It randomly splits the available dataset into a training and a validation (or hold-out) dataset.\n模型使用训练部分进行拟合，验证部分用于估计其性能。 Models are fitted with the training part and the validation part is used to estimate its performance.\n验证方法（数据集切分方法） 验证集方法涉及一次训练轮次。然而，模型使用较少的样本进行训练，最终性能由于随机分割而具有高度可变性。\nThe validation set approach involves one training round. Models are however trained with fewer samples and the final performance is highly variable due to random splitting.\n**留一法交叉验证（LOOCV，Leave-one-out cross-validation）**需要与数据集中的样本数量相同的训练轮次，然而在每一轮中几乎所有的样本都用于训练。它始终提供相同的性能估计。\nLOOCV requires as many training rounds as samples there are in the dataset, however in every round almost all the samples are used for training. It always provides the same performance estimation.\nk折交叉验证是最流行的方法（将数据集切分成K份）。它比留一法交叉验证涉及更少的训练轮次。与验证集方法相比，性能估计的变异性更小，并且更多的样本用于训练。\nk-fold is the most popular approach. It involves fewer training rounds than LOOCV. Compared to the validation set approach, the performance estimation is less variable and more samples are used for training. 分类 Classi\fcation I 【Week 2-1】 问题形成 在机器学习分类问题中：\nIn a machine learning classification problem:\n我们使用数据集 ${(x_i,y_i):1\\leq i\\leq N}$ 构建模型 $\\hat{y}=f(x)$。\nWe build a model $\\hat{y}=f(x)$ using a dataset ${(x_i,y_i):1\\leq i\\leq N}$.\n我们有一个模型质量的概念。\nWe have a notion of model quality.\n对 $(x_i,y_i)$ 可以理解为\u0026quot;样本 $i$ 属于类别 $y_i$\u0026quot;，或者\u0026quot;样本 $i$ 的标签是 $y_i$\u0026quot;。\nThe pair $(x_i,y_i)$ can be read as \u0026ldquo;sample $i$ belongs to class $y_i$\u0026rdquo;, or \u0026ldquo;the label of sample $i$ is $y_i$\u0026rdquo;.\n分类问题有二分类、多类分类等类型。数据集成为预测变量和标签对。\n在标签空间下的数据集 在预测变量空间下的数据集 判决域 在分类问题中，我们使用预测空间中的决策区域的概念。\nIn classification problems, we use the notion of decision regions in the predictor space.\n决策区域由与相同标签相关联的点组成。\nA decision region is made up of points that are associated to the same label.\\ 可以通过识别其边界来定义区域。\nRegions can be defined by identifying their boundaries.\\ 分类中的解决方案模型是将预测空间划分为由决策边界分隔的决策区域。\nA solution model in classification is a partition of the predictor space into decision regions separated by decision boundaries. 线性分类器 线性分类器使用决策区域之间的线性边界：\nLinear classifiers use linear boundaries between decision regions:\n线性边界由线性方程 $ w^T x = 0 $ 定义。\nLinear boundaries are defined by the linear equation $ w^T x = 0 $.\n扩展向量 $ x = [1, x_1, x_2\u0026hellip;]^T $ 包含预测变量，$ w $ 是系数向量。\nThe extended vector $ x = [1, x_1, x_2\u0026hellip;]^T $ contains the predictors and $ w $ is the coefficients vector.\n为了对样本进行分类，我们只需确定它位于边界的哪一侧。\nTo classify a sample we simply identify the side of the boundary where it lies.\n如果我们知道线性边界的系数向量 $\\mathbf{w}$，分类一个样本就非常简单： If we know the coefficients vector $\\mathbf{w}$ of a linear boundary, classifying a sample is very simple:\n构建扩展向量 $\\mathbf{x}_i$。\nBuild the extended vector $\\mathbf{x}_i$.\n计算 $\\mathbf{w}^T \\mathbf{x}_i$。\nCompute $\\mathbf{w}^T \\mathbf{x}_i$.\n使用以下事实进行分类：\nClassify using the following facts:\n如果 $\\mathbf{w}^T \\mathbf{x}_i \u0026gt; 0$，我们在边界的一侧。\nIf $\\mathbf{w}^T \\mathbf{x}_i \u0026gt; 0$, we are on one side of the boundary.\n如果 $\\mathbf{w}^T \\mathbf{x}_i \u0026lt; 0$，我们在边界的另一侧！\nIf $\\mathbf{w}^T \\mathbf{x}_i \u0026lt; 0$, we are on the other!\n如果 $\\mathbf{w}^T \\mathbf{x}_i = 0$\u0026hellip; 我们在哪里？（恰在边界上）\nIf $\\mathbf{w}^T \\mathbf{x}_i = 0$\u0026hellip; where are we?\n基本质量指标 通过比较预测值和真实标签，我们可以在数据集中识别出： By comparing predictions and true labels, we can identify in a dataset:\n每个类别中正确分类的样本（真实预测）。 The correctly classified samples (true predictions) in each class. 每个类别中错误分类的样本（错误预测）。 The incorrectly classified samples (false predictions) in each class. 两个常见且等效的质量指标是准确率A和错误率（或误分类率）E = 1 − A，定义为： Two common and equivalent notions of quality are the accuracy A and the error (or misclassification) rate E = 1 − A, defined as:\nA = 正确分类的样本数 / 总样本数 A = #correctly classified samples / #samples\nE = 错误分类的样本数 / 总样本数 E = #incorrectly classified samples / #samples\n逻辑回归模型 给定一个线性边界 $\\mathbf{w}$ 和一个预测向量 $\\mathbf{x}_i$，量 $\\mathbf{w}^T \\mathbf{x}_i$ 可以解释为样本到边界的距离。 Given a linear boundary $\\mathbf{w}$ and a predictor vector $\\mathbf{x}_i$, the quantity $\\mathbf{w}^T \\mathbf{x}_i$ can be interpreted as the distance from the sample to the boundary.\n如果我们在逻辑函数中设 $d = \\mathbf{w}^T \\mathbf{x}_i$，我们得到：_ _If we set $d = \\mathbf{w}^T \\mathbf{x}_i$ in the logistic function, we get:\n$ p(\\mathbf{w}{=tex}^T \\mathbf{x}{=tex}_i) = \\frac{e^{\\mathbf{w}^T \\mathbf{x}_i}}{1 + e^{\\mathbf{w}^T \\mathbf{x}_i}}{=tex} $\n对于固定的 $\\mathbf{w}$，我们简记为 $p(\\mathbf{x}_i)$：_ _For a fixed $\\mathbf{w}$, we will simply denote it as $p(\\mathbf{x}_i)$ to simplify the notation:\n当 $\\mathbf{w}^T \\mathbf{x} \\rightarrow \\infty$ 时，逻辑函数 $p(\\mathbf{x}_i) \\rightarrow 1$\nWhen $\\mathbf{w}^T \\mathbf{x} \\rightarrow \\infty$, the logistic function $p(\\mathbf{x}_i) \\rightarrow 1$\n当 $\\mathbf{w}^T \\mathbf{x} \\rightarrow -\\infty$ 时，逻辑函数 $p(\\mathbf{x}_i) \\rightarrow 0$\nWhen $\\mathbf{w}^T \\mathbf{x} \\rightarrow -\\infty$, the logistic function $p(\\mathbf{x}_i) \\rightarrow 0$\n线性分类器 $w$ 对样本进行标记：\nA linear classifier $w$ labels samples:\n如果 $w^T x_i \u0026gt; 0$，则标记为\u0026quot;正类\u0026quot;。\\\nIf $w^T x_i \u0026gt; 0$, then label as \u0026ldquo;positive\u0026rdquo;.\n如果 $w^T x_i \u0026lt; 0$，则标记为\u0026quot;负类\u0026quot;。\\\nIf $w^T x_i \u0026lt; 0$, then label as \u0026ldquo;negative\u0026rdquo;.\n关键点：\nKey points to notice:\n如果 $w^T x_i = 0$（点 $x_i$ 在边界上），则 $p(x_i) = 0.5$。\\\nIf $w^T x_i = 0$ (the point $x_i$ is on the boundary), then $p(x_i) = 0.5$.\n如果 $w^T x_i \u0026gt; 0$（点 $x_i$ 在正类区域），则随着远离边界，$p(x_i) \\rightarrow 1$。\\\nIf $w^T x_i \u0026gt; 0$ (the point $x_i$ is in the positive region), then $p(x_i) \\rightarrow 1$ as it moves away from the boundary.\n如果 $w^T x_i \u0026lt; 0$（点 $x_i$ 在负类区域），则随着远离边界，$p(x_i) \\rightarrow 0$。\\\nIf $w^T x_i \u0026lt; 0$ (the point $x_i$ is in the negative region), then $p(x_i) \\rightarrow 0$ as it moves away from the boundary.\n关键结论：\nHere is the crucial point:\n$p(x_i)$ 是分类器对 $y_i$ 为正类的确信度。\\\n$p(x_i)$ is the classifier\u0026rsquo;s certainty that $y_i$ is positive.\n$1 - p(x_i)$ 是分类器对 $y_i$ 为负类的确信度。\\\n$1 - p(x_i)$ is the classifier\u0026rsquo;s certainty that $y_i$ is negative.\n参数化与非参数化方法 线性分类器属于参数化方法家族：假设一种形状（在这种情况下是线性的），并使用我们的数据集在所有具有预选形状的边界中找到最佳边界。\nLinear classifiers belong to the family of parametric approaches: a shape is assumed (in this case linear) and our dataset is used to find the best boundary amongst all the boundaries with the preselected shape.\n非参数化方法提供了一种更灵活的替代方案，因为它们不假设任何类型的边界。 Non-parametric approaches offer a more flexible alternative, as they do not assume any type of boundary.\n最近邻 新样本被分配为最接近（最相似）的训练样本的标签。\nNew samples are assigned the label of the closest (most similar) training sample.\n边界没有明确定义（尽管它们存在并且可以获取）。\nBoundaries are not defined explicitly (although they exist and can be obtained).\\ 整个训练数据集需要被记忆。这就是为什么我们有时说最近邻是一种基于实例的方法。\nThe whole training dataset needs to be memorised. That\u0026rsquo;s why sometimes we say NN is an instance-based method. K近邻 K近邻算法（kNN）是最近邻算法的一个简单扩展，其过程如下：\nK-Nearest Neighbors (kNN) is a simple extension of the nearest neighbors algorithm, which proceeds as follows:\n给定一个新样本x：\nGiven a new sample x:\\\n我们计算它与所有训练样本xi的距离。\nWe calculate the distance to all the training samples xi.\n提取K个最近的样本（邻居）。\nExtract the K closest samples (neighbors).\\\n获取属于每个类别的邻居数量。\nObtain the number of neighbors that belong to each class.\\\n将样本x的标签分配为邻居中最常见的类别。\nAssign the label of the most popular class among the neighbors.\nKNN的特点：\n存在一个隐式的边界，尽管它不用于分类新样本。\nThere is always an implicit boundary, although it is not used to classify new samples.\n随着K的增加，边界变得不那么复杂。我们从过拟合（小K）转向欠拟合（大K）分类器。\nAs K increases, the boundary becomes less complex. We move away from overfitting (small K) to underfitting (large K) classifiers.\\\n在二分类问题中，K的值通常是一个奇数。这是为了防止样本的最近邻中一半属于每个类别的情况。\nIn binary problems, the value of K is usually an odd number. The idea is to prevent situations where half of the nearest neighbours of a sample belong to each class.\\\nkNN可以轻松地应用于多分类场景。\nkNN can be easily implemented in multi-class scenarios.\n分类 Classi\fcation II 【Week 2-2】 先验概率、后验概率与贝叶斯分类器 先验概率（Prior Probability） 先验概率是指在没有任何额外信息的情况下，某个事件发生的概率。它通常基于已有的知识或经验。例如，在分类问题中，先验概率 $P(C)$ 表示类别 $C$ 在没有任何特征信息的情况下出现的概率。\n后验概率（Posterior Probability） 后验概率是指在观察到某些证据或特征后，某个事件发生的概率。在分类问题中，后验概率 $P(C|X)$ 表示在观察到特征 $X$ 后，类别 $C$ 出现的概率。\n贝叶斯定理（Bayes\u0026rsquo; Theorem） 贝叶斯定理是计算后验概率的基础，其公式为： $$ P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)} $$ 其中： - $P(C|X)$ 是后验概率，即在观察到特征 $X$ 后类别 $C$ 的概率。 - $P(X|C)$ 是似然（Likelihood），即在类别 $C$ 下观察到特征 $X$ 的概率。 - $P(C)$ 是先验概率，即类别 $C$ 的概率。 - $P(X)$ 是证据（Evidence），即特征 $X$ 的概率，通常可以通过全概率公式计算： $$ P(X) = \\sum_{i} P(X|C_i) \\cdot P(C_i) $$\n贝叶斯分类器（Bayesian Classifier） 贝叶斯分类器是一种基于贝叶斯定理的分类方法。它的基本思想是：给定一个样本的特征 $X$，计算每个类别 $C$ 的后验概率 $P(C|X)$，然后将样本分配到后验概率最大的类别。\n具体步骤如下： 1. 计算先验概率：根据训练数据估计每个类别的先验概率 $P(C)$。 2. 计算似然：估计在给定类别 $C$ 下特征 $X$ 的似然 $P(X|C)$。 3. 计算证据：计算特征 $X$ 的概率 $P(X)$。 4. 计算后验概率：使用贝叶斯定理计算每个类别的后验概率 $P(C|X)$。 5. 分类决策：将样本分配到后验概率最大的类别。\n朴素贝叶斯分类器（Naive Bayes Classifier） 朴素贝叶斯分类器是贝叶斯分类器的一种简化版本，它假设特征之间是条件独立的。即： $$ P(X|C) = \\prod_{i=1}^{n} P(x_i|C) $$ 其中 $x_i$ 是特征 $X$ 的第 $i$ 个分量。\n朴素贝叶斯分类器的后验概率公式为： $$ P(C|X) = \\frac{P(C) \\cdot \\prod_{i=1}^{n} P(x_i|C)}{P(X)} $$ 由于 $P(X)$ 对于所有类别是相同的，因此在分类决策时可以忽略它，只需比较分子部分： $$ C_{\\text{pred}} = \\arg\\max_{C} P(C) \\cdot \\prod_{i=1}^{n} P(x_i|C) $$\n先验概率 $P(C)$ 是在没有任何特征信息的情况下类别 $C$ 的概率。 后验概率 $P(C|X)$ 是在观察到特征 $X$ 后类别 $C$ 的概率。 贝叶斯分类器 通过计算后验概率来进行分类决策。 朴素贝叶斯分类器 假设特征之间条件独立，简化了计算。 可以达到最佳准确率\n判别分析 Discriminant analysis 在判别分析中，我们假设各类别的密度是高斯分布。如果有一个预测变量 $x$，则类别A的密度为： $$ p(x|y = A) = \\frac{1}{\\sigma_A\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x-\\mu_A}{\\sigma_A}\\right)^2} $$ 其中 $\\mu_A$ 是均值，$\\sigma^2_A$ 是高斯分布的方差。\n如果有 $K$ 个预测变量，高斯类的密度表示为： $$ p(x|y = A) = \\frac{1}{(2\\pi)^{p/2}|\\Sigma_A|^{1/2}}e^{-\\frac{1}{2}(x-\\mu_A)^T\\Sigma_A^{-1}(x-\\mu_A)} $$ 这里 $x = [x_1, \\dots, x_K]^T$ 包含所有的预测变量，$\\mu_A$ 是均值，$\\Sigma_A$ 是协方差矩阵。\n对于类别A的密度，可以得到类似的表达式。\n当类密度满足高斯分布的时候，利用两个分类的协方差矩阵判断，如果相等就是Linear Discriminant Analysis（线性判别分析）(LDR)，如果不相等就是Quadratic Discriminant Analysis（二次判别分析）(QDA)\n比较学过的分类器 边界形状：逻辑回归和LDA构建线性边界，QDA构建二次边界。kNN不施加任何特定形状\n稳定性：对于样本较少的情况，逻辑回归可能非常不稳定，而DA方法产生稳定的解决方案。\n异常值：逻辑回归对位于边界之外的样本表现出稳健性，而LDA和QDA可能会受到影响。\n多分类：在判别分析中，多分类问题可以很容易被实现。\n先验知识：可以容易地通过贝叶斯方法整合进去。\n拓展贝叶斯分类器 即加入错判的代价作为参数\n改变T会改变分类器边界。得到的结果是能够达到最低错判成本。\n混淆矩阵 在类别敏感问题中，准确率和错误率并不是最合适的质量指标，因为不同类别的样本被错误分类的成本是不同的。\nIn class-sensitive problems, accuracy and error rate are not the most suitable quality metrics, as the cost of misclassifying samples from different classes varies.\n除了使用每个类别的错误分类成本外，我们还可以评估分类器在处理每个类别时的表现。这正是混淆矩阵或列联表所展示的信息。\nIn addition to using the misclassification cost for each class, we can assess how well the classifier deals with each class individually. This is precisely the information that a confusion or contingency matrix shows.\n对角线上是正确分类的结果，其他位置是错误分类的结果。\n对于二分类任务：\n其他指标 错误率和准确率是无法让我们研究分类器如何处理每个类别的性能指标。 Error rate and accuracy are performance rates that do not allow us to investigate how a classifier treats each class.\n灵敏度（召回率或真阳性率）：TP/(TP+FN) Sensitivity (recall or true positive rate): TP/(TP+FN) 特异性（真阴性率）：TN/(TN+FP) Specificity (true negative rate): TN/(TN+FP) 精确率（阳性预测值）：TP/(TP+FP) Precision (positive predictive value): TP/(TP+FP) 这些指标可以用作质量指标。 These rates can be used as quality metrics.\n单独提升一个质量指标很容易。例如，如果我们将每个样本都标记为正例，就能达到完美的灵敏度。 Improving one quality metric individually is easy. For instance, if we label every sample as positive, we would achieve a perfect sensitivity.\n问题在于，提升一个质量指标会损害其他指标。 The problem is that improving one quality metric deteriorates others.\n我们通常同时考虑成对的质量指标：\nWe usually consider pairs of quality metrics simultaneously:\n敏感性和特异性。\nSensitivity and specificity.\\ 精确率和召回率。\nPrecision and recall. F1 Score F1分数是另一个广泛使用的性能指标，它提供了精确率和召回率之间的平均值。\nThe F1-score is another widely used performance metric that provides an average between precision and recall. $$ F_1 = 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}} $$\nROC ROC（受试者工作特征）平面用于表示分类器在敏感性和1-特异性方面的性能。 The ROC (Receiver Operating Characteristic) plane is used to represent the performance of a classifier in terms of its sensitivity and 1-specificity.\n我们希望灵敏度接近1，1-特异性接近0（左上角）。 We would like the sensitivity to be close to 1 and the 1-specificity to be close to 0 (top left corner).\n我们无法同时使用两个指标对分类器进行排序。\nWe cannot rank classifiers using two metrics simultaneously.\n通常的做法是固定其中一个指标的最小值，并优化另一个指标，例如：在特异性至少为70%的情况下获得最高的灵敏度。\nThe usual practice is to fix a minimum value for one of the metrics and optimise the other, for instance: obtain the highest sensitivity with a minimum specificity of 70%.\n**AUC（曲线下面积）**是衡量可校准分类器性能的指标。\nThe AUC (Area Under the Curve) is a measure of goodness for a classifier that can be calibrated.\n好的分类器的AUC接近1，差的分类器接近0.5。 Good classifiers will have AUC close to 1, bad classifiers close to 0.5.\n方法论 Methodology II【week 3-1】 流水线 Pipeline 管道（pipeline）描述了一系列操作的顺序。 The term pipeline describes a sequence of operations.\n（有监督的）机器学习管道是使用一组预测变量（输入）生成预测（输出）的操作序列。 A (supervised) machine learning pipeline is the sequence of operations that produce a prediction (output) using a set of predictors (input).\n我们可以从每张图片中提取特征（例如萼片长度和宽度，花瓣长度和宽度），并构建一个以这些特征为输入的线性模型。 We can extract features from each picture (e.g., sepal length and width, petal length and width) and build a linear model that takes these features as input.\n机器学习解决方案不仅仅是一个模型。\nMachine learning solutions are more than just one model.\n它们可以包含多个阶段，形成一个处理管道：\nThey can include several stages that form a processing pipeline:\n转换阶段（处理输入数据）。\nTransformation stages (where input data is processed).\n多个机器学习模型并行运行。\nSeveral machine learning models running in parallel.\n最终聚合阶段（将各个输出组合成一个单一输出）。\nA final aggregation stage (where individual outputs are combined to produce one single output).\n训练后，管道的参数保持不变。\nAfter training, the parameters of a pipeline remain fixed.\\\n管道可以被测试和部署。\nPipelines can be tested and deployed.\n预测的质量取决于所有管道阶段。\nThe quality of the prediction depends on all the pipeline stages.\n数据标准化 Normalization 距离的概念是许多机器学习技术的基础：\nThe notion of distance is behind many machine learning techniques:\n在回归中，预测误差 4 e_i = y_i - \\hat{y}{=tex}_i $ 可以被视为一种距离。\nIn regression, the prediction error $ e_i = y_i - \\hat{y}{=tex}_i $ can be seen as a distance.\\ 在分类中，我们使用了样本与边界之间的距离（逻辑回归），以及样本之间的距离（kNN）。\nIn classification, we used the distance between samples and boundaries (logistic regression), and between samples (kNN).\\ 在聚类中，K-means 聚类是基于样本与原型之间的距离创建的。\nIn clustering, K-means clusters were created based on the distance between samples and prototypes.\\ 在密度估计中，标准差量化了样本与样本均值之间的平均距离。\nIn density estimation, the standard deviation quantifies the average distance between samples and the sample mean. 数值表示对我们的数据有影响，可能会影响最终模型和算法的性能。\nThe numerical representations of our data can have an impact on the final model and the performance of our algorithms.\n最小最大归一化 Min-max normalisation 产生在0到1范围内的值\n标准化 Standardisation 产生的结果均值为0，标准差为1\n标准化相关 在测试和部署过程中，我们应预料到会出现超出范围的值（例如，在最小-最大归一化中，z = 1:2）。\nDuring test and deployment, we should expect out-of-range values (e.g., z = 1:2 in min-max normalization).\\\n异常值可能会产生负面影响（例如，一个比第二大值大10倍的异常值会将最小-最大归一化压缩到[0; 0.1]）。\nOutliers can have a negative impact (e.g., an outlier 10 times larger than the second largest value will squeeze min-max normalization to [0; 0.1]).\n存在非线性缩放选项，例如使用逻辑函数的Softmax缩放或对数缩放。\nNon-linear scaling options exist, for instance, softmax scaling, which uses the logistic function, or logarithmic scaling.\\\n原始数据集中的值可能具有相关性。\nThe original values in your dataset might be relevant.\n通常，我们需要考虑意外的效果和扭曲。\nIn general, we need to consider unintended effects and distortions.\n许多机器学习算法在缩放后可能会产生不同的解决方案。\nMany machine learning algorithms might produce different solutions after scaling.\n转换 Transformation 数据转换是通过改变样本的表示方式来进行的数据操作。\nTransformations are data manipulations that change the way that we represent our samples.\n它们可以被视为将样本从一个空间移动到另一个空间。\nThey can be seen as moving samples from one space to another.\n归一化是单独对每个属性进行操作的一种转换示例：它应该作为我们流水线中的另一个阶段。 Normalisation is one example of a transformation that operates on each attribute separately: it should be included as another stage in our pipeline.\n在某些变换中，原始空间和目标空间具有相同的维度。\nIn some transformations, the original and destination spaces have the same number of dimensions.\n线性变换可以看作是旋转和缩放。\nA linear transformation can be seen as a rotation and scaling.\\ 非线性变换没有唯一的描述。\nThere is no unique description for non-linear transformations. 如果目标空间的维度少于原始空间，我们称之为降维。\nIf the destination space has fewer dimensions than the original one, we talk about dimensionality reduction.\n特别是在特征选择后，目标空间由原始属性的子集定义。\nIn particular, after feature selection, the destination space is defined by a subset of the original attributes.\\ 在特征提取中，新属性被定义为对原始属性的操作。\nIn feature extraction, the new attributes are defined as operations on the original attributes. 主成分分析 PCA 主成分分析（PCA）识别样本对齐的方向。\nPrincipal components analysis (PCA) identifies the directions along which samples are aligned.\n这些方向定义了一个与原始空间维度相同的目标空间。\nThese directions define a destination space with the same number of dimensions as the original space.\n使用数据集，PCA构建了一个线性变换，并为每个成分分配了一个分数。\nUsing a dataset, PCA builds a linear transformation and additionally assigns a score to each component.\n非线性变换 一种适用于先前示例的解决方案可以是一个由以下部分组成的管道： A solution for the previous example could be a pipeline consisting of:\n一个合适的非线性变换。 A suitable non-linear transformation.\n随后是第二层线性分类器。 Followed by a second layer of linear classifiers.\n最后是一个实现逻辑函数的单元。 A final unit implementing a logical function.\n该管道在原始空间中产生圆形边界。 This pipeline produces circular boundaries in the original space.\n复杂模型\u0026amp;Kernel方法 Complex models and kernel methods 许多复杂的机器学习模型实际上可以被视为一个变换后接一个简单模型。 Many complex machine learning models can in fact be interpreted as a transformation followed by a simple model.\n我们知道如何变换数据，只需要学习在目标空间上操作的模型。 We know how to transform our data and only need to learn the model that operates on the destination space.\n我们不知道变换，因此也需要学习它。 We don\u0026rsquo;t know the transformation, hence we need to learn it too.\n这可能涉及通过验证选择正确的变换，或通过训练调整给定变换的参数。 This can involve selecting the right transformation (via validation) or tuning the parameters of a given transformation (via training).\n核方法，如支持向量机，使用所谓的核函数隐式定义这种变换。 Kernel methods, such as support vector machines, implicitly define such transformations using so-called kernel functions.\n降维 Dimensionality reduction 可以使用PCA先进行降维，然后构建模型。\n特征选择 特征选择是一种减少数据集维度的方法，它假设只有原始属性的一个子集是相关的。 Feature selection is a method to reduce the dimensionality of a dataset that assumes that only a subset of the original attributes are relevant.\n为了选择最相关的特征，我们需要能够为不同的特征子集分配一个分数。 To select the most relevant features, we need to be able to assign a score to different subsets of features.\n如果我们的数据集有M个属性，总共有2^M − 1个子集可以考虑（例如，如果我们的数据集有10个属性，我们大约有1000个选项）。 If our dataset has M attributes, there are a total of 2^M − 1 subsets that we could consider (e.g., If our dataset has 10 attributes, we have roughly 1000 options). 在监督学习中，我们可以使用我们的目标指标来评估一个属性子集的相关性。 What do we mean by relevant? In supervised learning, we can use our target metric to evaluate how relevant a subset of attributes is. 我们仍然需要在每个特征子集上训练一个模型。最终的相关性还取决于我们训练模型的能力。 We still need a model trained on each subset of features. The final relevance will also depend on our ability to train a model. 特征选择可以被看作是一种验证形式，其中我们选择使用不同属性子集的模型。 Feature selection can be seen as a form of validation, where we select models that use different subset of attributes. Filtering 最简单的特征选择方法是单独考虑每个属性。\nThe simplest approach to feature selection is to consider each attribute individually.\n可以通过拟合模型并获取其验证性能来分配分数。\nA score can be assigned by fitting a model and obtaining its validation performance.\n然后选择最佳组件。\nThen, the best components are selected.\n但是可能效果不好\nWrapping 如果我们怀疑特征之间的交互可能至关重要，我们别无选择，只能一起评估它们，而不是分开评估。 If we suspect that the interaction between features might be crucial, we have no choice but to evaluate them together, rather than separately.\n包装方法通过以下方式考虑预测变量之间可能的交互： Wrapping approaches consider possible interaction between predictors by:\n训练具有不同特征子集的模型 Training a model with different subsets of features 使用验证方法评估每个生成的模型 Evaluating each resulting model by using validation approaches 选择具有最高验证性能的子集 Picking the subset with the highest validation performance. 贪婪搜索可以用来减少选项的数量。\nGreedy search can be used to reduce the number of options.\n特征提取 Feature extraction 特征提取可以显著降低数据集的维度，通过使用一些精心设计的特征来总结数据集。 Feature extraction can reduce dramatically the dimensionality of a dataset summarising it using a few well-designed features.\n集成 Ensembles 集成方法允许我们创建一个结合基础模型优势的新模型。 Ensemble methods allow us to create a new model that combines the strengths of base models.\n基础模型需要尽可能多样化，可以通过以下方式创建： Base models need to be as diverse as possible and can be created by:\n使用数据的随机子集训练一系列模型。 Training a family of models with random subsets of the data.\n使用属性的随机子集训练不同的模型。 Training different models with random subsets of attributes.\n训练完全不同的模型家族。 Training different families of models altogether.\nBagging Bootstrap 是一种从数据集中提取随机样本的统计方法。 Bootstrap is a statistical method that extracts random samples from a dataset.\n给定一个训练数据集，Bagging 通过自助法生成 K 个子数据集，并使用每个子数据集训练 K 个简单的基础模型。 Given a training dataset, bagging generates K sub-datasets by bootstrapping and trains K simple base models with each sub-dataset.\n最终的模型 f(x) 通过对基础模型 fk(x) 的预测结果进行平均或投票来组合。 The final model f(x) combines the predictions of the base models fk(x) by averaging or voting.\nDecision trees 决策树分类器通过仅使用一个预测变量实施一系列分割规则，将预测空间划分为多个决策区域。\nDecision tree classifiers partition the predictor space into multiple decision regions by implementing sequences of splitting rules using one predictor only.\n这导致了一种可以表示为树的算法。\nThis leads to an algorithm that can be represented as a tree.\n决策树的构建：\n在决策树中，根节点对应于整个未分割的数据集，而叶节点是其中一个决策区域。\nIn a decision tree, the root corresponds to the whole, unpartitioned dataset and a leaf is one of the decision regions.\n目标是创建纯净的叶节点，即尽可能包含来自同一类别的样本。\nThe goal is to create pure leaves, i.e. containing as many samples from the same class as possible.\\ 在分类过程中，样本被分配到其所在叶节点中的多数类别。\nDuring classification, a sample is assigned to the majority class in the leaf where the sample is located. 决策树是递归构建的：从根节点开始，我们递归地将每个区域分割成两部分。\nDecision trees are built recursively: Starting from the root, we recursively split each region into two.\n分割是轴平行的（使用一个预测变量进行决策）。\nSplits are axis-parallel (decisions using one predictor).\\ 选择的分割方式使得结果区域的纯度高于任何其他分割方式。\nThe chosen split is such that the purity of the resulting regions is higher than any other split.\\ 当满足给定条件时停止，例如区域中的样本数量。\nWe stop when a given criterion is met, such as the number of samples in a region. 决策树无法解决XOR问题\n决策树的问题：\n树模型简单且能轻松处理数值型和类别型预测变量。 Trees are simple and can handle easily both numerical and categorical predictors.\n然而，树模型存在记忆训练样本的风险，即过拟合。 However, trees run the risk of memorising training samples, i.e. overfitting.\n剪枝技术和停止准则可以帮助防止过拟合。 Pruning techniques and stop criteria can help to prevent this.\n不同的训练数据集可能导致不同的树结构。 Different training datasets can lead to a different tree structures.\n某些分类问题可以很容易地表示为树，但树可能难以学习（例如异或问题）。 Some classification problems can be easily represented as a tree, but the tree might be hard to learn (e.g. XOR).\n使用随机森林集成决策树可以解决这些问题。\n随机森林 随机森林通过随机化训练样本和预测器来训练许多单独的树。预测是通过对各个预测进行平均得到的。\nRandom forests train many individual trees by randomising the training samples and the predictors. Predictions are obtained by averaging the individual predictions.\n它们通常具有很高的准确性，但训练成本较高，并且比单棵树更难解释。\nIn general they have great accuracy, but can be expensive to train and are harder to interpret than a single tree.\nBoosting Boosting 采用了一种不同的方法：它生成一系列简单的基础模型，其中每个后续模型都专注于前一个模型无法正确处理的数据样本。 Boosting follows a different approach: it generates a sequence of simple base models, where each successive model focuses on the samples that the previous models could not handle properly.\n神经网络与深度学习【Week 3-2】 模式与结构 模式是我们数据中的规律性，而结构是目标群体中的规律性。 Patterns are regularities in our data, and structure is a regularity in our target population.\n机器学习项目依赖于通过识别数据中的模式来发现潜在结构。 Machine learning projects rely on discovering the underlying structure by identifying patterns in data.\nThe Curse of Dimensionality 维度灾难是一个警告。无关的属性不会相互抵消，它们会表现为虚假的模式。添加更多属性（以防万一）实际上可能导致模型性能变差。我们可以使用特征选择技术来降低问题的维度。 The curse of dimensionality is a warning. Irrelevant attributes do not cancel each other out, they show up as spurious patterns. Adding more attributes (just in case) can actually result in worse-performing models. We can use feature selection techniques to reduce the dimensionality of the problem but first, let\u0026rsquo;s use our domain knowledge.\n神经网络 神经网络是一种受人类神经系统启发而设计的计算系统，通常作为机器学习模型家族的一部分。\nA neural network is a computing system loosely inspired by the human nervous system, commonly used as a family of Machine Learning models.\n从计算角度来看，神经网络由相互连接的单位组成，这些单位（松散地）模仿神经元。这种架构具有吸引力，因为： From a computational angle, neural networks consist of interconnected units that (loosely) mimic neurons. This architecture is appealing since:\n神经科学表明，生物神经网络可以解决任何问题。 Neuroscience suggests biological neural networks can solve any problem. 数学表明，人工神经网络可以再现任何输入/输出关系，只要它们足够复杂。 Mathematics suggests artificial neural networks can reproduce any input/output relationship, provided they are complex enough. 因此，神经网络模型家族可以被视为一种通用机器。 Hence, the family of neural network models can be seen as a universal machine. 感知器 perceptron 感知器是神经网络的基本单元。\nThe perceptron is the basic unit of a neural network.\n它由一个权重向量 w 和一个激活函数 h(⋅) 定义，将扩展向量 x 映射到输出 a。\nIt is defined by a weight vector w and an activation function h(⋅) that map an extended vector x to an output a.\n系数 w0 被称为偏置。\nThe coefficient w0 is known as the bias.\n激活函数一般是非线性的。\n层 Layer 层是使用相同输入的感知器的集合。 A layer is a collection of perceptrons that use the same input.\n层中的每个感知器都会产生一个独立的输出。 Each perceptron within a layer produces a separate output.\n由 L 个感知器和 K 个输入组成的一层有 L × (K + 1)个权重。\n架构 Architecture 神经网络的架构描述了各层之间的连接方式。\nThe architecture of a neural network describes how layers are connected.\n输入层是预测向量，隐藏层生成内部特征，输出层生成预测结果。\nThe input layer is the predictor vector, hidden layers produce internal features, and the output layer produces the prediction.\n神经网络 神经网络由按架构连接的感知器层组成。 A neural network consists of perceptrons arranged in layers that are connected according to an architecture.\n每个连接有一个参数（连接的权重）。 There is one parameter per connection (the connection\u0026rsquo;s weight).\n每一层生成中间特征。 Each layer produces intermediate features.\n层数决定了神经网络的深度（因此区分了浅层神经网络和深层神经网络）。 The number of layers determines the depth of the neural network (hence the distinction between shallow and deep neural networks).\n感知器作为线性分类器 使用阶跃函数作为激活函数，则得到线性分类器\n感知器实现逻辑门功能 激活函数为阶跃函数。\n感知器实现网格模式检测 结合线性分类器和逻辑功能可以得到： 从计算角度看待神经网络 从认知角度来看，大型神经网络具有吸引力，因为它们为我们提供了创建新的、日益复杂的概念所需的灵活性，这些概念可能对做出预测至关重要。\nFrom a cognitive point of view, large neural networks are appealing, as they give us the necessary flexibility to create new and increasingly complex concepts that might be relevant to make a prediction.\n然而，更高的灵活性增加了过拟合的风险（我们可以将其视为网络创建和使用无关概念）。\nHowever, higher flexibility increases the risk of overfitting (which we can see as a network creating and using irrelevant concepts).\n大量的参数需要调整，因此计算需求可能过高。\nA large number of parameters need to be tuned, therefore the computational requirements might be too high.\n对于复杂的输入，例如由数百万像素组成的图片，这种情况更加严重。\nFor complex inputs, such as pictures consisting of millions of pixels, this is even more severe.\n神经网络成本函数 给定数据集${\\left(\\boldsymbol{x}i,y_i\\right),1\\leq i\\leq N}$，标签取值为0或1，常用负对数似然函数： $$ l(\\boldsymbol{W})=-\\frac{1}{N}\\sum{n=1}^Ny_i\\log\\left[\\hat{y}_i\\right]+\\left(1-y_i\\right)\\log\\left[1-\\hat{y}_i\\right] $$ 其中$\\hat{y}i=h{\\boldsymbol{W}}(x_i)$。可以扩展到多类分类器。\n梯度下降与反向传播 梯度下降法是寻找成本函数 l(W) 最优系数集 W 的首选方法。 Gradient descent is the method of choice to find the optimal set of coefficients W for the cost function l(W).\n获取梯度很容易，但计算成本可能很高。 Obtaining the gradient is easy, but can be computationally expensive.\n反向传播是一种计算梯度的有效算法。 Back-propagation is an efficient algorithm to compute the gradient.\n然后，优化算法使用该梯度来更新 W。 This gradient is then used by the optimisation algorithm to update W.\n反向传播利用了微积分中的链式法则。 Back-propagation exploits the chain rule of calculus.\n事实证明，要计算某一层的梯度，我们只需要来自下一层的信息。 It turns out that to compute the gradient in one layer, we just need information from the next layer.\n反向传播从输出开始：它获取成本并向后计算隐藏单元的梯度。 Back-propagation starts from the output: it obtains the cost and proceeds backwards calculating the gradients of the hidden units.\n训练模型的注意事项 初始化：如果初始权重为零，反向传播将无法进行。初始权重值应为随机值。\nIf the initial weights are zero, back-propagation fails. Initial weight values should be random.\\ 过拟合：神经网络可能包含数百万个参数，使用正则化和基于验证的早停来避免过拟合。\nNeural networks can have millions of parameters. Use regularisation and validation-based early stop to avoid overfitting.\\ 最小值：代价函数具有多个局部最小值，从不同的随机初始值重新训练。\nThe cost function has multiple local minima. Retrain from different random starting values.\\ 缩放输入：输入值的范围可能影响权重的值，标准化以确保输入被平等对待。\nRange of input values can affect the values of the weights. Standardise to ensure inputs are treated equally.\\ 架构：不同的架构适用于不同的问题。\nDifferent architectures suit different problems. 迁移学习 一个已经成功训练用于问题A的神经网络可以重用于相关问题B，例如： A neural network that has been successfully trained for problem A can be reused for related problem B, for instance:\n我们可以保持早期阶段不变，使其成为一个固定的转换阶段T(x)。 We can leave the early stages unchanged, becoming a fixed transformation stage T(x).\n我们可以使用新数据f(z)重新训练后期阶段。 We can retrain the late stages using new data f(z).\n我们本质上是在转移一个已经学习到的变换，并将其重新用于不同的问题： We are in essence transferring an already learnt transformation and reusing it for a different problem:\n无需训练 T(x)（问题 A 和问题 B 的参数相同）。 No need to train T(x) (same parameters for problems A and B).\n问题 B 中 f(z) 的最优参数将接近问题 A 中找到的参数（训练时间更短！）。 The optimal parameters of f(z) for problem B will be close to the ones found for problem A (shorter training time!).\n全连接层 全连接层中的每个感知器都接收来自前一层的所有输出。 Each perceptron in the fully connected layer receives all the outputs from the previous layer.\n全连接层具有大量的参数，训练它们可能具有挑战性。 Fully connected layers have a large number of parameters, and training them can be challenging.\n网格数据中的等变性 Equivariance in grid data 图像和时间序列是由与定义空间关系的规则网格相关联的各个属性组成的复杂数据类型。\nImages and time series are complex data types consisting of individual attributes associated to a regular grid defining a spatial relationship.\n某些网格数据表现出等变性，即相同的模式可以出现在网格的不同位置。\nSome grid data exhibit the equivariance property, according to which the same pattern can be expected in different locations of the grid.\n卷积层 卷积层施加了额外的限制： Convolutional layers impose additional restrictions:\n感知器被排列成一个称为特征图的网格， Perceptrons are arranged as a grid known as a feature map,\n专注于输入网格中的不同有限区域， focus on different limited regions in the input grid and\n并共享它们的参数，表示为一个称为核的网格。 share their parameters, represented as a grid called kernel.\n特征图被高效地计算为核与输入的卷积， The feature map is efficiently calculated as a convolution of the kernel\n或者换句话说，用核过滤输入。 and the input or in other words, filtering the input with the kernel.\n卷积层可以拥有多个特征图，每个特征图都与不同的概念相关联。它们形成了一组堆叠的图。 Convolutional layers can have several feature maps, each of which is associated to a different concept. They form a stack of maps.\n卷积核的维度为 H × W × D，其中 H 是高度，W 是宽度，D 是深度（输入特征图的数量）。\nThe dimensions of a kernel are H × W × D, where H is the height, W is the width, and D is the depth (number of input feature maps).\\\n每个卷积核的总权重数为 H × W × D + 1（包括偏置）。\nThe total number of weights per kernel is H × W × D + 1 (including the bias).\n训练卷积层意味着使用数据来调整每个卷积核的权重。\nTraining a convolutional layer means using data to tune the weights of each kernel.\n池化层 池化层减少了特征图的大小。\nPooling layers reduce the size of feature maps.\n池化层通过将一定区域内的值缩减为一个单一数值来定义，并插入在连续的卷积层之间。\nPooling layers are defined by reducing a certain area to a single number and are inserted between successive convolutional layers.\n池化层有两种类型：\nThey come in two flavors:\n最大池化：输出是滤波器区域内最大的值。\nMax pooling: The output is the largest value within the filter area.\\ 平均池化：输出是滤波器区域内值的平均值。\nAverage pooling: The output is the average of the values within the filter area. 需要注意的是，池化层不需要训练！\nNote that pooling layers do not need to be trained!\n深度学习架构 深度神经网络并不是任意层的任意序列。 Deep neural networks are not arbitrary sequences of arbitrary layers.\n相反，它们具有适合特定目标的预定义架构。 On the contrary, they have a predefined architecture that is suitable for a specific goal.\n在分类任务中，常见的架构是： In classification, it is common to see architectures in which:\n前几层定义了一些简单的概念，最后几层定义了许多复杂的概念。 The first layers define a few, simple concepts, the last layers define many, complex concepts. 随着网络加深，特征图逐渐缩小。 Feature maps shrink as we move deeper into the network. 相同的中间概念可以用于不同的目标。 The same intermediate concepts can be useful for different goals.\n我们可以使用迁移学习来重用现有的解决方案。 We can use transfer learning to reuse existing solutions.\n结构分析 Structure analysis【Week 3-4】 无监督学习 无监督学习不会将任何属性提升为标签类别：所有属性都被平等对待。\nUnsupervised learning does not elevate any attribute to the category of label: all the attributes are treated equally.\n无监督学习的本质可以归结为一个简单的问题：我的数据在哪里？\nThe essence of unsupervised learning is encapsulated in the simple question: where is my data?\n属性空间是无限的且大部分是空的，而对这个问题的答案将是一个模型，该模型将使我们能够识别出我们可能期望找到样本的区域。\nThe attribute space is infinite and mostly empty, and the answer to this question will be a model that will allow us to identify the regions where we could expect to find samples.\n有两种主要做法：\n密度估计：创建模型，使我们能够量化在属性空间的某个区域内找到样本的概率（概率密度）。\nDensity estimation: Creates models that allow us to quantify the probability of finding a sample within a region of the attribute space (probability density).\\ 结构分析：创建模型，识别属性空间内样本密度较高的区域（聚类分析）或方向（成分分析）。\nStructure analysis: Creates models that identify regions within the attribute space (cluster analysis) or directions (component analysis) with a high density of samples. 无监督学习的用处 无监督学习可以用于以原型样本的形式提供对population的总结。\nUnsupervised learning can be used to provide summaries of a population in the form of prototype samples. 无监督学习也可以用于发现结构。 Unsupervised learning can also be used to discover structure. 无监督学习可用于构建类别密度，描述在某个区域中找到来自给定类别的样本的概率。\nUnsupervised learning can be used to build class densities that describe the probability of finding a sample from a given class in a region. 概率密度还可用于识别异常，即可能属于不同总体的样本。\nA probability density is also useful to identify anomalies, i.e. samples that are likely to belong to a different population. 聚类分析 聚类是一类无监督学习算法，用于将数据集的结构描述为相似样本的组或簇。 Clustering is a family of unsupervised learning algorithms that describe the structure of a dataset as groups, or clusters, of similar samples.\nSimilarity as proximity 聚类可以定义为彼此接近的样本组。在这种情况下，我们使用接近度作为相似性的概念。 Clusters can be defined as groups of samples that are close to one another. In this case, we use proximity as our notion of similarity.\n使用距离作为相似度的度量，同一簇中的样本应该彼此接近，而不同簇中的样本应该相距较远。 Using distance as our notion of similarity, samples within the same cluster should be close to one another and samples from different clusters should be far apart.\n例：指标，平方距离\n质量指标 使用簇间散布和簇内散布。\n最优的聚类拥有最低的簇内散布，最大的簇间分布。\n使用聚类原型（如聚类中心）是描述聚类的一种简单方法。\nA simple way to describe a cluster is by using cluster prototypes, such as the centre of a cluster.\nK-means 聚类 簇内样本散布指标：\nK-means 将数据集划分为 K 个由其均值表示的簇，并按以下步骤迭代进行：\nK-means partitions a dataset into K clusters represented by their mean and proceeds iteratively as follows:\n原型被获取为每个簇的中心（或均值）。\nPrototypes are obtained as the centre (or mean) of each cluster.\\ 样本被重新分配到具有最近原型的簇。\nSamples are re-assigned to the cluster with the closest prototype. 随着 K-means 算法的进行，我们会看到样本被重新分配到不同的簇，直到达到某个稳定解，此时没有样本被重新分配。\nAs the K-means algorithm proceeds, we will see samples been reassigned to different clusters until at some point we reach a stable solution, where no sample is reassigned.\n最终解是局部最优解，不一定是全局最优解。\nThe final solution is a local optimum, not necessarily the global one.\n肘部法 Elbow method 使用Kmeans的时候需要指定一个超参数簇数量K，但是有些时候我们不知道这个K是多少。\n验证策略可以建议一个合适的超参数K值。 Validation strategies can suggest a suitable value for the hyperparameter K.\n然而，选择产生最低I(C0)的K值并不奏效，因为随着聚类数量的增加，I(C0)总是会降低。 However, choosing the value of K producing the lowest I(C0) would not work, as I(C0) always decreases as the number of clusters increase.\n假设真实的聚类数量为 KT。对于 K \u0026gt; KT，我们应预期质量的提升速度会比 K \u0026lt; KT 时更慢，因为我们将拆分真实的聚类。 Assume the true number of clusters is KT. For K \u0026gt; KT, we should expect the increase in quality to be slower than for K \u0026lt; KT, as we will be splitting true clusters.\n真实的聚类数量可以通过观察 K 值超过某个点后质量提升速度减缓来识别。 The true number of clusters can be identified by observing the value of K beyond which the improvement slows down.\n非凸聚类 Non-convex clusters Kmeans产生的簇是圆形的，在非凸样本上的效果不好。\n在非凸聚类中，我们可以通过从一个样本到另一个样本的小跳跃到达任何样本。\nIn non-convex clustering, we can reach any sample by taking small jumps from sample to sample.\n非凸场景提出了一种不同的聚类概念，即样本是连接的，而不仅仅是接近的：如果我与你相似，你与他们相似，那么我也与他们相似。\nNon-convex scenarios suggest a different notion of cluster as a group of samples that are connected, rather than simply close: if I am similar to you, and you are similar to them, I am similar to them too.\n这种将聚类视为一组连接样本的概念是许多聚类算法的基础，例如DBSCAN（基于密度的噪声应用空间聚类）。\nThis notion of cluster as a group of connected samples is behind many clustering algorithms, such as DBSCAN (density-based spatial clustering of applications with noise).\nDBSCAN DBSCAN属于基于密度的算法家族，其中使用每个样本周围样本密度的估计将数据集划分为聚类。DBSCAN belongs to the family of density-based algorithms, where an estimation of the density of samples around each sample is used to partition the dataset into clusters.\nDBSCAN定义了两个量，半径r和阈值t。首先计算每个样本周围半径r的邻域内的样本数量（不包括自身）作为密度。然后，识别出三种类型的样本： DBSCAN defines two quantities, a radius r and a threshold t. A density is first calculated as the number of samples in a neighbourhood of radius r around each sample (excluding itself). Then, three types of samples are identified:\n核心：其密度等于或高于阈值t。 Core: its density is equal or higher than the threshold t. 边界：其密度低于阈值t，但其邻域内包含一个核心样本。 Border: its density is lower than the threshold t, but contains a core sample within its neighbourhood. 离群值：任何其他样本。 Outlier: Any other sample. DBSCAN算法流程如下：\nThe DBSCAN algorithm proceeds as follows:\n识别核心点、边界点和离群点。\nIdentify core, border, and outlier samples.\\ 彼此在邻域内的核心点对会被连接。连接的核心点形成簇的主干。\nPairs of core samples that are within each other\u0026rsquo;s neighbourhood are connected. Connected core samples form the backbone of a cluster.\\ 边界点被分配到其邻域内核心点数量最多的簇。\nBorder samples are assigned to the cluster that has more core samples in the neighbourhood of the border sample.\\ 离群点不分配到任何簇。\nOutlier samples are not assigned to any cluster. 层次聚类 Hierarchical clustering 给定一个包含N个样本的数据集，存在两种平凡的聚类解决方案：一个包含所有样本的单一簇，以及每个样本自成一个簇的解决方案。 Given a dataset consisting of N samples, there exist two trivial clustering solutions: one single cluster that includes all the samples, and the solution where each sample is a cluster on its own.\nK-means生成K个簇，但我们需要在1 ≤ K ≤ N之间选择K。 K-means produces K clusters, but we need to choose K within 1 ≤ K ≤ N.\n在DBSCAN中，簇是自动发现的，但最终的簇数量取决于半径r和阈值t的值。 In DBSCAN clusters are discovered automatically, but the final number of clusters depends on the values of the radius r and the threshold value t.\n这种模糊性最终揭示了数据集的结构可以在不同层次上探索，从而揭示不同的特性。 This ambiguity ultimately reveals that the structure of a dataset can be explored at different levels that expose different properties.\n层次聚类是一系列通过逐步构建【不同层次】聚类安排的聚类方法。\nHierarchical clustering is a family of clustering approaches that proceed by progressively building clustering arrangements at different levels.\n生成的聚类安排集合是层次化的，因为一个层次中的聚类包含来自下一层次中一个或多个聚类的所有样本。\nThe resulting collection of clustering arrangements is hierarchical in the sense that a cluster in one level contains all the samples from one or more clusters in the level below.\n聚类在不同层次之间关系的表示称为树状图。 The representation of the relationship between clusters at different levels is called a dendrogram.\n在底部，每个样本都是一个单独的聚类。 At the bottom, each sample is one cluster. 在顶部，整个数据集形成一个聚类。 At the top, the whole dataset forms one cluster. 构建树状图有两种基本策略： There exist two basic strategies to build a dendrogram:\n分裂法或自上而下法从树状图的顶部开始分割聚类，直到底部层次。 The divisive or top-down approach splits clusters starting from the top of the dendrogram and stops at the bottom level. 凝聚法或自下而上法从底部开始合并两个聚类，直到达到顶部层次。 The agglomerative or bottom-up approach merges two clusters, starting from the bottom until we reach the top level. 决定应当怎样切分/合并聚类的选项：\n单链法：使用两个簇中最近的两个样本之间的距离。这种方法会产生任意形状的簇。\nSingle linkage: uses the distance between the two closest samples from two clusters. This option results in clusters of arbitrary shapes.\n全链法：使用两个簇中最远的两个样本之间的距离。这种方法倾向于产生球形簇。\nComplete linkage: uses the distance between the two further samples from each pair of clusters. This choice produces clusters that tend to have a spherical shape.\n组平均法：使用两个簇中样本之间的平均距离。这种方法也会产生球形簇，但对异常值更具鲁棒性。\nGroup average: uses the average distance between samples in two clusters and also produces spherical shapes, although they are more robust to outliers.\n组分分析 component analysis 组件分析使我们能够识别数据在空间中对齐的方向。 Component analysis allows us to identify the directions in the space that our data are aligned with.\n这对于转换数据集、清理数据和降低其维度非常有用。 This can be useful to transform our dataset, clean it and reduce its dimensionality.\n密度估计 Density estimation 【week 4-2】 数据分布 数据集是分布在属性空间内的样本集合，但并没有占据整个空间。\n我的大多数样本在哪里？ Where are most of my samples?\n我应该在这个属性空间的区域中期待一个样本吗？ Should I expect a sample in this region of the attribute space?\n在这个区域找到样本的概率是多少？ What is the probability that I will find a sample in this region?\n给定一个概率，我将在哪里找到下一个样本？ Given a probability, where will I find my next sample?\n切分并计数 将空间划分为较小的区域并计算每个区域内的样本数量，是描述观察到的分布的一种简单方法。\nPartitioning the space into smaller regions and counting the number of samples within each region is a simple way of describing the observed distribution.\n这也为我们提供了从同一总体中提取更多样本时预期结果的指示，即真实分布。\nIt also gives an indication of what to expect if we extract more samples from the same population, i.e. the true distribution.\n计数本身不会提供有用的定量答案，但我们可以将其转换为比率。\nCounts will not give useful quantitative answers, but we can transform them into rates.\n概率密度 概率密度是描述我们数据真实分布的模型。\nProbability densities are models that describe the underlying true distribution of our data.\n密度估计 在机器学习中，我们使用数据来构建概率密度，这个任务称为密度估计。\nIn machine learning, we use data to build probability densities, and this task is called density estimation.\n我们可以构建考虑所有属性的概率密度，也可以构建考虑部分属性的概率密度，后者称为边缘概率密度。\nWe can build a probability density that considers all the attributes, or probability densities that consider a subset of the attributes, which we call marginal probability densities.\n在具有两个属性 $x_1$ 和 $x_2$ 的数据集中，概率密度的简化数学表示如下：\nIn a dataset with two attributes $x_1$ and $x_2$, the simplified mathematical notation for the probability densities is as follows:\n概率密度表示为 $p(x_1, x_2)$，或者使用向量表示法 $p(\\mathbf{x})$。\nThe probability density is denoted by $p(x_1, x_2)$ or, using vector notation, $p(\\mathbf{x})$.\n边缘概率密度表示为 $p(x_1)$ 和 $p(x_2)$。\nThe marginal probability densities are denoted $p(x_1)$ and $p(x_2)$.\n非参数方法的密度估计 非参数方法不指定概率密度的形状。\nNon-parametric methods do not specify the shape of the probability density.\n直方图 histogram 直方图是最简单且最著名的非参数密度估计方法。\nThe histogram is the simplest and best known non-parametric method for density estimation.\n直方图通过将特征空间划分为大小相等的区域（称为\u0026quot;箱\u0026quot;）来构建。\nA histogram is built by dividing the feature space into equal-sized regions called bins.\n密度通过落在每个箱中的样本比例来近似估计。\nThe density is approximated by the fraction of samples that fall within each bin.\n核方法 Kernel method Kernel methods proceed by building an individual density around each sample first and then combining all the densities together. Individual densities have the same shape (the kernel), for instance a Gaussian. 核方法首先在每个样本周围构建一个单独的密度，然后将所有密度结合在一起。每个密度具有相同的形状（即核），例如高斯分布。\n非参数方法总结：\n非参数密度估计方法不假设概率密度的具体形状。\nNon-parametric methods for density estimation do not assume any specific shape for the probability density.\\\n它们包含需要指定的超参数，例如箱体大小或核类型。\nThey contain hyperparameters that need to be specified, such as the bin size or the type of kernel.\n直方图提供离散的概率密度，而核方法生成平滑的概率密度。\nThe histogram provides a discrete probability density, whereas kernel methods produce a smooth one.\\\n注意维度灾难：直方图可能最终每个箱体中只有一个样本；核方法中的个体分布可能最终彼此孤立。\nBeware of the curse of dimensionality: histograms can end up having one sample per bin; individual distributions in kernel methods might end up being isolated from one another.\n参数方法的密度估计 参数化方法指定了概率密度的形状。 Parametric approaches specify the shape of the probability density.\n密度估计的问题在于估计其参数。 The problem of density estimation consists of estimating its parameters.\n有许多可用的模型，包括： There are many available models, including:\n高斯分布，通常表示为 N (μ, Σ)。 The Gaussian distribution, usually denoted by N (μ, Σ).\n对数正态分布。 Log-normal distribution.\n均匀分布。 Uniform distribution.\n伽马分布。 Gamma distribution.\n高斯分布 高斯分布或正态分布 N (μ, σ) 由两个参数 μ 和 σ 定义，分别描述其位置和宽度（均值和标准差）。\nThe Gaussian or normal distribution N (μ, σ) is defined by two parameters μ and σ describing its location and width.\n$p(x_1)=\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left(\\frac{x_1-\\mu}{\\sigma}\\right)^2}$\n中心极限定理 Central Limit Theorem, CLT 中央极限定理（CLT）指出，大量独立随机量的和服从高斯分布。\nThe Central Limit Theorem (CLT) states that the sum of a large number of independent, random quantities has a Gaussian distribution.\n如果你的数据是高斯分布的，那么就没有什么可发现的了。\nIf your data is Gaussian, there is little to discover.\n多元高斯分布 高斯分布可以扩展到二维、三维\u0026hellip;属性空间： The Gaussian distribution can be extended to 2D, 3D\u0026hellip; attribute spaces:\n$p(\\boldsymbol{x})=\\frac{1}{\\sqrt{(2\\pi)^k|\\boldsymbol{\\Sigma}|}}e^{-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^T\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})}$\n$\\boldsymbol x=[x_1,\\ldots,x_P]^{T^{\\prime}}$ 包含所有属性。\n$\\boldsymbol x=[x_1,\\ldots,x_P]^{T^{\\prime}}$ contains all the attributes.\n均值 $\\boldsymbol\\mu$ 和协方差矩阵 $\\Sigma$ 描述了分布的位置和形状。\nThe mean $\\boldsymbol\\mu$ and covariance matrix $\\Sigma$ describe the position and shape of the distribution.\n给定一个高斯分布 p(x1, x2)，以下陈述是等价的： Given a Gaussian distribution p(x1, x2), the following statements are equivalent:\n属性 x1 和 x2 是独立的（例如，我们无法根据一个预测另一个的值）。 Attributes x1 and x2 are independent (e.g., we cannot predict the value of one based on the other). 协方差矩阵 Σ 是对角矩阵。 The covariance matrix Σ is diagonal. p(x1, x2) 可以表示为边际密度 p(x1) 和 p(x2) 的乘积，而它们本身也是高斯分布。 p(x1, x2) can be obtained as the product of the marginal densities p(x1) and p(x2), which are themselves Gaussian. 这一性质可以扩展到更高维的属性空间。\n高斯分布估计 给定一个包含N个样本xi的数据集，可以使用最大似然方法来估计高斯分布的参数。\nGiven a dataset consisting of N samples xi, the parameters of a Gaussian distribution can be estimated using maximum likelihood approaches.\n高维度总是存在问题并导致过拟合。\nHigh dimensionality is always problematic and leads to overfitting.\n对协方差矩阵的约束和正则化技术可用于稳定解。\nConstraints on the covariance matrix and regularisation techniques can be used to stabilise the solution.\n混合模型 Mixture models 数据集可能表现出多个模式（簇），这种情况下单一的高斯密度函数并不适用。 Datasets can exhibit more than one mode (clumps) for which single Gaussian densities are not suitable.\n在这种情况下，混合密度模型如高斯混合模型（GMM）是一个方便的选择。 In such cases, mixture densities such as Gaussian Mixture Models (GMM) constitute a convenient choice.\n期望最大化（EM）算法是一种迭代过程，用于将高斯混合模型（GMM）拟合到数据集，类似于K均值算法。 The Expectation-Maximization (EM) algorithm is an iterative process to fit a Gaussian Mixture Model (GMM) to a dataset, similar to the K-means algorithm.\n噪声与离群值（异常值） Noise and outliers 样本从同一总体中提取时，总会表现出一定程度的随机性，并偏离潜在的模式。这种偏离被称为噪声。 Samples extracted from the same population will always exhibit some level of randomness and deviate from the underlying pattern. Such deviations are known as noise.\n有时，样本可能非常不同，以至于我们怀疑其偏离不仅仅是由于噪声。我们称这些样本为异常值或异常点。 Sometimes a sample can be so different that we doubt its deviation is just due to noise. We call these samples outliers or anomalies.\n异常值是属于完全不同总体的样本。 Outliers are samples that belong to a different population altogether.\n检测异常值对许多应用（例如欺诈检测或网络安全）非常重要。 Detecting outliers is important for many applications (for instance, fraud detection or cybersecurity).\n如果在模型训练中使用异常值，它们也可能产生负面影响。 Outliers can also have a negative impact if used during model training.\n两类异常值方法：\n异常检测算法旨在识别异常值。\nAnomaly detection algorithms aim at identifying outliers. 异常值检测之后会根据应用采取相应的行动，例如移除。\nOutlier detection is followed by actions that depend on the application, for instance, removal.\\ 鲁棒性是一种设计需求，用于减轻异常值的影响。\nRobustness is a design requirement that mitigates the impact of outliers. 例如，不同的成本函数可以以不同的方式考虑大偏差的成本。\nFor instance, different cost functions can account differently for the cost associated to large deviations. 基本异常值检测算法 异常检测算法的主要思想是量化观察到与总体模式有一定距离的样本的概率。如果这个概率很低，则该样本为异常。 The main idea behind an anomaly detection algorithm is to quantify the probability of observing samples some distance away from the general pattern. If this probability is low, the sample is an anomaly.\n类别密度估计分类器 分类器应用贝叶斯规则将后验概率转化为先验概率和类别密度。 Classifiers that apply Bayes rule turn posterior probabilities into priors and class densities.\n类别密度 p(x|C) 描述了每个类别 C 在预测空间中样本的分布。 A class density p(x|C) describes the distribution of samples in the predictor space for each class C.\n类别密度通过密度估计方法获得。 Class densities are obtained using density estimation methods.\n我们需要为每个类别分别拟合一个概率分布。 We need to fit a probability distribution for each class separately.\n应用类别密度到朴素贝叶斯分类器 高斯分布是类密度最流行的选择。 Gaussian distributions are the most popular choice for class densities.\n在高维场景中，参数的总数非常大：对于P个预测变量，我们有P（均值）+ P²（协方差）个参数。 In high-dimensional scenarios, the total number of parameters is very large: for P predictors, we have P (mean) + P² (covariance) parameters.\n朴素贝叶斯分类器做出了（朴素）假设，即预测变量是独立的，因此一个P维高斯分布可以表示为它的P个边缘分布的乘积。 Naive Bayes classifiers make the (naive) assumption that predictors are independent, hence a P-dimensional Gaussian distribution can be expressed as the product of its P marginal distributions.\n由于这个额外的约束，我们需要获得P（均值）+ P（方差）个参数，这降低了过拟合的风险。 As a result of this additional constraint, we need to obtain P (means) + P (variances) parameters, which reduces the risk of overfitting.\n应用类别密度到聚类 K-means 可以被视为 GMM 拟合的一种版本，其中高斯分布具有相同的对角协方差矩阵，因此聚类倾向于呈球形。\nK-means can be seen as a version of GMM fitting, where the Gaussian distributions have the same diagonal covariance matrix and hence clusters tend to be spherical.\nGMM 可以作为一种聚类方法，生成椭球形的聚类。首先我们拟合 K 个高斯密度，然后将每个样本分配到最可能的密度。\nGMM can be used as a clustering method that produces ellipsoidal clusters. First we fit K Gaussian densities and then we assign each sample to the most likely density.\n","date":"2025-01-29T00:00:00Z","permalink":"https://katomelon.github.io/blog/p/2025-ml-note/","title":"机器学习 全四周 概念整理"},{"content":"NAT 网络地址转换 NAT = Network Address Translation 背景：ipv4 的可分配地址少，不够用了，且分配不公平。\nNAT 的做法：把内部网（家庭网）流出的数据报由同一个 NAT IP 地址发出，将不同的内部 IP 映射到不同的端口号上。对内部网，数据报视作由该 NAT 路由发出。 这样一来本地网只需要一个 IP 地址，但对外部网来说：\nIP 地址可不需要通过 ISP 提供 内部设备或地址可随意更换，无需通知外部网 可在不调整内部网地址的情况下更换 ISP （重要）内部网设备不可从外部显式访问 NAT 的实现：\n对向外流出的数据报： 将数据报的来源 IP 地址替换为 NAT 路由的地址（和映射分配的端口）再发给外网。 外部服务器会收到这条请求，其回复的目标 IP 地址会是 NAT 路由的地址和端口。 对向内流入的数据报： 将回复中的目标 IP 地址和端口替换为内部网实际主机的 IP 地址和端口 端口号是 16 位的，这说明一次 NAT 可以支持最多六万个同时连接。 NAT 的争议问题：\n路由器应当只处理第三层（即网络层）的问题。注意，事实上一个 NAT box 应当被视作网络层成员，因为虽然看起来是端口 - 端口通信，但还是第三层的。 地址短缺问题应当通过 ipv6 解决 端到端复杂性违例：使用 NAT 不符合原先网络层的所有设计，这使得应用开发者在开发应用时必须考虑到有 NAT 存在的情况，不能大规模使用 IP 确认的 P2P 等等。 客户端无法访问到 NAT 之后的服务器。 IPv6 提案动机：使用 32 位 IP 地址，能给宇宙里的每一颗沙子都分一个地址用。 一些额外的东西：修改了数据头的格式，可加速处理与转发，改善服务质量（QoS，Quality of Service）。\nIPv6 数据头 为固定 40 字节大小的头，不允许分片。\n优先级：指定 flow 中数据报的优先级 flow 标签：指定数据报是否在同一个 flow 中 next header：指定上层的数据协议 感觉这东西考的概率不大。 IPv6 的其他变化 移除了数据报中的校验和部分：IPv6 认为在链路层和传输层（如 TCP 和 UDP）已经有足够的错误检测机制，所以可以省略 IP 层的校验和，从而减少每个中继节点（hop）处理数据包的时间，提高转发效率。 在 IPv4 中，选项（options）是 IP 头部的一部分，可以使头部变得很长，影响数据包的处理速度。而在 IPv6 中，选项被允许存在，但它们不再是头部的一部分，而是通过一个称为“Next Header”（下一个头部）的字段来指示。这意味着选项信息被放在扩展头部中，使基本的 IPv6 头部保持固定长度（40 字节），从而简化了处理。 支持 ICMPv6 \u0026mdash; ICMP（Internet Control Message Protocol）是用于网络设备间传递控制信息的协议。在 IPv6 中，引入了一个新的版本 ICMPv6。ICMPv6 不仅保留了原有的 ICMP 功能，还增加了一些新的消息类型，比如“Packet Too Big”消息，用于通知发送方数据包太大，需要分片。ICMPv6 还包括多播组管理功能，用于管理多播组的成员关系。 了解即可。\n从 IPv4 迁移 \u0026amp; 向下兼容 没有办法让所有路由器一夜之间从 IPv4 迁移到 IPv6，因此需要能够在有 IPv4 的路由器的网络中传输 IPv6 数据包。 做法：隧道技术（tunneling）,将整个 IPv6 数据报封装到 IPv4 的负载 payload 中进行传输 软件定义网络 SDN 传统网络：\n数据平面通过转发表 forwaring table 根据最长前缀匹配原则进行数据包转发。 控制平面会为每个路由器计算出转发表 转发表只能根据 IP 地址来决定怎样转发数据包 软件定义网络：更加灵活\n可使用编程语言 Java、Python 等编写自己的控制算法 可根据数据包头的任意部分进行转发。 SDN 数据平面：\n由控制平面下发一系列“match-action”规则，可对数据包做很多操作（转发、修改数据等等） 更加灵活（例如，可单独路由视频数据包，将隐私数据包单独路由，丢弃可以数据包等） SDN 控制平面：\n单个中心化的控制器（不是分布式系统） 可编程，而非固定的。可以对控制器进行编程。 可以创造自己的路由算法然后直接在网络上测试，不用先花大量美金去造个硬件路由器。 每个路由器会带有一张流向表 flow table，由路由控制器计算并分发出去。 OpenFlow OpenFlow 是一种 SDN 协议。构成如下： 匹配规则 + 动作。 路由协议 找出从发送主机到接收主机间，经过的路由器的“好的”路线。\n路线：数据包从给定的初始源主机到给定的最终目的主机所经过的路由器序列 “好”：低成本，速度快，低拥塞。 路由算法分类 信息是否全局？ 全局 - 所有路由器都能得到网络全局的拓扑情况和信息等——link state 算法 去中心化 - 路由器知道与自己相连的邻居，到邻居间的连接成本——distance vector 算法\n网络结构动态还是静态？ 静态：路由随时间变化基本不变化 动态：路由会快速变化，定期更新，响应连接成本变化等。\nLink-state 路由算法 使用迪杰斯特拉算法作最短路计算。\n这个是个很成块的大题，此处略，见串讲的讲解： 【互联网协议串讲 - 北邮国际学院 (全 3p)】 【精准空降到 1:26:47】 https://www.bilibili.com/video/BV1qy4y177LM/?p=2\u0026share_source=copy_web\u0026vd_source=dac6d447bab3520339763cd9fb9b0afa\u0026t=5207\nDistance vector 路由算法 使用 Bellman-Fold 算法。\n【互联网协议串讲 - 北邮国际学院 (全 3p)】 【精准空降到 1:40:02】 https://www.bilibili.com/video/BV1qy4y177LM/?p=2\u0026share_source=copy_web\u0026vd_source=dac6d447bab3520339763cd9fb9b0afa\u0026t=6002\n自治系统（AS）与路由 AS = autonomous systems\n上述的路由算法都太理想化了：认为所有路由器完全相同，所有网络均扁平，这和实际不符。 真实的网络系统：\n有百万个主机和路由目的地等，你无法在路由表中存储所有目标。 只采用 distance vector 方法进行路由表交换会导致大量网络拥塞。 互联网是网中之网，每个网络管理员可能希望控制自己网络中的路由规则。 intra-AS 路由（AS 内路由）\n在同一个 AS 网络内进行路由 同个 AS 内的所有路由器必须运行相同协议 不同 AS 内的路由可以运行不同协议 存在网关路由：在自己 AS 的边缘，和其他 AS 的路由相连接。 inter-AS 路由（AS 间路由）\n在不同 AS 间路由 网关路由器同时做内路由和外路由 考虑 AS 的路由情况，路由表/转发表是有 AS 内和 AS 间的路由算法共同配置决定的。AS 内路由决定 AS 内的路由情况，二者共同作用决定外部目标的路由情况。\nintra-AS 路由（AS 内路由） IGP = interior gateway protocols 内部网关协议 常见的内部路由协议：\nRIP: Routing Information Protocol 咱们讲的是这个 -\u0026gt; OSPF: Open Shortest Path First (IS-IS protocol essentially same as OSPF) IGRP: Interior Gateway Routing Protocol (Cisco proprietary for decades, until 2016) OSPF 协议采用 link state 算法：网络间会分发 link state 数据包，得到每个节点的拓扑结构，然后基于迪杰斯特拉算法进行计算路由。 路由器会向整个 AS 内的所有路由器广播（flood）通告 OSPF 数据包，这一步是直接通过 IP 协议而非 TCP 或 UDP。\nIS-IS 路由协议和 OSPF 基本一样。\n高级特性：\n安全性：所有 OSPF 消息均需认证 允许多个相同成本的路径（RIP 不支持） 支持不同 ToS 的链路成本指标 集成的单播和多播支持。MOSPF（多播 OSPF）利用与标准 OSPF 相同的拓扑数据库来计算多播路由信息。 在大型网络中，OSPF 可以采用分层结构，通过划分区域来提高网络的可扩展性和管理效率。 inter-AS 路由（AS 间路由） 任务：\n需要知道哪个目的地能够到达 AS2，哪能到达 AS3\n要把这个信息告诉当前 AS1 内的所有路由器 BGP = Border Gateway Protocol\neBGP：从相邻的 AS 获取子网可达性信息。eBGP（外部 BGP）用于在不同的自治系统（AS）之间交换路由信息。通过 eBGP，AS 可以从相邻的 AS 获取到哪些子网是可达的。\niBGP： 将可达性信息传播给所有 AS 内部的路由器。iBGP（内部 BGP）用于在同一个自治系统内部传播路由信息。这样，AS 内部的所有路由器都可以知道哪些子网是可达的。\nBGP 不仅仅是传递路由信息，它还要根据这些信息和预先设定的策略来选择最佳路由。所谓“好”路由通常指的是更优的路径，这可以基于多种因素，例如路径长度、路由策略、带宽等。 通过 BGP，某个子网可以向整个互联网公告它的存在。这意味着其他 AS 可以通过 BGP 了解该子网的可达性，并据此进行路由选择。 ","date":"2024-08-08T00:00:00Z","permalink":"https://katomelon.github.io/blog/p/ipn-w3/","title":"互联网协议 - Week3概念整理"},{"content":"\n考核 考核形式和绝大多数英方课形式类似，coursework + final exam，考试占 75%，平时占 25%。平时成绩由一次小组视频作业、两次小测，一次 Lab 组成。\n考核项 类型 占比 说明 期末考试 Final 75% ( ˇωˇ) 课程小测 *2 Coursework 10% 第一次小测时间 March 14th，后面的待通知，具体时间地点考试内容详见 QMPlus 和相关邮件，闭卷考试。 小组视频 Coursework\n5% 介绍一个国内的互联网应用，包括基本描述，网络架构，需求，影响和未来发展。最长五分钟。 Lab Coursework\n10% icebound：LAB 是上机实际操作 packet tracer 这个软件。在 LAB 开始前一周会教大家使用这个软件。LAB 当天，就像考试一样大家进机房，之后会发一个考试题，上面有本次 LAB 的要求。做完之后会有助教来检查，并且问你两个互联网协议的问题。助教会根据你的回答还有 LAB 的完成情况给分。（2024.08.08更新：今年lab没有考试环节，是要求你做完实验之后，填写一个实验手册，给你搭好的网络拓扑截图、运行一些指令等等，然后将你的工程文件和手册全部提交到qmplus上） 课程排课共四周：\n周数 内容 第一周 课程基本介绍，传输层 Part1，小组视频作业 第二周 传输层 Part2，网络层 Part1，第一次小测 第三周 网络层 Part2，数据链路层 Part1，Lab 的 Tutorial （随后 Lab） 第四周 数据链路层 Part2，网络管理与安全，第二次小测 本资料的术语未经严格校对，仅供参考。\n1.1 何为互联网 本部分定义/介绍了课程中需要使用的一些基本概念：互联网的组成、元素，何为协议，网络边缘与网络核心，一些基础术语（比特、字节、网络数据），何为物理媒介、何为带宽等等。\n互联网的组成 从构成的角度 互联网是连接计算机与计算机的网络（这门课的另外一个名字叫计算机网络），即数以百万计的相连计算机构成了互联网。\n主机 Host：即终端系统，运行着网络应用程序。 通信链路 Communication links：即光纤 fiber、铜线 copper、无线电 radio、卫星 satellite 等用于传输信息的链路设备。 带宽 Bandwidth：数据链路的传输速率。 数据包交换器 Packet switches：用来转发 Forward 数据包，可以是路由器 Router 或交换机 switches。 此外：\n互联网是“网中之网”，连接起多个互联网服务提供商 ISP, Internet Service Provider，能够互相通信。 数据的正常通信需要依靠事先约定好的协议 Protocol 才能进行。例如 TCP、IP、HTTP、802.11(WiFi 标准) 等等。 有多种多样的互联网标准 Internet Standards，如 RFC，IETF 等等。 从服务的角度 向应用提供服务的基础设施 Web, VoIP (Voice over IP), email, games, e-commerce, social nets 等。 向应用提供编程接口的基础设施 允许发送和接收应用程序“连接”到互联网的钩子，提供服务选项，类似于邮政 何为协议 Protocol 原 protocols define format, order of messages sent and received among network entities, and actions taken on message transmission, receipt 译 协议定义了网络实体之间发送和接收消息的格式、顺序，以及消息发送、接收时所采取的动作 1.2 网络边缘 网络边缘与网络核心 想象互相连接无数设备的互联网是一张大网，“网络边缘 Network Edge”即网络的终端设备（服务器 Server 与客户端 Client），通过“接入网 Access Network”与网络的其他部分相连，通过“核心网 Network Core”连接网中之网，互相通信。\n网络边缘 Network Edge：是用户和计算机与网络相连的部分，即主机 Hosts（服务器 + 客户端），服务器通常在数据中心里。 接入网 Access Network、物理媒介 Physical Media：是将网络边缘与网络的其他部分相连的部分。例如家庭网，办公网，移动网络等等。 核心网 Network Core：网络的“中心”部分，即互相连接的路由器，网中之网。 比特 Bit 和字节 Byte 一个比特 bit 即一个二进制数字 Binary Digit（0 或 1） 一个字节 Byte 为一组共 8 个比特，可视为 0-255（或 -128 到 127）的数字，或两位十六进制 Hexadecimal 数字（A0, FF, 10 这样的） 数据量（KB MB GB）通常以字节为单位，传输速度（Kb/s Mb/s）通常以比特作单位。 网络数据包 Packet 数据包（packet），另译作报文/数据报文 很大的数据难以传输，将数据切分成小单元（即数据包 Packets）会很有用。 数据包传输起来相对更快，可以检查传输错误，如果发现错误，可以重新传输。\n数据包传输过程 获取应用消息\n将数据切分为长度为 L 个比特的数据包\n将数据包以传输速率 transmission rate（表示为 R）传输进接入网\nlink transmission rate, aka link capacity, aka link bandwidth\n链路传输速率，又名链路容量，又名链路带宽\n1.3 核心网 Network Core 数据包的传输延迟 Delay 例：1500B 的数据以 1Mb/s 的速率传输，（传输延迟）耗时多少？ Time = 1500×8 (bits) / 1000000 (bits/s) = 0.012s = 12ms 核心网是内部相连的多个路由器构成的网格。在进行数据包交换时，主机会将应用层数据分解为数据包。从来源到目的地的链路上，将数据包由一个路由器上转发到另一个中。每个数据包都以全容量传输。数据包必须完整到达路由器后才可进行下一步的传输。\n例：如图所示，传输1500B大小的数据包有多长时间的（传输）延迟？ Time = 1500×8 (bits) / 1000000 (bits/s) + 1500×8 (bits) / 2000000 (bits/s) = 18ms\n队列 Queue 与丢包 Loss 如果到达的数据包超过了链路的传输速率：\n数据包将进入排队，等待传输。 如果缓冲区 Buffer已满，数据包将会丢失。 核心网的两大功能 路由 Routing：决定数据包从来源到目的地的路线 Route。 转发 Forwarding：将数据包从路由器的输入移动到合适的路由器出口。 互联网的结构 每个使用者不可能直接连接（需要的连接数太多），所以每个使用者先连接到互联网服务提供商。 互联网服务提供是一项可行的业务，所以会有多家 ISP 进行竞争。 多个 ISP 间也需要互相连接（通过 Internet eXchange Provider），不然中国电信的用户就没法给中国移动的用户发微信了。 可能出现区域网络用来连接接入网到 ISP。 谷歌等大型企业则通过内容提供网络向用户提供服务。 一种结构：谷歌等大公司会直接将其数据中心连接到互联网，通常会绕过一级 ISP 和区域性 ISP。 1.4 延迟 Delay 丢包 Loss 吞吐量 Throughput 回顾 - 丢包和延迟是怎么产生的：队列Queue与丢包Loss\n延迟的组成 d_total = d_proc + d_queue + d_trans + d_prop d_proc：该节点的处理延迟。用于校验比特错误，决定输出链路。通常花费小于毫秒的时间。 d_queue：在输出链路上排队等待传输的延迟，取决于路由器的拥塞程度。 d_trans：传输延迟。数据包的传输延迟Delay = L/R。可理解为路由器将比特写入到输出端的耗时。 d_prop：传播 Propagate 延迟。为数据在介质中传播的时间，= 物理链路长度 d/传播速度 s 传输延迟和传播延迟是两个不同的概念。 丢包 Loss 队列（或称缓冲区 Buffer）容量有限，队列满后到达的数据包将丢失。 丢失的数据包可能由上个节点重传，可能由数据发送源重传，也可能就不传了。 吞吐量 Throughput 原 throughput: rate (bits/time unit) at which bits transferred between sender/receiver 译 吞吐量：发送端/接收端之间传输比特的速率 (比特/时间单位) 分瞬时值和某段时间的平均值。 瓶颈链路 Bottleneck Link：限制端到端吞吐量的链路。 共用链路计算端到端吞吐量时应取平均到每条链路上的吞吐量计算瓶颈值。\n1.5 协议层与服务模型 为什么需要分层 显式指定结构可明确复杂系统的各个部分（参考模型待下文讨论）。 模块化简化了系统的维护和更新步骤（某部分的实现的变更对系统的其他部分透明）。 TCP/IP 模型（互联网模型） TCP = 传输控制协议 Transmission Control Protocol IP = 互联网协议 Internet Protocol 自顶向下：应用层、传输层、网络层、链路层、物理层。\n应用层：支持网络应用程序（FTP、SMTP、HTTP 等） 传输层：进程之间的数据传输（TCP、UDP） 网络层：数据报 Datagrams 从源到目的地的路由（IP、路由协议） 链路层：临近网路单元间的数据传输（以太网 Ethernet、WiFi、PPP） 物理层：即比特在线缆上的传输 ISO/OSI 参考模型 ISO = 国际标准化组织 Internet Standards Office OSI = 开放系统互连标准 Open System Interconnection 相比互联网模型添加了表示层和会话层\n表示层：允许应用解释数据含义，例如加密压缩等。 会话层：提供数据同步、检查点、数据恢复交换等。 互联网模型缺少这两层，如有需要应当在应用层实现。 传输模型的各层 第七层 - 应用层：计算机上的应用程序的数据。 第六层 - 表示层：与字符集的设定和表示有关（在 TCP/IP 和现实互联网中不存在） 第五层 - 会话层：与连接的整个生命周期有关（在 TCP/IP 和现实互联网中不存在） 第四层 - 传输层：与机器间的端到端连接有关。包含有关稳定性的信息、有关机器上的哪个程序将会发送/接受此数据的信息。 第三层 - 网络层：使得数据能够从出发的计算机到接受的计算机。带有计算机的 IP 地址，带有校验和 checksum 以检验数据是否受损。 第二层 - 数据链路层：使得数据能够到达临近（在同一本地网络下）的计算机。包含 MAC（Media Access Control）地址以区分计算机。 第一层 - 物理层：天上的无线电，地下的电缆。 各层的设备 路由器：第三层的设备。读取第三层的地址，决定数据包将往哪个方向走。 交换机：第二层的设备。读取第二层的地址，决定哪台临近的计算机应当得到该信息。交换机通常比路由器要简单。 中继器：第一层的设备。加强信号或重构受损的信号，使得能够继续发送他。 TCP/IP：分层与表头 TCP/IP 模型的绝大多数层都伴随一个“表头”（“表头和表尾”） 表头中包含与发送的信息相分离的信息，用以说明关于此信息的一些信息。 （除物理层）每层都会附加一个 Header，例如网络层会附加一个网络地址表示该数据包将被发送到哪里，传输层会附加一个端口号表示要将该包发送给哪个程序。 下层状态会包含上层所有的包。例如第二层（链路层）中会包含来自第三层、第四层的包。 为什么有 ISO/OSI 和 TCP/IP 两种模型 一句话说完：ISO/OSI 模型是委员会提出的，很理想化，且规划模型花费了很长时间；TCP/IP 模型是工程师提出的，目标是能上线用起来再说，在实验中对该协议进行新的修改和应用。ISO/OSI 模型提出的时候，TCP/IP 已经发展的大到不可改变的程度了。 会话层和表示层很有用，但是现在还不存在。 1.6 发展历史 略，详见 PPT。\n2.1 应用层基础概念 应用层简述 应用层是作为程序员最常接触到的一层。在应用层，网络这个概念被抽象出来，你可以访问到达“Socket（套接字/该单词的原始含义为插座）”的数据流。由应用层来决定程序发送、接收到的数据的格式。 一般不同的应用会有不同的格式。\nSocket（套接字👎） 进程通过其 Socket 发送/接受数据。 Socket 可近似为门，发送消息的进程将消息推到门外，由门外的传输基础设施将消息送达到接收进程。 寻址 Addressing 过程 只依靠接收信息的计算机的 IP 地址不足以识别进程，因为一台主机上可以运行多个进程。为收到消息，进程也需要有标识符。标识符包括 IP 地址和该进程的端口号。 一些约定好的服务端口：HTTP 服务器 - 80， 邮件服务器 - 25。\n应用层协议定义 消息的类型：例如表示该消息为请求或响应。\n消息语法：消息中有哪些字段，都是怎样描述的。\n消息语义：字段的含义。\n规则：关于怎样处理消息的发送/响应。\n开放协议：在 RFC 中有定义，允许互相操作，例如 HTTP，SMTP 等。\n私有 Proprietary 协议：例如 Skype 等软件自己的协议。\nTCP/IP 的四种地址 第二层 - 物理地址/链路地址：由节点的 LAN 或 WAN 定义的地址 第三层 - 逻辑地址（32 位 IPv4，128 位 IPv6）：逻辑地址用于通用通信，不依赖底层物理网络 第四层 - 端口地址：用于区分不同的进程 第七层 - 应用规定的地址：有些应用会有用户友好型的地址，例如电子邮件地址。 会话层与表示层 这课件排的稀碎，为什么在这又提一遍。。。\n会话层：关注两主机间连接的生命周期，身份验证与鉴权 表示层：为应用“翻译”数据，例如确定用于编码的具体的字符集等。 这俩层在现实的互联网中都不存在，都在应用层实现。 3.1 传输层服务 传输层服务与协议 原 provide logical communication between app processes running on different hosts 译 为在不同主机上运行的应用程序进程之间提供逻辑通信 传输层协议在终端系统上运行。\n发送端将应用层消息分成段 Segments，传给网络层。 接收端将段合成消息，传给应用层。 传输层对应用有不止一种协议，例如 TCP 和 UDP。 传输层与网络层 网络层负责主机间的逻辑通信 传输层负责进程间的逻辑通信（依赖于\u0026amp;增强了网络层的服务） 3.2 多路复用 Mux 与解复用 Demux 原 Combining several streams of data into a single stream 译 将多个数据流合并为单个流 例如，刷手机的时候，一边看网页，一边看邮件，一边看微信，这些所有连接都通过同一条链路。\n原 A stream of data is separated out into its individual components 译 多路复用的相反过程，将一个数据流拆分为其独立的组成 手机接收到的数据流被拆分，发送给合适的程序。 解复用是怎样工作的 主机接收到 IP 数据报 Datagram 后：\n每个数据报都会有其来源 IP 地址和其目标 IP 地址 每条数据报都会携带传输层的数据段 传输层的数据段带有来源和目标端口号 主机即可根据 IP 地址与端口号，将数据定位到合适的 Socket 中。\n无连接的解复用过程（UDP） 回忆一下两个前提：\n创建的 Socket 拥有一个本地的端口号 将数据报发送到 UDP Socket 时，必须指定目标 IP 地址和端口 主机接收到 UDP 数据段时： 检查数据段中的目标端口 将 UDP 数据段指向到该端口的 Socket 具有相同目标端口但来自不同 IP 地址的 IP 数据报会被指向到同一个 socket。 面向连接的解复用过程（TCP） TCP socket 由四个元组指定：\n源 IP 地址 源端口号 目标 IP 地址 目标端口号 服务器主机可以同时支持多个 TCP socket。每个 socket 都由其自己的四个元组指定。解复用时，接收端使用这四个值将数据段指向合适的 socket。 Web 服务器对每个连接的客户端都有不同的 socket。 一些常用的端口号 80：标准 HTTP 服务器 22：从远程计算机登录 SSH 25：SMTP，用来发邮件 143：IMAP，用来收邮件 443：HTTPS 服务器，安全标准的 HTTP 只是约定俗成，你愿意的话也可以自己改。 3.3 无连接传输：UDP UDP = User Datagram Protocol 是最简单的可用的互联网传输协议。“尽力而为”，UDP 数据段可能会丢失，可能到达应用层时是无序的。 无连接（Connectionless）是指：\n在 UDP 发送端和接收端间无需握手，可以直接发送 每个 UDP 包都独立于其他包处理 UDP 协议用于：\n直播等多媒体应用，其对丢包可容忍，对速率要求高。 DNS（=Domain Name System） SNMP（=Simple Network Management Protocol） 若需要为 UDP 添加可靠性，则应在应用层添加可靠性处理和错误恢复等。\n为什么选择 UDP 不需要建立连接 通信简单，在发送端、接收端没有连接状态 表头大小更小 没有拥塞控制，传播更快 UDP 消息段表头 来源端口、目标端口、消息段长度、校验和（↓）\nUDP 校验和 目标：侦测到传输的消息段中的传输错误（例如比特反转）\n发送端：将消息段（包含表头字段）视作 16 位整数，将字段内容相加，将计算得到的校验和放到 UDP 的校验和字段。 接收端：计算收到的消息段的校验和，检查计算得到的值与表头的值是否一致。如果不一致则出现了错误，如果一致则一般没有错误（可能侦测不到部分情况的错误）。\nUDP 消息头封装 3.4 可靠消息传输（rdt）法则 可靠消息传输向上层提供（抽象的）可靠消息传输服务，在不可靠信道上依赖 rdt 协议进行可靠传输。 前置知识：有限状态机 课件里描述协议机制用的。例 - 投币道闸： rdt 1.0 - 通过可靠信道进行可靠传输 要求信道完全可靠，没有比特错误，也没有丢包。 将发送端和接收端分别拆成状态机：发送端打包消息然后发送，接收端等待接收消息然后解包。 rdt 2.0 - 信道存在比特错误 信道可能将数据包中的部分数据出现比特反转，校验和可以检测到此错误。 怎样从错误中恢复：\nACK（acknowledgement）：接收端显式通知发送端，数据包成功接收 NAK（negative acknowledgements）：接收端显式通知发送端，数据包存在错误 收到 NAK 后，发送端会重新传输数据包。 rdt 2.0 中的新机制：错误检测，反馈（发送 ACK、NAK） rdt 2.0 的致命缺陷 ACK/NAK 损坏：发送端不清楚接收端的情况，也不能直接重传，有可能会重复。 怎样处理重复？\n若 ACK 或 NAK 损坏，则重新传输。 发送端对每个数据包添加序列号（seq），接收端将无视重复的数据包。 发送端发出一个数据包后，会等待接收端的响应后再决定下一步的行为 rdt 2.1 - 解决 2.0 的一些问题 发送端为每个数据包添加两种序列号之一#(0, 1)，检查收到的 ACK/NAK 是否损坏。 接收端检查收到的数据包是否重复。 发送端： 接收端： rdt 3.0 - 存在错误和丢包的信道 信道可能出现丢包。数据包和 ACK/NAK 都有可能丢失。 做法：发送端等待 ACK 一定时间，时间过后如果没有收到 ACK 就重发。 （如果数据包或 ACK 只是延迟而非超时丢包，重传会重复，但是序列号已经能处理此问题了） ","date":"2024-08-08T00:00:00Z","permalink":"https://katomelon.github.io/blog/p/ipn-w1/","title":"互联网协议 - 课程概要 \u0026 Week1概念整理"},{"content":"Java 的历史 Sun 公司最早设计其运行在行动电话、PDA 上的编程语言，希望其是一门安全的语言，开发起来效率高。后面 PDA 失败了，重点转向了嵌入式软件系统，这时语言叫 Oak 语言。1993 年万维网出现，Mosaic 浏览器出现，该语言改名 Java，Mosaic 浏览器可以动态的下载 Java 代码运行（当时叫 applets），用作 Web 交互，允许用户在 Web 页面上打游戏、做表格之类的。\nJava 的特性 能做绝大多数其他传统主流语言可做的东西，但更简洁，更简单。\n没有自动类型转换，是强类型语言 没有指针操作 没有 GOTO，没有全局变量，没有头文件 没有类似 C 语言的 struct 和 union（因为有面向对象了，没必要） 拥有自动垃圾回收机制（不需要手动 free 内存） 不允许多重继承 不兼容 C、C++ 语言特性：\n简单（我是没觉得，啰嗦倒是真啰嗦） 面向对象 平台不相关（一次编写到处运行） 鲁棒（拥有错误检查） 安全（拥有权限控制特性） 多线程 动态 Java 程序的运行流程 Java 支持一次编写到处运行。Java 程序从编写到运行经历如下步骤：\n编写源代码，可以使用你喜欢的任意编辑器。 通过指令 javac 将代码编译成字节码（xxx.class）（而非直接的可执行文件） 通过指令 java 运行该字节码。会先检查这段字节码的所有字节是否合法，是否违反 java 安全限制，随后通过解释器在对应平台上的 Java 虚拟机（JVM，Java Virtual Machine）上执行该程序。 对于程序 MyProgram.java，应当先运行 javac MyProgram.java，随后运行 java MyProgram（注意没有.class），就可以运行程序了。\n因为不同平台的指令集不同，Java 没有采取交叉编译，而是通过先编译成字节码再在对应平台通过平台自己的解释器执行，实现跨平台特性，即一次编写到处运行。 Java 基础 这是一段最基础的 Java 代码：\n1 2 3 4 5 6 7 8 9 10 /** * MyProgram.java * * Created on 28 June 2010, 17:56 */ public class MyProgram { public static void main(String[] args) { System.out.println(\u0026#34;Hello World!\u0026#34;); } } /** MyProgram.java\u0026hellip; Java 的注释和 C 比较类似：\n// 这是单行注释\n/* 这是\n多行\n注释 */\n长下面这样的叫 javadoc 注释。 Javadoc 这个工具能识别代码内的 Javadoc 注释，然后把他们整理成文档。 public class MyProgram { Java 对面向对象的崇拜已经到了痴狂的地步，所以每个程序的最根部应该是一个类，例如本程序的类名就是 MyProgram。（实际上在 Java21 还是 22 开始就允许你单写 main 开始，不过英方这边教的我建议还是按着 Java8 的规范来，虽然他确实要求用 OpenJDK21） 如果该类为 public，则要求该代码的文件名必须等于类名。例如此文件应当为 MyProgram.java。\npublic static void main(String[] args) { 我们一行一行看：\npublic：是指访问限制。public 说明该方法全局可访问。 static：表示静态方法。即该方法为单例，与类相关，而非与对象相关。后面面向对象会讲。 void：表示返回值类型。void 表示没有返回值。 main：方法名。名为 main 的方法会被系统当作程序入口，和 C 类似。 String[] args：指通过命令行传入的其他参数。详见 Lab 1. System.out.println(\u0026ldquo;Hello World!\u0026rdquo;); 这个东西类似 C 语言的 printf。ln 表示打印完了会带一个换行。\nJava 程序的编写、编译和运行 编写代码时，应当遵循 KISS 原则：Keep It Simple Stupid JDK = Java Development Kit，开发套件，安装之后可以编译 Java 程序。 JRE = Java Runtime Environment，运行时环境，安装后可以运行 Java 程序。\n现在基本上安装 JDK 时里面会带一份 JRE，不然光让开发不让运行也太蠢了。\n一些基本的 JDK 指令：\njavac: compiler java: launcher for Java applications javadoc: API documentation generator jar: manages JAR files jdb: Java debugger 有关抄袭和剽窃 不要直接复制别人的代码。在你被允许使用他人代码时（例如 Mini Project 使用老师给的代码），应该写明 Javadoc 注释说明代码来源。\nJava 编程基础 最基本的程序结构模板如下：（暂时还没提到面向对象）\n1 2 3 4 5 6 class ClassName { public static void main(String[] args) { // 声明变量和方法 // 写表达式... } } 变量 Java 的变量可以在程序中的任何位置声明。变量声明方法与 C 语言类似。\n1 2 typeName name1, name2, ... namen; typeName name1 = initvalue; 推荐的标识符命名方式：驼峰标记法。变量名使用小驼峰，类名使用大驼峰。\n数据类型 Java 是强类型语言。意思是你声明变量时就需要指定该变量的数据类型。 基本数据类型： 注意：0.2363 类型的数据默认当成 double，0.2363F 才是单精度浮点；86827263927 默认当作 int，86827263927L 才会当作 long。\n每种数据类型在 Java 中都有一个默认值，某些时候 Java 会把变量初始化为默认值。 字符串 String 不是 Java 的基本类型，而是一种对象。\n类型转换 Java 变量类型可自低到高自动无损转换。\n1 byte =\u0026gt; short =\u0026gt; int =\u0026gt; long =\u0026gt; float =\u0026gt; double 反向转换时需要使用强制转换声明（Type Cast）：\n1 2 j = (int)(x + 1.3); // j = 8 i = (int)x + (int)1.3; // i = 7 反向转换时，由于变量范围问题，转换结果可能会被截断或溢出。\n// TODO：写一份转换对照\n保留字/关键字 这些词语不能当作用户标识符（变量名类名方法名等等） 赋值与操作符 基本用法：赋值符还是 =，自增自减运算符 ++ 和 -- 也能用。\n++i 和 i++ 的效果和 C 语言也还是一样的，前者自身作为表达式会返回 i+1，后者自身作为表达式会返回 i，二者都会将 i 本身的值加一。 代数操作符 加减乘除，取模（余数）。 所有代数操作符都可以与赋值符结合在一起用。\n1 2 3 4 5 6 int c = 3; c += 7; // c = c + 7 = ? c -= 5; // c = c - 5 = ? c *= 6; // c = c * 6 = ? c /= 3; // c = c / 3 = ? c %= 3; // c = c % 3 = ? 条件运算符（三元运算符） 比较特殊的一种运算符。\n1 a = (这个条件成立吗 ? 如果成立值就是我 : 否则就是我); 操作符优先级 高优先级操作符优先运算，低优先级运算符后运算。 同优先级的运算符：二元操作符从左向右运算，赋值符从右向左运算。 关系运算符/逻辑运算符 和 C 语言类似。 控制结构 选择结构：if、if else、switch 循环结构：while、do while、for\n1 2 3 4 5 6 if (这里的条件成立) { System.out.println(\u0026#34;passed\u0026#34;); } else { System.out.println(\u0026#34;failed\u0026#34;); } switch 结构和 C 类似，需要写 break，否则会一直执行下面的东西。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 char grade = \u0026#39;a\u0026#39;; switch (grade) { case \u0026#39;a\u0026#39;: System.out.println(\u0026#34;excellent\u0026#34;); break; case \u0026#39;b\u0026#39;: System.out.println(\u0026#34;good\u0026#34;); break; case \u0026#39;c\u0026#39;: System.out.println(\u0026#34;not bad\u0026#34;); break; case \u0026#39;d\u0026#39;: System.out.println(\u0026#34;bad\u0026#34;); break; default: System.out.println(\u0026#34;no such grade!\u0026#34;); } for 循环跟 C 的写法很类似\n1 2 3 for (int i = 0; i \u0026lt; 3; i++) { System.out.println(“i = ” + i); } do while 会先做那段代码块里的东西，再判断条件如何。也就是不管什么情况都会先把代码块里的东西执行一次。while 用法和 C 语言类似。\n1 2 3 4 5 6 7 8 9 10 11 do { System.out.println(\u0026#34;i = \u0026#34; + i); i++; } while (i \u0026lt; 3); // 或者 int i = 0; while (i \u0026lt; 3) { System.out.println(\u0026#34;i = \u0026#34; + i); i++; } break：退出当前循环 continue：结束当前循环轮次 和 C 语言类似\nJava 支持内层和外层循环标记，标记后可以在内层直接退出外层循环。 通过 outer inner 标记内层和外层循环，跳出（continue 或 break）时选择跳出外层循环。 如图： 面向对象基础 Java 是面向对象的编程语言，最基础的实体是类（Class）。\n面向对象的设计方法将代码拆分为由类规划的对象。对象拥有自己的属性和方法。\n例如，我要设计一个系统，该系统和动物、动物行为相关。 现在我要设计小猫。 小猫拥有小猫的属性（Properties）：例如毛色、年龄，名字、体重等等，这些数据是和这只小猫相关的，一只小猫可以拥有这些数据作为属性。 小猫拥有小猫的动作（方法（Methods））：例如叫、吃东西、跑步等等。小猫可以拥有这些东西作为函数方法，也就是这只小猫的动作。\n我们通过编写一个类来定义这个对象到底拥有什么。类就像是一个饼干模具，决定了饼干长什么样子，但是类本身并不是一个饼干。 一旦类编写好了，我们通过代码，基于类来创建对象（Object）。创建出的对象拥有这个类定义的全部内容。每个属性和方法现在都将属于这个对象而非类（除非声明为静态方法/属性）。\n为了减少代码量，增加可维护性，类是可以进行继承的。例如，我的代码要拓展到动物，那么我定义一个类叫动物，让接下来的小猫、小狗都继承这个动物。这样，一旦后续需求变动（例如，需要为所有动物都添加“身高”的属性），那么我直接修改动物类的属性，继承了其的小猫小狗等都会拥有这个身高属性，就不需要一个一个修改这么多具体动物的类了。\n面向对象更加灵活，还支持多态、重载和重写。\n动物类实现了方法移动，但是狗可以跑，而兔子可以跳，这时狗和兔子就可以分别 重写（Override） 自己的移动方法，实现自己的逻辑。狗的跑只需要跑的距离作为参数，而兔子的跳需要跳的距离和高度两个参数，于是就可以在狗和兔子中分别重载（Overload） 这个方法，使其能够接收不同的参数，返回不同的内容，抛出不同的异常等等。 注意重载方法必须修改参数，不允许不动参数只动返回，那就报错了。 面向对象还支持多态。继承了动物的小猫既是动物，也是小猫。当我使用 Animal cat = new Cat() 将一个 Cat 送进 Animal 类型的变量时，他就既具有动物的特性也有小猫的特性，这里我们后面再聊。\n还有更多特性：构造函数等等\u0026hellip; 我们先有一个整体、宏观的印象，具体的东西我们后面再聊。\n","date":"2024-08-07T00:00:00Z","permalink":"https://katomelon.github.io/blog/p/java-w1/","title":"JAVA高级语言程序设计 - Week1概念整理"},{"content":"注意下：如果你不是电管专业的，那么你可能需要的并不是这个文档。这个文档是电管选修课“数据库技术与应用”的资料整理。其他专业的数据库似乎是英方课。\n上面是习题，往下翻是实验报告。\n习题及答案整理 一 简述数据库管理系统的主要功能 二 三 四 五 六 七 八 九 十 十一 背景习题 3.3.1 在上一题\n十二 十三（上机实验三） 实验报告一 \u0026amp; 二 本机主机名与 IP 地址（作业要求） 指令 ipconfig -all 会查询到本机所有的网络状态。 本机的校园网连接信息为： 实验 1-1 在本机（WIndows）安装 MySQL Community 8.3 下载与安装 下载 MySQL Community Server 8.3 版本的 msi 安装包 我选择 Custom 安装模式，修改安装位置将 MySQL 安装到 F 盘下。 安装完成后允许打开 Configurator，把数据目录也配置到 F 盘下。 配置本机为开发用设备，设置端口号 3306， 配置用户。除了根用户外我额外配置了一个以我学号和姓名命名的 DB Admin 角色。 其他设置（服务等）保持默认。Configurator 自动开始初始化。 安装完成。 检查安装是否成功 通过命令行，以我的姓名学号用户身份连接该 MySQL 数据库： （上面那条报错是 PowerShell 不允许把当前目录下的 exe 以指令形式执行，忘了这茬事了）\n连接正常，安装成功。\n实验 1-2 在本机（Windows）安装 PostgreSQL 作业要求：\n作业 1：上机安装一个 DBMS，建议使用 mySQL 或华为 openGauss\n根据华为官网消息，OpenGauss 数据库内核为 PostgreSQL，而后者使用 BSD 开源，且社区相对更完善。本次实验选择安装 PostgreSQL 作为实验内容。 下载与安装 下载 Postgres for Windows 安装包。 把安装目录切到 F 盘\n确认安装所有组件（之前我装过一次，PgAdmin 之类的还留在本机上） 数据目录也放 F 盘 为根用户设置密码，设置服务端口为 5432。 不配置本地化，直接一路默认 Next。开始安装。 安装完毕。 检查安装是否成功 pg 提供了一个 SQL Shell 供我们测试。\n安装成功。\n实验 1-3 在 Linux 云服务器（CentOS 7）上通过 Docker 部署运行 MariaDB 有一台 2h2g 的轻量云服务器，公网 IP 为 （censored）。实验在该机器上安装 MariaDB。 MariaDB 作为 MySQL 的一个分支，旨在规避 MySQL 被 Oracle 闭源的风险，保持了开源和社区驱动的特性，具有性能强大、稳定可靠的优点，支持更多存储引擎并拥有活跃的社区支持。而 Docker 作为轻量级的容器技术，可以将应用程序及其依赖项打包到一个可移植的容器中，实现快速部署、跨平台运行、隔离性和快速启动停止，适用于微服务架构和持续集成部署。是现代运维比较常用的一个方案。 查询可用的 Docker 镜像：\n拉取 Docker Hub 社区镜像。\n起容器。映射到 3306 上。\n放开一下 3306 端口 检查下容器运行情况 容器运行正常，安装完毕。\n检查是否安装成功 用 Navicat 点一下\n连接没有问题。 检查一下 默认表存在。 安装成功。 （安全起见，这个容器在确认安装正常后就已经销毁了）\n实验报告二 实验二要求将习题 2.4.1 中的数据存储到数据库中。该习题的数据如下。 实验一中安装了三种数据库，但是因为都是关系型数据库，所以操作上大体没有差别。本次实验使用 MySQL 操作。\n实验 2-1 通过命令行 SQL 语句操作数据库 添加包含自己学号和姓名的表 系作业要求。编写 SQL 语句建表：\n1 2 3 4 CREATE TABLE Students ( StudentID BIGINT(10), Name NVARCHAR(10) ); 插入我的学号和姓名：\n1 INSERT INTO Students (StudentID, Name) VALUES (123456789, \u0026#39;censored\u0026#39;); 建库建表，数据定义 首先建库建表。编写 SQL 语句如下： （题目中未指定主键，不创建 pk）\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 -- 创建以学号姓名命名的数据库 CREATE DATABASE xxxx; USE xxxx; -- 创建Product表 CREATE TABLE Product ( maker CHAR(1), model INT(4), type ENUM(\u0026#39;laptop\u0026#39;, \u0026#39;pc\u0026#39;, \u0026#39;printer\u0026#39;) ); -- 创建PC表 CREATE TABLE PC ( model INT(4), speed DECIMAL(4, 2), ram INT(4), hd INT(3), price INT(4) ); -- 创建Laptop表 CREATE TABLE Laptop ( model INT(4), speed DECIMAL(4, 2), ram INT(4), hd INT(3), screen DECIMAL(3, 1), price INT(4) ); -- 创建Printer表 CREATE TABLE Printer ( model INT(4), color BOOLEAN, type ENUM(\u0026#39;ink-jet\u0026#39;, \u0026#39;laser\u0026#39;), price INT ); 执行建库建表： 插入数据 编写 SQL 语句用于插入数据。题目里的数据太多，对每个表都先插入前 15 条。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 INSERT INTO Product (maker, model, type) VALUES (\u0026#39;A\u0026#39;, 1001, \u0026#39;pc\u0026#39;), (\u0026#39;A\u0026#39;, 1002, \u0026#39;pc\u0026#39;), (\u0026#39;A\u0026#39;, 1003, \u0026#39;pc\u0026#39;), (\u0026#39;A\u0026#39;, 2004, \u0026#39;laptop\u0026#39;), (\u0026#39;A\u0026#39;, 2005, \u0026#39;laptop\u0026#39;), (\u0026#39;A\u0026#39;, 2006, \u0026#39;laptop\u0026#39;), (\u0026#39;B\u0026#39;, 1004, \u0026#39;pc\u0026#39;), (\u0026#39;B\u0026#39;, 1005, \u0026#39;pc\u0026#39;), (\u0026#39;B\u0026#39;, 1006, \u0026#39;pc\u0026#39;), (\u0026#39;B\u0026#39;, 2007, \u0026#39;laptop\u0026#39;), (\u0026#39;C\u0026#39;, 1007, \u0026#39;pc\u0026#39;), (\u0026#39;D\u0026#39;, 1008, \u0026#39;pc\u0026#39;), (\u0026#39;D\u0026#39;, 1009, \u0026#39;pc\u0026#39;), (\u0026#39;D\u0026#39;, 1010, \u0026#39;pc\u0026#39;), (\u0026#39;D\u0026#39;, 3004, \u0026#39;printer\u0026#39;); 1 2 3 4 5 6 7 8 9 10 11 12 13 14 INSERT INTO PC (model, speed, ram, hd, price) VALUES (1001, 2.66, 1024, 250, 2114), (1002, 2.10, 512, 250, 996), (1003, 1.42, 512, 80, 478), (1004, 2.80, 1024, 250, 649), (1005, 3.20, 512, 250, 630), (1006, 3.20, 1024, 320, 1049), (1007, 2.20, 1024, 200, 510), (1008, 2.20, 2048, 250, 770), (1009, 2.00, 1024, 250, 650), (1010, 2.80, 2048, 300, 770), (1011, 1.86, 2048, 160, 959), (1012, 2.80, 1024, 160, 649), (1013, 3.06, 512, 80, 529); 1 2 3 4 5 6 7 8 9 10 11 INSERT INTO Laptop (model, speed, ram, hd, screen, price) VALUES (2001, 2.00, 2048, 240, 20.1, 3673), (2002, 1.73, 1024, 80, 17.0, 949), (2003, 1.80, 512, 60, 15.4, 549), (2004, 2.00, 512, 60, 13.3, 1150), (2005, 2.16, 1024, 120, 17.0, 2500), (2006, 2.00, 2048, 80, 15.4, 1700), (2007, 1.83, 1024, 120, 13.3, 1429), (2008, 1.60, 1024, 100, 15.4, 900), (2009, 1.60, 512, 80, 14.1, 680), (2010, 2.00, 2048, 160, 15.4, 2300); 1 2 3 4 5 6 7 8 INSERT INTO Printer (model, color, type, price) VALUES (3001, true, \u0026#39;ink-jet\u0026#39;, 99), (3002, false, \u0026#39;laser\u0026#39;, 239), (3003, true, \u0026#39;laser\u0026#39;, 899), (3004, true, \u0026#39;ink-jet\u0026#39;, 120), (3005, false, \u0026#39;laser\u0026#39;, 120), (3006, true, \u0026#39;ink-jet\u0026#39;, 100), (3007, true, \u0026#39;laser\u0026#39;, 200); 实验 2-2 使用 Navicat 查看数据库 连接到数据库： 查看数据库中的表： 可以看到数据已经成功插入。\n实验报告三 实验设计背景 北邮人论坛是北京邮电大学的校园论坛，成立于 2003 年 9 月 26 日 ，由 seasir 和 chit 担任创始站长 ，并由北邮人团队负责维护和运营，是北邮学生、校友和教职工交流信息、分享经验、讨论问题的核心平台。现已经成为北邮校内最大的信息交流平台 ，在北邮及周边学校中拥有较为固定的使用人群，在高校论坛里十分火爆，人气颇高。论坛不仅是一个信息交流的平台，也是北邮人社交和文化生活的重要组成部分。\n实验仿照北邮人论坛，设计一个简易版本的论坛雏形，用于完成基本的论坛职能。\n数据关系设计 从整体上来看，此业务可大致拆分为如下交互实体：\n用户，有如下属性：是否为管理员，真实姓名，论坛昵称，学号，用户名，密码，发过什么帖子，发过什么回复，注册时间。 版块，有如下属性：版块名称，版块简介，版块管理员，版块下有帖子，一个帖子只能在一个版块下。一个版块可以有多个管理员，一个管理员也可以是多个版块的管理。 帖子，有如下属性：帖子标题，帖子正文，楼主，帖子有什么回复。 回复，有如下属性：回复发送者，回复的是什么帖子，回复内容。 当然北邮人论坛还有很多功能如点赞、首页推荐等，本次实验设计不涉及，只做一个最简化版本的实现。我们结合实际业务需求，可以设计出如下表关系：\n用户（Users）\n属性： UserID：用户唯一标识符，主键，自增。 IsAdmin：布尔值，表示用户是否为管理员。 RealName：用户的真实姓名。 Nickname：用户在论坛上的昵称。 StudentNumber：用户的学号，唯一。 Username：用户的用户名，唯一。 Password：用户的密码。 RegisterTime：用户的注册时间。 关系： 一个用户可以发多个帖子（Posts 表中 AuthorID 外键）。 一个用户可以发多个回复（Replies 表中 AuthorID 外键）。 版块（Forums）\n属性： ForumID：版块唯一标识符，主键，自增。 ForumName：版块名称。 ForumDescription：版块简介。 ForumAdminID：版块的管理员（可以在 ForumAdmins 表中定义多个管理员）。 关系： 一个版块可以包含多个帖子（Posts 表中 ForumID 外键）。 一个版块可以由多个管理员管理（通过 ForumAdmins 表实现多对多关系）。 版块管理员（ForumAdmins）\n属性： ForumAdminID：版块管理员关系的唯一标识符，主键，自增。 UserID：用户唯一标识符，外键，引用 Users 表中的 UserID。 ForumID：版块唯一标识符，外键，引用 Forums 表中的 ForumID。 关系： 一个管理员可以管理多个版块。 一个版块可以有多个管理员。 帖子（Posts）\n属性： PostID：帖子唯一标识符，主键，自增。 Title：帖子标题。 Content：帖子正文。 AuthorID：发帖人，外键，引用 Users 表中的 UserID。 ForumID：所属版块，外键，引用 Forums 表中的 ForumID。 PostTime：发帖时间。 关系： 一个帖子只能在一个版块下（通过 ForumID 外键）。 一个帖子可以有多个回复（Replies 表中 PostID 外键）。 回复（Replies）\n属性： ReplyID：回复唯一标识符，主键，自增。 Content：回复内容。 AuthorID：回复发送者，外键，引用 Users 表中的 UserID。 PostID：回复的帖子，外键，引用 Posts 表中的 PostID。 ReplyTime：回复时间。 关系： 一个回复只能对应一个帖子（通过 PostID 外键）。 一个回复由一个用户发送（通过 AuthorID 外键）。 补充说明 每个用户都有唯一的 UserID，StudentNumber，和 Username。 每个版块都有唯一的 ForumID。 每个帖子都有唯一的 PostID。 每个回复都有唯一的 ReplyID。 每个版块可以有多个管理员，通过中间表 ForumAdmins 实现。 每个管理员可以管理多个版块，同样通过 ForumAdmins 表实现。 ER 图 此业务的 ER 图描述如下：\n建表 SQL 语句 我们结合数据设计和各字段属性的域、存储长度等，可写出如下 SQL 建表语句：\n建表设计 用户表（Users） 此表存储论坛用户的信息。\n1 2 3 4 5 6 7 8 9 10 CREATE TABLE Users ( UserID INT PRIMARY KEY AUTO_INCREMENT, IsAdmin BOOLEAN NOT NULL, RealName VARCHAR(100) NOT NULL, Nickname VARCHAR(100) NOT NULL, StudentNumber VARCHAR(50) UNIQUE NOT NULL, Username VARCHAR(50) UNIQUE NOT NULL, Password VARCHAR(255) NOT NULL, RegisterTime DATETIME NOT NULL ); 版块表（Forums） 此表存储论坛的版块信息。\n1 2 3 4 5 6 7 CREATE TABLE Forums ( ForumID INT PRIMARY KEY AUTO_INCREMENT, ForumName VARCHAR(100) NOT NULL, ForumDescription TEXT, ForumAdminID INT, FOREIGN KEY (ForumAdminID) REFERENCES Users(UserID) ); 版块管理员表（ForumAdmins） 此表存储版块的管理员信息，一个版块可以有多个管理员，一个管理员也可以管理多个版块。\n1 2 3 4 5 6 7 CREATE TABLE ForumAdmins ( ForumAdminID INT AUTO_INCREMENT PRIMARY KEY, UserID INT NOT NULL, ForumID INT NOT NULL, FOREIGN KEY (UserID) REFERENCES Users(UserID), FOREIGN KEY (ForumID) REFERENCES Forums(ForumID) ); 帖子表（Posts） 此表存储论坛的帖子信息。\n1 2 3 4 5 6 7 8 9 10 CREATE TABLE Posts ( PostID INT PRIMARY KEY AUTO_INCREMENT, Title VARCHAR(200) NOT NULL, Content TEXT NOT NULL, AuthorID INT NOT NULL, ForumID INT NOT NULL, PostTime DATETIME NOT NULL, FOREIGN KEY (AuthorID) REFERENCES Users(UserID), FOREIGN KEY (ForumID) REFERENCES Forums(ForumID) ); 回复表（Replies） 此表存储对帖子回复的信息。\n1 2 3 4 5 6 7 8 9 CREATE TABLE Replies ( ReplyID INT PRIMARY KEY AUTO_INCREMENT, Content TEXT NOT NULL, AuthorID INT NOT NULL, PostID INT NOT NULL, ReplyTime DATETIME NOT NULL, FOREIGN KEY (AuthorID) REFERENCES Users(UserID), FOREIGN KEY (PostID) REFERENCES Posts(PostID) ); 实践建库 \u0026amp; 建表 创建一个叫数据库课设论坛的数据库，用来做这次上机作业。\n开始建表 建表完成。 用 Navicat 可视化看一下表结构： 建表成功，实验完成。\n实验报告四 准备实验环境（建库建表，录入数据） 实验一没把数据录完，这里重新录入。\n实验使用的数据来自习题 2.4.1。其数据模式如下：\nProduct(maker,model,type)\nPC(model,speed,ram,hd,price)\nLaptop(model,speed,ram,hd,screen,price)\nPrinter(model,color,type,price)\n将题目数值录入电子化表格如下： （电子表格忽略了浮点数值的小数点后的尾部 0，通过 SQL 录入时不受此处影响）\nmaker model type A 1001 pc A 1002 pc A 1003 pc A 2004 laptop A 2005 laptop A 2006 laptop B 1004 pc B 1005 pc B 1006 pc B 2007 laptop C 1007 pc D 1008 pc D 1009 pc D 1010 pc D 3004 printer D 3005 printer E 1011 pc E 1012 pc E 1013 pc E 2001 laptop E 2002 laptop E 2003 laptop E 3001 printer E 3002 printer E 3003 printer F 2008 laptop F 2009 laptop G 2010 laptop H 3006 printer H 3007 printer model speed ram hd price 1001 2.66 1024 250 2114 1002 2.1 512 250 995 1003 1.42 512 80 478 1004 2.8 1024 250 649 1005 3.2 512 250 630 1006 3.2 1024 32 1049 1007 2.2 1024 200 510 1008 2.2 2048 250 770 1009 2 1024 250 650 1010 2.8 2048 300 770 1011 1.86 2048 160 959 1012 2.8 1024 160 649 1013 3.06 512 80 529 model speed ram hd screen price 2001 2 2048 240 20.1 3673 2002 1.73 1024 80 17 949 2003 1.8 512 60 15.4 549 2004 2 512 60 13.3 1150 2005 2.16 1024 120 17 2500 2006 2 2048 80 15.4 1700 2007 1.83 1024 120 13.3 1429 2008 1.6 1024 100 15.4 900 2009 1.6 512 80 14.1 680 2010 2 2048 160 15.4 2300 model color type price 3001 TRUE int-jet 99 3002 FALSE laser 239 3003 TRUE laser 899 3004 TRUE int-jet 120 3005 FALSE laser 120 3006 TRUE int-jet 100 3007 TRUE laser 200 编写 SQL 语句如下：\n注意：设计时，将 price 项的 Domain 视作两位小数的浮点数，因为理论上钱可以精确到人民币分/美分等，不能排除后续插入的产品的价格不是浮点数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 -- 创建数据库 CREATE DATABASE IF NOT EXISTS DatabaseLabFour; USE DatabaseLabFour; -- 创建表Product CREATE TABLE Product ( maker VARCHAR(255), model INT PRIMARY KEY, type VARCHAR(255) ); -- 插入数据到Product表 INSERT INTO Product (maker, model, type) VALUES (\u0026#39;A\u0026#39;, 1001, \u0026#39;pc\u0026#39;), (\u0026#39;A\u0026#39;, 1002, \u0026#39;pc\u0026#39;), (\u0026#39;A\u0026#39;, 1003, \u0026#39;pc\u0026#39;), (\u0026#39;A\u0026#39;, 2004, \u0026#39;laptop\u0026#39;), (\u0026#39;A\u0026#39;, 2005, \u0026#39;laptop\u0026#39;), (\u0026#39;A\u0026#39;, 2006, \u0026#39;laptop\u0026#39;), (\u0026#39;B\u0026#39;, 1004, \u0026#39;pc\u0026#39;), (\u0026#39;B\u0026#39;, 1005, \u0026#39;pc\u0026#39;), (\u0026#39;B\u0026#39;, 1006, \u0026#39;pc\u0026#39;), (\u0026#39;B\u0026#39;, 2007, \u0026#39;laptop\u0026#39;), (\u0026#39;C\u0026#39;, 1007, \u0026#39;pc\u0026#39;), (\u0026#39;D\u0026#39;, 1008, \u0026#39;pc\u0026#39;), (\u0026#39;D\u0026#39;, 1009, \u0026#39;pc\u0026#39;), (\u0026#39;D\u0026#39;, 1010, \u0026#39;pc\u0026#39;), (\u0026#39;D\u0026#39;, 3004, \u0026#39;printer\u0026#39;), (\u0026#39;D\u0026#39;, 3005, \u0026#39;printer\u0026#39;), (\u0026#39;E\u0026#39;, 1011, \u0026#39;pc\u0026#39;), (\u0026#39;E\u0026#39;, 1012, \u0026#39;pc\u0026#39;), (\u0026#39;E\u0026#39;, 1013, \u0026#39;pc\u0026#39;), (\u0026#39;E\u0026#39;, 2001, \u0026#39;laptop\u0026#39;), (\u0026#39;E\u0026#39;, 2002, \u0026#39;laptop\u0026#39;), (\u0026#39;E\u0026#39;, 2003, \u0026#39;laptop\u0026#39;), (\u0026#39;E\u0026#39;, 3001, \u0026#39;printer\u0026#39;), (\u0026#39;E\u0026#39;, 3002, \u0026#39;printer\u0026#39;), (\u0026#39;E\u0026#39;, 3003, \u0026#39;printer\u0026#39;), (\u0026#39;F\u0026#39;, 2008, \u0026#39;laptop\u0026#39;), (\u0026#39;F\u0026#39;, 2009, \u0026#39;laptop\u0026#39;), (\u0026#39;G\u0026#39;, 2010, \u0026#39;laptop\u0026#39;), (\u0026#39;H\u0026#39;, 3006, \u0026#39;printer\u0026#39;), (\u0026#39;H\u0026#39;, 3007, \u0026#39;printer\u0026#39;); -- 创建表PC CREATE TABLE PC ( model INT PRIMARY KEY, speed DECIMAL(4,2), ram INT, hd INT, price DECIMAL(10,2) ); -- 插入数据到 PC 表 INSERT INTO PC (model, speed, ram, hd, price) VALUES (1001, 2.66, 1024, 250, 2114), (1002, 2.1, 512, 250, 995), (1003, 1.42, 512, 80, 478), (1004, 2.8, 1024, 250, 649), (1005, 3.2, 512, 250, 630), (1006, 3.2, 1024, 32, 1049), (1007, 2.2, 1024, 200, 510), (1008, 2.2, 2048, 250, 770), (1009, 2.0, 1024, 250, 650), (1010, 2.8, 2048, 300, 770), (1011, 1.86, 2048, 160, 959), (1012, 2.8, 1024, 160, 649), (1013, 3.06, 512, 80, 529); -- 创建表Laptop CREATE TABLE Laptop ( model INT PRIMARY KEY, speed DECIMAL(3,2), ram INT, hd INT, screen DECIMAL(4,1), price DECIMAL(10,2) ); -- 插入数据到Laptop表 INSERT INTO Laptop (model, speed, ram, hd, screen, price) VALUES (2001, 2.00, 2048, 240, 20.1, 3673.00), (2002, 1.73, 1024, 80, 17.0, 949.00), (2003, 1.80, 512, 60, 15.4, 549.00), (2004, 2.00, 512, 60, 13.3, 1150.00), (2005, 2.16, 1024, 120, 17.0, 2500.00), (2006, 2.00, 2048, 80, 15.4, 1700.00), (2007, 1.83, 1024, 120, 13.3, 1429.00), (2008, 1.60, 1024, 100, 15.4, 900.00), (2009, 1.60, 512, 80, 14.1, 680.00), (2010, 2.00, 2048, 160, 15.4, 2300.00); -- 创建表 Printer CREATE TABLE Printer ( model INT PRIMARY KEY, color BOOLEAN, type VARCHAR(50), price DECIMAL(10, 2) ); -- 插入数据到 Printer 表 INSERT INTO Printer (model, color, type, price) VALUES (3001, TRUE, \u0026#39;int-jet\u0026#39;, 99), (3002, FALSE, \u0026#39;laser\u0026#39;, 239), (3003, TRUE, \u0026#39;laser\u0026#39;, 899), (3004, TRUE, \u0026#39;int-jet\u0026#39;, 120), (3005, FALSE, \u0026#39;laser\u0026#39;, 120), (3006, TRUE, \u0026#39;int-jet\u0026#39;, 100), (3007, TRUE, \u0026#39;laser\u0026#39;, 200); 执行建库建表和数据插入： 数据插入成功。\n习题 6.2.2 查询\u0026amp;查询结果 我们编写 SQL 语句如下。查询结果截图下附。\na) 查询硬盘容量至少 30G 的笔记本电脑制造商及该电脑的速度 1 2 3 4 SELECT Product.maker, Laptop.speed FROM Product JOIN Laptop ON Product.model = Laptop.model WHERE Laptop.hd \u0026gt;= 30; b) 查询制造商 B 生产的任意类型的所有产品的型号和价格 1 2 3 4 5 6 7 8 9 10 11 12 13 14 SELECT Product.model, PC.price FROM Product JOIN PC ON Product.model = PC.model WHERE Product.maker = \u0026#39;B\u0026#39; UNION SELECT Product.model, Laptop.price FROM Product JOIN Laptop ON Product.model = Laptop.model WHERE Product.maker = \u0026#39;B\u0026#39; UNION SELECT Product.model, Printer.price FROM Product JOIN Printer ON Product.model = Printer.model WHERE Product.maker = \u0026#39;B\u0026#39;; c) 查询只卖笔记本电脑不卖 PC 的厂商 1 2 3 4 5 6 7 SELECT DISTINCT maker FROM Product WHERE type = \u0026#39;laptop\u0026#39; AND maker NOT IN ( SELECT maker FROM Product WHERE type = \u0026#39;pc\u0026#39; ); d) 查询出现在两种或两种以上 PC 中的硬盘的大小 1 2 3 4 SELECT hd FROM PC GROUP BY hd HAVING COUNT(DISTINCT model) \u0026gt;= 2; e) 查询每对具有相同速度和 RAM 的 PC 的型号。每一对只能列出一次; 例如，若 (i,j) 已被列出，则 (j,i) 就不能再被列出。 1 2 3 SELECT p1.model AS model1, p2.model AS model2 FROM PC p1 JOIN PC p2 ON p1.speed = p2.speed AND p1.ram = p2.ram AND p1.model \u0026lt; p2.model; 习题 6.3.1 查询\u0026amp;查询结果 a) 找出速度在 3.0 以上的 PC 制造商 方法 1：使用 IN 操作符\n1 2 3 4 5 6 7 SELECT DISTINCT maker FROM Product WHERE model IN ( SELECT model FROM PC WHERE speed \u0026gt; 3.0 ); 方法 2：使用 EXISTS 操作符\n1 2 3 4 5 6 7 SELECT DISTINCT maker FROM Product p WHERE EXISTS ( SELECT 1 FROM PC pc WHERE pc.model = p.model AND pc.speed \u0026gt; 3.0 ); b) 找出价格最高的打印机 方法 1：使用 MAX 函数和子查询对比\n1 2 3 SELECT model, price FROM Printer WHERE price = (SELECT MAX(price) FROM Printer); 方法 2：使用 ALL 操作符\n1 2 3 SELECT model, price FROM Printer p WHERE price \u0026gt;= ALL (SELECT price FROM Printer); 【选作】习题 6.4.6 查询\u0026amp;查询结果 a) 查询 PC 的平均速度： 1 2 SELECT AVG(speed) AS avg_speed FROM PC; b) 查询价格高于 $1000 的笔记本电脑的平均速度： 1 2 3 SELECT AVG(speed) AS avg_speed FROM Laptop WHERE price \u0026gt; 1000; c) 查询厂商“A”生产的 PC 的平均价格： 1 2 3 4 5 6 7 SELECT AVG(price) AS avg_price FROM PC WHERE model IN ( SELECT model FROM Product WHERE maker = \u0026#39;A\u0026#39; AND type = \u0026#39;PC\u0026#39; ); ","date":"2024-08-06T00:00:00Z","permalink":"https://katomelon.github.io/blog/p/db-hw/","title":"数据库 - 作业题目 \u0026 实验报告整理"},{"content":"\n说明 第二题范围为课件 Lecture 5 - Lecture 8，共四节。本题总分 30 分，分为 Part A 和 Part B 两部分，Part A 为简要描述题，需要简要描述给出的概念，并需要给出一些例子。Part B 为讨论/描述题。\nDescribe: Give a clear description that includes all the relevant features - think of it as \u0026lsquo;paintings a picture with words\u0026rsquo; 描述：给出一个清晰的描述，包括所有相关的特征，把它想象成“用文字画一幅画”。 Discuss: Examine, analyse carefully, and present considerations pro and con regarding the problems, or items involved; give a complete and detailed answer. 讨论：仔细检查、分析，并就所涉及的问题或项目提出赞成和反对的考虑；给出一个完整而详细的答案。 Lecture 5 介绍了宏观经济，具体话题为：\n经济衡量指标，经济结构，收入流动，商业周期 Lecture 6 及 7 介绍了商业战略，具体话题为：\n战略，战略分析及工具，战略类型，波特五力 Lecture 8 介绍了市场营销，具体话题为：\n营销的概念，营销组合，市场识别，市场细分 Lecture 5 – Macroeconomics 本节结构梳理 本节介绍了衡量经济的指标（GDP），介绍了收入流动及乘数效应、加速器效应、通胀的概念，简单介绍了商业周期和其他衡量经济的指标。\nMacroconomics deals with the way the economy works at international or national level 宏观经济学研究的是国际或国家层面的经济运行方式 Microeconomics deals with individual organisations and their customers and suppliers 微观经济学研究的是个体组织及其客户和供应商 Macroeconomics 宏观经济学 定义 the branch of economics that studies economic aggregates (grand totals) e.g. the level of prices, output and employment in the economy 翻译 经济学中研究经济总量的一个分支，如经济中的价格、产出和就业水平。 宏观经济学关注的是整个经济。本节聚焦国家层面。\nAggregate demand 总需求 - the total level of spending in the economy 经济中总支出水平 Aggregate supply 总供给 - the total amount of output in the economy 经济中的总产出 The structure of the national economy 国民经济结构 Primary 第一产业 - the production and extraction of natural resources e.g. mining and from agriculture 自然资源的生产和提取 Secondary 第二产业 - output of the manufacturing and construction sectors (e.g. phone makers) 制造和建造业 Tertiary (Services) 第三产业/服务业 - production of services; includes finance, leisure, retail, communications and transport 服务产业，金融，休闲等 Measures of economic structure 衡量 在 GDP 中的占比 雇佣的劳动力占比 对国家国际收支的贡献 定义 Balance of payments - \u0026ldquo;a record of all transactions between domestic consumers and firms and those based overseas\u0026rdquo; 翻译 国际收支——“国内消费者和公司与海外公司之间所有交易的记录” Circular flow of income 收入循环流动 定义 the means by which money circulates in an economy between households and firms 翻译 货币在家庭和企业间流动的方式 The level of wealth within an economy is influenced by the interactions between households, firms and government 一个经济体的财富水平受到家庭、企业和政府之间相互作用的影响\nIt follows that any change in income from employment is directly related to changes in expenditure by consumers. 就业收入的任何变化都与消费者支出的变化直接相关。\nPhillips machine model of the economy The Phillips Machine is actually a physical hydraulic computer that can predict the running of the national economy to within 4% accuracy. 菲利普斯机实际上是一台物理液压计算机，它可以预测国民经济的运行，精度在4%以内。\nMultiplier effect 乘数效应 定义 the addition to total income and expenditure within an economy resulting from an initial injection of expenditure 翻译 在一个经济体中，由于最初的支出注入而增加的总收入和总支出 注入资金影响收入循环流动，首先影响家庭和企业，经济的其他部分随后受间接影响。\nAccelerator effect 加速器效应 定义 When a small change in demand has a much larger effect on investment or supply 翻译 当需求的微小变化会对投资或供给产生大得多的影响时 投资水平取决于国民收入或需求的变化率 因为它取决于变化率，可能会有很大的波动\nInflation 通货膨胀 定义 a rise in the general level of prices of services and goods 翻译 服务和商品价格总水平的上升 同样的钱买到更少商品。\ndemand-pull inflation - 钱进入市场-\u0026gt;乘数效应-\u0026gt;需求增加-\u0026gt;需求拉动型通胀 cost-push inflation - 产品原料涨价-\u0026gt;产品价格升高-\u0026gt;成本推动型通胀 通胀恶性循环：\nBusiness cycle 商业周期 国家经济很少保持稳定。\n撤资导致经济下降，注资导致经济上升 商业周期描述了经济活动的波动 衰退还是繁荣，扩张还是紧缩，发展还是萧条 Other indicators 其他指标 Unemployment rates 失业率 Output levels 产出水平 Average earnings 平均收入 Disposable income 可支配收入 Consumer spending 消费者支出 Inflation rate 通胀率 Interest rates 利率 Overseas trade figures 海外贸易 Exchange rates 汇率 Lecture 6 - Strategic Analysis Two levels of strategies Corporate-level - 企业应当做什么业务，公司的文化和领导结构（在公司合并时很重要） Business-level - 竞争，价值产生，持续竞争优势 Corporate strategy 企业战略 定义 Identification of the purpose of the organisation and the plans and actions to achieve that purpose 翻译 确定组织的目的，以及实现该目的的计划和行动 索尼的例子：\nSony continues to prioritize management with a long-term view, and defines its purpose as to \u0026ldquo;fill the world with emotion through the power of creativity and technology,\u0026rdquo; and its management direction as \u0026ldquo;getting closer to people.\u0026rdquo; 索尼一直以长远的眼光来优先考虑管理，并将其目标定义为“通过创造力和技术的力量让世界充满情感”，将其经营方向定义为“更接近人”。\nPresident and CEO Kenichiro Yoshida explained that people are at the core of the Sony Group\u0026rsquo;s business portfolio, outlined actions to strengthen Group management, and laid out the direction of the evolution of each business. 总裁兼首席执行官吉田健一郎解释说，人是索尼集团业务组合的核心，概述了加强集团管理的行动，并制定了每项业务的发展方向。\nMost organisations begin the strategy formulation process by creating a Mission statement and a Vision statement 大多数组织通过创建使命声明和愿景声明开始战略制定过程：\nMission Statement - 我们的业务是什么，没有时间线，如果没有重大变更可以持续很久 Vision Statement - 我们想成为什么，与目标和未来更相关，通常和成功后的成就相关联 Business Strategy 业务战略 公司战略涉及整个组织集团层面部门级别 商业策略专注于特定的业务、子公司或运营单位 包括仔细考虑它的目的、资源以及它如何与它所处的环境相互作用。 商业战略是实现长期目标的一种手段商业战略。有时被称为“业务单位战略”或“职能战略”。包括：\nmarketing strategies 市场 new product development strategies 新产品开发 human resource strategies 人力 financial strategies 财务 legal strategies 法律 supply-chain strategies 供应链 information technology management strategies IT管理 一些例子：\nGeographic expansion 地理扩张 Diversification 多样化发展 Merger \u0026amp; Acquisition 并购 Market penetration 市场渗透 Retrenchment 裁员 Liquidation 清算 Joint venture 合资 Strategic analysis approaches rational approach 理性分析 - 把战略看作科学合理的过程，使用 SWOT、PESTEL 等技术，主要目标是使利益最大化 flexible approach 灵活分析 - 环境多变复杂的时候，用 SWOT 之类的工具困难且不合理。利益最大化不一定是最优解，环境多变，历史数据可能无关紧要。有一种方法是场景分析，设想若干场景，为每种可能准备应急方案。创业公司喜欢用这种法子，比较灵活。 **creative approach 创意分析 **- 强调想象力，将灵活分析更进一步。取决于管理者想象力创造力。环境变化后，理性分析和灵活分析可能落后。 behavioural approach 行为分析 - 战略本身不是理性过程。可能与利益相关者出现冲突，多方主导者的行为可能影响公司战略。可能影响 SWOT 分析的开展。创业公司常用。 incremental approach 渐进分析 - 长期战略规划不明确，管理者会逐渐适应公司，战略会随外部环境变化，渐进分析提供了明确有效的方法。 lack of strategy 没有战略，被动管理，易出现危机（救火） PESTEL 分析 （前面讲过） SWOT 分析 SWOT 即 Strengths, Weaknesses, Opportunities, Threats.\nStrengths - 内部，专业营销知识，新产品，业务地点，业务流程质量，任何对业务产品/服务有利的方面。 Weaknesses - 内部，缺少营销知识，同质化的产品，业务地点很烂，产品服务质量差。 Opportunities - 外部，发展中的市场，合并 合资 联盟，进入新细分市场，国际市场，竞争者退出。 Threats - 外部，新竞争者，和竞争者打价格战，竞争者有新创新产品，竞争者有牛逼进货渠道，你的产品和服务收大税。 这里 ppt 给了一个对沃尔玛做 SWOT 分析的例子，建议看下，在 ppt 的 35 页开始。\nLecture 7 - Strategic Planning 战略分析用于确定现在业务情况如何，战略规划用于研究未来 5-10 年业务如何。\nStrategic Planning 战略规划 Strategic planning is usually the responsibility of top-level executives at corporate headquarters and senior managers at domestic or foreign subsidiaries 战略规划通常是公司总部的高层管理人员和国内外子公司的高级管理人员的责任。\nStrategic Objectives: clear statement of objectives is needed and a vision of the future direction of the organisation is required. 战略目标:需要明确的目标陈述，并且需要对组织的未来方向有一个愿景。 Strategic Definition: strategy development can be broken down into formulation of different strategic options and then selection. 战略定义:战略发展可以分解为制定不同的战略，然后进行选择。 Strategic Implementation: After strategy development, enactment of the strategy occurs as strategy implementation 战略实施:战略制定后实施 Key Factors in Strategic Planning 关键因素 Competitive Advantage 竞争优势 Adding Value 附加价值 Mass markets or niche markets 大市场或利基市场（缝隙市场的意思） Cost based strategies 基于成本的战略 Market based strategies 基于市场的战略 Types of strategic planning 类型 Growth Plans - 发展计划，有些大公司以及发展到了很难知道未来怎么发展的情况了。需要非常创新型的思维。\nContingency Plans - 应急计划，若事情没有按计划发展的计划。\noutward looking 向外看\ninward looking 向内看\noutward looking 外向型战略 Competitive Advantage 竞争优势 成本优势/价格领导力 **New product development 新产品开发 **领先竞争对手 **Contraction/Expansion 收缩/扩张 **专注于自己的核心竞争力，还是扩展到更大市场 Global 全球化 寻求扩大全球业务 inward looking 内向型战略 Downsizing 减小规模 裁剪/卖掉不需要的业务 **Delayering 减小层级 **扁平化管理结构，加速决策 Restructuring 重构 完全重新思考业务模式 **Reengineering 重工程化 **looking at new and innovative ways of doing things to leverage the organisation\u0026rsquo;s performance 寻找新的和创新的做事方式，以利用组织的绩效 Porter’s 5-force model 波特五力模型 Before any company plans to expand into new markets, divests product lines, acquires new businesses, or sells divisions, it should ask itself, ‘Is the industry/market exciting/attractive?’任何公司在计划开拓新市场、剥离产品线、收购新业务或出售部门之前，都应该问问自己:“这个行业/市场是否令人兴奋/有吸引力?”\n迈克尔·波特(Michael Porter)在1980年开发了一个分析行业内竞争的框架\nBy using this analysis technique, managers can develop their competitive strategy 通过使用这种分析技术，管理者可以制定他们的竞争战略。这种分析的结果有助于管理者意识到机会并克服威胁\n使用四种力量，分析其合在一起后如何影响市场。关注服务客户和赚取利润的能力，五种力量决定了该公司对市场的吸引力。任何一种力量的变化都应当重新评估市场。\nThreat of potential entrants 潜在进入者的威胁 主要受进入壁垒的影响：品牌忠诚度，进入成本（如人员再培训），资本要求，分销渠道，经验曲线/学习曲线，可预期的报复，政府政策等等。可通过预期市场发展、预期利润抵消。\nBargaining power of suppliers 供应商议价能力 受如下因素影响：供应商数量，供应商的客户数量，数量的重要性，投入差异化程度，替代供应的可行性，更换供应商的成本（例如专业设备），企业和供应商的垂直整合能力威胁，相对于销售价格的成本。\n垂直整合（Vertical integration）：例如公司A生产组件，公司B生产产品，公司C负责分销；公司B自己开始生产组件了，这叫后向整合（Backwards integration）；公司B自己开始分销了，这叫前向整合（Forwards integration）；公司B把组件产品分销全包了，这叫全部整合（Full integration）。\nBargaining power of buyers 买家议价能力 受以下因素影响： 有多少客户，客户可选的供应商数量，产品分化性，已有和预期的替代产品，买家数量，买家更换供应的成本，后向整合能力，买家对价格的敏感性。\nThreat of substitutes 替代品的威胁 替代品不一定是相同产品，能满足需求的产品都算（例如俩航空公司打架，结果开火车的给他们客户抢走完了）。受以下因素影响：买家替代的意愿，替代的相关价格，更换的成本，替代品的可用性，perceived level of product differentiation/quality 产品差异化/质量的感知水平\nDegree of rivalry 竞争程度 四力合一。\nLimitations of Porter’s 5-force model 局限性 It assumes relatively static market structures based originally on the economic situation in the eighties 基于80年代初的经济状况 假设市场结构还是静态的 Does not take into account new business models and the dynamism of the industries 没有考虑到新的商业模式和行业的活力 For example, technological innovations and dynamic market entrants from start-ups that will completely change business models within short times. 例如，技术创新和来自初创企业的充满活力的市场进入者将在短时间内彻底改变商业模式。 For instance, the computer and software industry is often considered as being highly competitive. 例如，计算机和软件行业通常被认为是竞争激烈的行业。 Lecture 8 – Marketing Marketing 市场营销 定义 Marketing is the social and managerial process by which individuals and groups obtain what they need and want through creating and exchanging products and values with others. 翻译 市场营销是个人和团体通过创造和与他人交换产品和价值来获得他们需要和想要的东西的社会和管理过程 The Marketing Concept 营销的概念 Needs, wants \u0026amp; demands Human need\n定义 a state of deprivation that you feel 翻译 感受到的一种被剥夺的状态 Human want\n定义 tthe form that a human need takes as shaped by culture and individual personality 翻译 Human want 的形式是由文化和个人个性所塑造的 Demands\n定义 human wants that are backed by buying power 翻译 由购买力支持的 human want Products and services 产品与服务 Product\n定义 anything that can be offered to a market for attention, acquisition, use or consumption that might satisfy a want or need 翻译 任何可以提供给市场以引起注意、获取、使用或消费的东西，可以满足某种欲望或需求 Services\n定义 activities, benefits or satisfactions that are offered for salle\nServices are products that consist of activities, benefits or satisfactions that are offered for sale and are essentially intangible 翻译 供出售的活动、利益或满意程度\n服务是由供销售的活动、利益或满足组成的产品，本质上是无形的 Value, satisfaction and quality Customer value\n定义 the consumer\u0026rsquo;s assessment of the product\u0026rsquo;s overall capacity to satisfy his or her needs 翻译 消费者对产品满足其需求的总体能力的评估 Customer satisfaction\n定义 the extent to which a product\u0026rsquo;s perceived performance matches a buyer\u0026rsquo;s expectations 翻译 产品的感知性能与购买者期望相匹配的程度 产品性能高于顾客预期，顾客高兴，低于顾客预期，顾客伤心\nExchange, transactions and relationships Exchange\n定义 the act of obtaining a desired object from someone by offering something in return 翻译 通过提供某种东西作为回报而从某人那里得到想要的东西的行为 Transaction\n定义 a trade between two parties that involves at least two things of value, agreed-upon conditions, a time of agreement and a place of agreement 翻译 交易双方之间至少涉及两种有价值的东西、约定的条件、约定的时间和约定的地点的交易 Relationship marketing\n定义 the process of creating, maintaining and enhancing strong, value-laden relationships with customers and other stakeholders. 翻译 与客户和其他利益相关者建立、维护和加强强有力的、充满价值的关系的过程。 Market 定义 the set of all actual and potential buyers of a product or service 翻译 一种产品或服务的所有实际的和潜在的购买者 Marketing Mix 营销组合 定义 the set of controllable tactical marketing tools product, price, place and promotion - that the firm blends fo produce the response it wants in the target market 翻译 一套可控的战术营销工具——产品、价格、地点和促销——企业将其混合在一起，以在目标市场产生它想要的反应。 也称作 4 \u0026lsquo;P\u0026rsquo;s：Product、Promotion、Price、Place。又补充了 3 个：People、Process、Physical Environment\n数字时代的新 4P：Personalization、Participation、Peer2Peer、Predictive modelling\nProduct differentiation Core Product\n定义 the problem-solving services or core benefits that consumers are really buying when they obtain a product 翻译 消费者在购买产品时真正购买的解决问题的服务或核心利益 Actual Product\n定义 a product\u0026rsquo;s parts, quality level, features, design, brand name, packaging and other attributes that combine to deliver core product benefits 翻译 产品的部件、质量水平、功能、设计、品牌名称、包装和其他属性结合起来，传递核心产品的好处。 Augmented product\n定义 additional consumer services and benefits built around the core and actual products 翻译 围绕核心产品和实际产品建立的额外消费者服务和利益 例如，买宽带送免费安装\nPotential product\n定义 Potential product - includes all of the features and services that could be envisaged as beneficial to customers, i.e. what can be used to differentiate the product next? 翻译 潜在产品——包括所有可能对客户有益的功能和服务，也就是说，接下来可以用什么来区分产品? Product Classification Consumer product - 买来自己消费 Non-durable product - 不耐用品。用几次就寄了，食品、消耗品等 Durable product - 耐用品，家具啥的 Convenience product - 便利产品，常用，不是很乐意比价，牙膏杂志之类的 Shopping product - 购物产品，很乐意比价，电视啥的 Speciality product - 特色产品，有独特价值，类似劳力士手表 Unsought product - 不需要的产品，比如人寿保险 Industrial product - 工业产品，个人/机构买来进一步加工的产品。例如电脑芯片 Branding 品牌 定义 a name, term, sign, symbol or design, or a combination of these, intended to identify the goods or services of one seller or group of sellers and to differentiate them from those of competitors 翻译 一个名称、术语、标志、符号或设计，或这些的组合，旨在识别一个或一组销售者的商品或服务，并将其与竞争对手的商品或服务区分开来。 Promotion 推销/促销 定义 Activities that communicate the product or service and its merits to target customers and persuade them to buy 翻译 向目标顾客宣传产品或服务及其优点并说服他们购买的活动 定义 Promotion Mix - The specific mix of advertising, personal selling, sales promotion and public relations that a company uses to pursue its advertising and marketing objectives 翻译 促销组合 - 公司为实现其广告和营销目标而使用的广告、个人销售、促销和公共关系的具体组合。 Promotion tools 促销工具 Advertising 广告 Personal selling 个人销售 Sales promotion 促销活动 Public relations 公共关系 Direct marketing 直接营销 Price 价格 定义 the amount of money charged for a product or service, or the sum of the values that consumers exchange for the benefits of having or using the product or service 翻译 为某种产品或服务收取的费用，或消费者为获得或使用该产品或服务的利益而交换的价值总和。 影响价格的因素\n常见的定价方法：\n基于成本 - 制作产品花多少钱 基于价值 - 顾客认为值多少钱 基于竞争 - 动态定价，看其他人（竞争对手）卖多少钱 Place 定义 all the company activities that make the product or service available to target customers 翻译 为目标客户提供产品或服务的所有公司活动 Identifying your target market 确定目标市场 首先需要进行市场切分。一些术语：\nMarket segmentation 定义 dividing a market into distinct groups of buyers with different needs, characteristics or behaviour, who might require separate products or marketing mixes 翻译 将市场划分为具有不同需求、特征或行为的不同买家群体，他们可能需要不同的产品或营销组合。 Mass marketing - 大众市场 Segmented markets - 分割市场 Niche marketing - 利基市场（这是个术语） Micromarketing - 微市场 Mass marketing - 大众市场 定义 using almost the same product, promotion and distribution for all consumers 翻译 对所有消费者使用几乎相同的产品、促销和分销 例如福特车一开始都是黑的。好处是可能可以面向更广大的市场，缺点是不同的客户希望产品能满足更小的需求。\nSegmented markets - 分割市场 定义 adapting a company\u0026rsquo;s offerings so they more closely match the needs of one or more segments 翻译 调整公司的产品，使其更符合一个或多个细分市场的需求。 认识到顾客有不同的需求、观念和购买行为。例如，商业/私人合同和“随用随付”移动电话市场细分。\n好处：市场营销更有效率，通信是面向特定客户的，这将带来更高的营销投资回报 坏处：可能会导致竞争水平降低，因为每个公司可能会确定一个重点细分市场 Niche marketing - 利基市场 定义 adapting a company\u0026rsquo;s offerings to more closely match the needs of one or more subsegments where there is often little competition 翻译 调整公司的产品以更紧密地满足一个或多个细分市场的需求，这些细分市场通常很少有竞争。 利基市场营销人员必须真正了解他们的消费者的需求，这样他们才愿意为他们的产品支付额外的费用。 利基市场允许小公司通过将资源集中在这些利基市场来竞争，这些利基市场可能是大公司所忽视的领域 Micromarketing - 微市场 定义 a form of target marketing in which companies tailor their marketing programmes to the needs and wants of narrowly defined geographic, demographic, psychographic or behavioural segments 翻译 一种目标营销的形式，在这种形式中，公司根据狭窄定义的地理、人口、心理或行为细分市场的需要和需求定制营销计划。 分两种：\nLocal marketing - 定制满足当地特殊群体要求 Individual marketing - 定制，满足个人客户需求，比如定制衣服，定制假期旅行啥的。 细分市场的其他方法（变量） Geographic segmentation Demographic segmentation Psychographic segmentation Behavioural segmentation ","date":"2023-12-31T00:00:00Z","permalink":"https://katomelon.github.io/blog/p/2023-em-question-2/","title":"企业管理 第二题 课件整理"},{"content":"\n说明 第一题范围为课件 Lecture 16 - Lecture 18，共三节。本题总分 30 分，分为 Part A 和 Part B 两部分，Part A 为概念定义题，要求对某概念给出明确定义；Part B 为描述题，要求给出例子并描述。在题干明确要求给出例子的情况下，若作答不给出例子则扣去分数。\nDefine: Clearly explain what a particular term means and give an example, if appropriate, to show what you mean 定义：清楚地解释一个特定术语的意思，如果合适的话，给出一个例子来说明你的意思 Explain: Set out in detail the meaning of something, with reasons. Start by introducing the topic give the ‘How’ or ‘Why’ 解释：详细说明某事的意义，并附有理由。从介绍话题开始，并给出“如何”或“为什么” Lecture 16 及 17 介绍了 HRM（人力资源管理），具体话题为：\n人力资源管理的职能、领导力、激励、商务沟通。 Lecture 18 介绍了企业社会责任，具体话题为：\n企业社会责任、利益相关者、承担社会责任的原因、企业治理、道德、生态责任。 Lecture 16 - HRM Functions, Leadership 本节结构梳理 此课件从人力资源管理讲起，给出了 HRM 的定义，用 Employment Cycle 说明员工在公司中度过的流程，也是 HRM 的具体职能；提出 HRM 是从 Personnel Management 发展起，表示公司重视将人事作为资产对待，支持、鼓励、保留员工。随后给出了 HRM 各个职能的具体解释。随后给出了领导力的定义，给出了三种领导风格，并表示领导风格应根据具体情况变化使用，并给出了一系列领导方法。\nWhat’s the ultimate success factor for a business organisation? Put People First! (For Real)\n何为人力资源管理 定义 Human Resource Management involves all management decisions and actions that affect the nature of the relationship between the organisation and its employees – its human resources 翻译 人力资源管理包含影响组织与其员工（人力资源）之间关系的所有管理决策和行动 Good management of people is essential to the success of any organisation. This is especially true in service industries where happy employees are more likely to give you happy customers\n良好的人力资源管理是企业成功的关键，尤其是服务业，高兴的员工可以带来客户。\nEmployment Cycle 定义 Everyone will experience most or all of the elements of the “Employment Cycle” as defined by Huczynski and Buchanan [2] in their working life 翻译 每个人在其职业生涯中都会经历 huzynski 和 Buchanan 所定义的“就业周期”的大部分或全部要素 HRM Cycle 中的每一部分都是 HRM 的职能。对单个员工来说，可能不会经历此 Cycle 中的全部过程。例如，吃处分了，就被开除了，没有什么升迁和职业规划的部分了。\n助记：招聘，选拔，引导，培训，绩效考核，奖励，处分，职业规划，升迁，结束（解雇等）。\nHRM 发展经历 - 从人事管理到人力资源管理 Personnel Management 于 1980 年代出现，至今很多企业也有。\n定义 The specialist management function responsible for determining and implementing the policies and procedures which determine the stages of the employment cycle 翻译 专门管理职能，负责确定和实施决定就业周期各阶段的政策和程序 During the 1980’s, people began to acknowledge the importance of employees and the concept of human resources, i.e. that they are assets of an organisation, was created\n在20世纪80年代，人们开始认识到员工的重要性，并创造了人力资源的概念，即他们是组织的资产，（所以人事管理变成人力资源管理了）\n设立人力部门不仅是作为实际执行部门，也是将员工看作应给予支持、鼓励、保留的观点。整个公司都该这样看。\nHRM 重要性 定义 The HRM function should be included in the organisational strategyThe Human Resource Manager or Director is now included in the top level management of an organisation e.g. the Board of Directors 翻译 人力资源管理职能应包括在组织战略中人力资源经理或总监现在组织的高层管理中，例如董事会等 We may not be able to attract or retain the personnel necessary to achieve our business objectives\n我们可能无法吸引或留住实现我们业务目标所需的人员\nHRM 职能 Employment Cycle 中的每一环都体现 HRM 职能，上面已经体现了。\n课件中有具体体现的：招聘、选拔、培训和发展、绩效考核、奖励、纪律处分。\nRecruitment 招聘 打广告招人，需要包含：\nJob description - An outline of the role of the job holder Person specification - An outline of the skills and qualities required of the post holder Selection 选拔 定义 The process of assessing candidates and appointing a post holder 翻译 评估候选人并任命一个职位的过程 方法：面试，心理测试，能力测试，技能练习，展示。\nInterview - most common method Psychometric testing - assessing the personality of the applicants Aptitude testing - assessing the skills of applicants In-tray exercise - activity based around what the applicant will be doing, e.g. writing a letter to a disgruntled customer Presentation - looking for different skills as well as the ideas of the candidate Training \u0026amp; Development 培训和发展 即 Progress opportunities 进步的机会\nPersonal Development - 培训等，不一定与工作直接相关 Continuing Professional Development (CPD) 继续专业发展 - 例如专业证书，注册会计师、工程师啥的 Performance Appraisal 绩效考核 考核不是简单地给员工的工作表现打分。绩效考核涉及员工和指定的评估师（Appraiser）。认同自己的优势、劣势以及帮助员工和组织的方法。可以带来潜能的开发、培训、奖励等。\nRewards 奖励 定义 The system of pay and benefits used by the firm to reward and motivate workers 翻译 公司用来奖励和激励工人的工资和福利制度 不止包括钱，还包括什么月度最佳员工、假期，员工福利（车子，医保，旅行）啥的都算。\nEmployment legislation 就业法律 英国的一些法案，其他国家也有类似的：\nEmployment Act 2002 Disability Discrimination Act 2005 Equal Opportunities Act Relevant health and safety regulations Discipline 纪律/处分 企业不能动不动就开除人，受就业法案限制。处理工作冲突有如下程序：\n很多步骤需要和工会等外部机构合作。\nInformal meetings 非正式会议 Formal meetings 正式会议 Verbal warnings 口头警告 Written warnings 书面警告 Grievance procedures 申诉 人力管理的关键角度 Leadership Motivation Communication 人际关系软技能/可转移技能 Leadership 领导力 定义 the process of influencing the activities of an organised group in its efforts toward goal setting and goal achievement 翻译 影响一个有组织群体的活动，使其努力建立目标和实现目标的过程 良好的领导往往是决定一个组织是否有效和成功的主要因素。\n一些成功的领导者：比尔盖茨（Bill Gates）、乔布斯（Steven Jobs）等\nLeadership style 领导风格 三种：Autocratic（独裁）、Participative（参与）、Free-rein（自由）\n好的领导者会使用这三种风格，在合适的时候使用合适的风格。坏领导者什么时候都只用一种。\nAutocratic 独裁 Motivates by threat 用威胁激励人 No confidence or trust in subordinates 不信任下属 Imposes decisions, never delegates 强加决策，从不授权 Makes decisions without consultation 不经讨论就做决定 May be valuable in some types of business where decisions need to be made quickly and decisively. This may need to be used – but has to be used carefully\n在某些需要快速果断地做出决策的业务类型中可能有价值 可能需要使用，但必须谨慎使用\nParticipative/Democratic 参与/民主 Encourages consultation and communication – e.g. team discussions, ideas boards, etc. 鼓励咨询和沟通，例如团队讨论、创意板等。\nEmployees can be motivated by their ability to participate in decisions Decision remains with the leader 员工参与决策的能力会激励他们 决策权掌握在领导者手中\n有说服力（Persuasive） - 领导者做出决定，并试图说服别人这个决定是正确的 协商（Consultative） - 之前有一个协商的过程 laissez faire (free rein) 放手/自由 助记：好处是责任共享，激发创意，激励员工，坏处是耽误时间，方向模糊\nthe leadership responsibilities are shared by all 领导责任共享\nCan be very useful in businesses where creative ideas are important 在需要创意时很重要\nCan be highly motivational, as people have control over their working life 人们可控制工作，高度激励性\nCan make coordination and decision making time consuming and lacking in overall direction 导致决策浪费时间，没有整体方向性\nApproaches to Leadership 领导方法 Leadership of change 变革的领导力 The most challenging aspect of business is leading and managing change 商业中最具挑战性的方面是领导和管理变革 商业环境受到快节奏经济的影响\n社会变革受外部和内部因素的双重影响\nGood leaders are aware of potential changes and create solutions or strategies to deal with these changes\nLeaders must manage and ease any transition caused by change e.g. restructuring, downsizing (redundancies etc.), growth\nMany of the issues are to do with HRM - i.e. people\n好的领导者意识到潜在的变化，并创造解决方案或策略来应对这些变化\n领导者必须管理和缓解任何由变化引起的过渡\n例如重组、缩减规模(裁员等)、增长\n许多问题都与人力资源管理有关，即人\nTrait 特质 定义 The concept that leaders are born as leaders 翻译 领导者天生就是领导者 they possess certain traits that will enable them to become leaders The theory is that the personality of individuals determines whether they will become leaders or not Often seen as gender biased – the concept of the“great man theory”\n他们拥有使他们成为领导者的某些特质 该理论认为，个人的个性决定了他们是否会成为领导者 常被视为性别偏见的“伟人论”概念\nTransformational 定义 a leader who treats relationships with followers in terms of motivation and commitment, influencing and inspiring followers to give more than mechanical compliance and to improve organisational performance 翻译 一位领导者在与追随者的关系中注重激励和承诺，通过影响和激发追随者，使其不仅仅是机械地遵循，而且提升组织绩效。 behavioural theories 行为理论 Imply that leaders can be trained 表示领导者可通过培训（取得） – focus on the way of doing things 注重做事的方式 Structure based behavioural theories 基于结构的行为理论 focus on the way the leader structures work for subordinates – task orientated 基于任务 Relationship based behavioural theories 基于关系的行为理论 focus on the development and maintenance of relationships – process orientated 基于过程 conversational 对话 Focus on reducing negative messages sent out through the everyday actions of the business both externally and, crucially, internally 减少内部（重要）和外部传递的负面信息 Build relationships and sense of belonging and identity with the organisation 在组织中建立关系、归属感和认同感 that gets communicated to customers, etc. 传达给客户 transactional 事务性 定义 a leader who treats relationships with followers in terms of an exchange, giving followers what they want in return for what the leader desires, following prescribed tasks to pursue established goals 翻译 领导者将与追随者的关系视为一种交换，通过给予追随者他们想要的，以换取领导者所期望的，从而完成既定目标的过程。 关注组织管理，关注程序和效率，管理当前问题。\ncontingency 应变 to be more successful, managers need to use different leadership styles at different times depending on the circumstances 不同时候使用不同的领导风格 Suggests leadership is not a fixed series of characteristics that can be transposed into different contexts 表示领导力不是固定特征，可以转换 使用何种风格取决于员工类型，业务文化，自然而然等等。\nFactors affecting leadership style 影响领导力风格的因素 风险 - 决策取决于包含的风险 业务类型 - 创意型，还是需求驱动 变革重要性 企业文化 - 长期持有，难以更改 任务特性 - 需要合作 指导还是结构等等 Lecture 17 - Motivation, Business Communication 本节结构梳理 此课件分为激励与商务沟通两部分。前半部分给出了激励的定义，为什么要有激励，坏激励的结果，关于激励的两个理论（双因素，马斯洛需求层次），给出了激励的因素和基本元素。后半部分给出了沟通的定义，为什么要沟通，沟通的组成及各个部分的明确内容，随后给出了 9 步高效倾听的方法，给出了阻碍组织沟通的原因。\nMotivation 激励 定义 the cognitive decision-making process through which goaldirected behaviour is initiated, energised and directed and maintained 翻译 认知决策过程，通过其启动、激励、引导和维持目标导向行为的过程。 管理者了解激励可影响员工行为\nPoor motivation 坏激励的结果 助记：产品少，产品烂，客户骂，客户走，成本高，员工走，员工骂。\nLower productivity levels 低生产力 Poor quality products and services 产品服务质量差 High levels of complaints from customers 客户投诉 Loss of customers with subsequently lower revenues 客户流失，收入下降 Higher costs 高成本 Higher staff turnover 高员工流失 Poor industrial relations 劳资关系差 助记：团队停，个人死，文化断，沟通乱\nTeams may not function appropriately 团队无法正常工作 Individuals may be placed in positions that donot maximise or utilise their skills 个人无法最大发挥 The culture of the business is not shared 企业文化不流通 Communication and decision making can be affected and mistakes occur 影响沟通决策，犯错 都会影响业务和客户的关系。\nStudies and theories on motivation 关于激励的研究和理论 Herzberg 的双因素影响理论 Hygiene factors 保健因素（一般只带来消极情感） Motivation factors 激励因素（一般与积极有关，有时也涉及消极情感） Maslow 马斯洛需求层次理论 大多数需求与生理心理生存有关\n需求实现从下至上\n自我实现的需要是一个健康个体的主要动机，自我实现意味着个体潜能充分发挥，使其成为所能实现的所有。\nMotivating Factors 激励因素 助记：尊重我，鼓励我，认可我，工作有挑战性，工作有保障，工作有创意，财务安全\nIndividual respect Challenging work Encouragement from management Financial security Opportunities to express creativity Job security Recognition from others Elements of Work Motivation 工作激励的元素 助记：行为，努力，坚持\nDirection of Behaviour - 选择什么行为 Level of Effort - 付出多少努力 Level of Persistence - 遇到困难时，坚持多长时间 Communication 沟通 定义 The communication process involves the transmission of information and the exchange of meaning between at least two people 翻译 沟通过程包括至少两个人之间信息的传递和意义的交换 需要沟通的原因：好沟通能鼓励激励员工，缺少沟通或错误方法消息会导致矛盾，没激励。\n沟通的组成 媒介：口头，纸，电子，图像和视觉 价值：激励，明确管理，反思，协调组织 目标：员工，客户，股东，供应商，本地社区 消息：目标，意图，类型，是否清晰，重要性 阻碍：语言，技术内容，无视反馈，情绪，文化，缺少信任，层次结构 Nine Steps to Effective Listening 九步高效倾听 助记：我也想编，但是这堆东西太勾史了，凑合着记吧。\nFace the speaker and maintain eye contact. 面朝说话者，眼神交互 Be attentive, yet relaxed. 关注且放松 Keep an open mind to the speaker’s message – try to feel what the speaker is feeling. 共感 Listen to the words and try to picture what the speaker is saying. 尝试描绘 Do not interrupt and do not impose your \u0026ldquo;solutions” 不要打断，不要强加 Wait for the speaker to pause to ask clarifying questions only to ensure understanding of something that has been said (avoiding questions that disrupt the speaker\u0026rsquo;s train of thought) 不要打断思路 Give the speaker regular feedback, e.g. summarize, reflect feelings etc. 要有反馈，例如总结或表示情感等 Pay attention to nonverbal cues \u0026ndash; to feelings, tone of voice, inflection, facial expressions, gestures, posture. 注意非语言交流 Be aware of potential barriers that impact your ability to listen effectively. 要注意到自己也存在障碍 Barriers to Effective Organisational Communication 组织高效沟通的阻碍 Organisation Size 组织规模越大越复杂，沟通越难。 助记：位太高，太专业，地理远，层次多\n高级管理者对员工遥远 部门很专业，互相不理解 地理距离导致决策者的隔离 权力层次复杂，信息传递层次多，可能导致失真或阻断 Lecture 18 - Corporate Social Responsibility (CSR) 本节结构梳理 本节从社会责任讲起，介绍了何为社会责任，为什么要承担社会责任，社会责任包含什么，给出了企业的外部代价和外部获益，给出企业各个利益相关者及各自具体的责任考量，讨论了企业为什么要和不要承担社会责任。随后介绍了公司治理与道德、怎样实现道德，以及绿色消费、环保等概念。\nCorporate Social Responsibility (CSR) 企业社会责任 助记：第一个是对小的（周边的人和社区），第二个是对大的（社会，环境，国际，道德）\n定义 A corporation should be held accountable for any of its actions that affect people, their communities and their environment (also known as their stakeholders). It implies that negative business impacts on people and society should be acknowledged and corrected if at all possible 翻译 一个公司应该对其影响人们、他们的社区和环境(也被称为他们的利益相关者)的任何行为负责。这意味着，如果可能的话，应该承认和纠正商业对人和社会的负面影响 定义 The responsibility of an organization for the impacts of its decisions and activities on society and the environment, resulting in ethical behavior and transparency which contributes to sustainable development, including the health and well-being of society; takes into account the expectations of stakeholders; complies with current laws and is consistent with international standards of behavior; and is integrated throughout the organization and implemented in its relations. 翻译 一个组织对其决定和活动对社会和环境的影响所承担的责任，从而导致有助于可持续发展，包括社会的健康和福祉的道德行为和透明度;考虑利益相关者的期望；遵守现行法律，符合国际行为标准；并在整个组织中整合并在其关系中实施。 为什么要承担社会责任 growing belief amongst many people that organisations have a duty to society greater than simply providing goods and services 人们认为企业有社会责任，不只是提供产品和服务 Commercial organisations should have a role in society alongside that of non-commercial entities such as the family 商业企业在社会中、非商业实体中也有作用 Companies that operate in a socially responsible way strengthen their reputations 名誉声誉 决定客户和合作伙伴是否配合，决定在社区中的站位 CSR goes beyond simple philanthropy and is more about corporate behaviour than it is about a company\u0026rsquo;s charitable donation budget 社会责任不是简单的做慈善，是企业行为（做慈善也很重要） 社会责任包含什么 助记：客户，雇员，企业，全部（特殊）群体\nyour goods and services meet customer requirements, and are provided in a fair way 满足客户需求，平等方式提供 your employees are given responsibility and opportunities to work with the organisation in supporting community projects 雇员有责任支持社区项目 the organisation is involved in activities and programmes that support the development of the whole community 公司支持整个社区发展 you involve all members of society by providing them with opportunities rather than marginalising them (e.g. disabled people) 支持全部群体（特殊群体），提供机会，而非边缘化 Costs and benefits 代价和获益 External benefits 外部利益 定义 Benefits that the organisation produces but does not receive any direct income from 翻译 组织产生但不从中获得任何直接收入的利益 例如，给公益捐款，提供市民使用回收设施，摆一个能报时的钟之类的。\nExternal costs 外部代价 定义 Costs that are the output of an organisation’s production processes , but the costs are born by stakeholders, not the organisation 翻译 代价是公司生产过程的产物，但代价由利益相关者承担，而不是公司 例如排放污染。有些立法要求企业对这些承担更多责任，企业需要把这些变成内部代价，或者减少外部代价。例如，减少排放（可能导致的内部代价就是生产成本增加），或者植树，抵消污染。\nStakeholders 利益相关者 定义 A person with an interest or concern in something 翻译 对某事有兴趣或关心的人 例如，公司老板算是利益相关者，公司周边可能受影响的人也是利益相关者。\nCustomers 客户 客户想知道产品怎样制作，想知道产品包含什么成分。\n负责任的公司不会使用不道德手段吸引潜在用户购买他们不喜欢的产品。例如不需要的昂贵手机号，不经用户同意欺骗其更换到另一公司等。\nEmployees 雇员 社会会把雇员行为上升到公司。 不道德的劳动环境等等，例如侵犯工人权利。 UK’s Best Workplace 2022 (1,000+ employees) [4]\nSalesforce CISCO Hilton Local communities 本地社区 Many organisations have recognised that having a strong corporate image within their local community is vital for success 许多组织已经认识到，在当地社区拥有强大的企业形象对成功至关重要\n市场导向型企业倾向于让自己看起来像好邻居，会办些赞助，慈善，回收啥的。\n讨论：是无私风险导致的还是利益导致的。\nIntermediaries 中介 Wholesalers 批发商 Retailers 零售商 Agents/franchisees 经销商 他们和客户、雇员的担心一致，需要考虑他们的担心。例如商业道德可能影响他们。\n例子：麦当劳在很多方面被批评，例如产品对健康的影响，怎样对待员工，对环境的影响。在19年被起诉，因为延长工作时间导致工人工作到深夜，伤害工人健康，安全培训不足，母公司对特许公司和员工的责任问题等。\n反例：大公司会收到流言干扰。麦当劳曾被错误认为用蚯蚓袋鼠牛眼犰狳肉做汉堡。\nGovernment 政府 政府受社会压力影响，社会受企业外部代价影响。企业需要关注哪些行为是社会不可接受的，考虑是否需要减少，否则可能会被政府规章限制。\n快速回应会带来好的印象，我们对大众负责，不用等到政府拿规章来管我们。\nSuppliers 供应商 好的供应商是成功的关键。\nTaking into account the needs and views of suppliers is a combination of shrewd business sense and good ethical practice 好的供应商是成功的关键考虑到供应商的需求和意见是精明的商业意识和良好的道德实践的结合 组织还应了解其供应商的做法，以确保他们也符合道德标准 供应商的不良做法可能会导致给你的组织带来负面形象 = 失去客户 例子：In 2008, the fashion chain Primark sacked three of its suppliers in India after an investigation for the BBC\u0026rsquo;s Panorama and The Observer uncovered children labouring in Indian refugee camps to produce some of its cheapest garments. The speed at which Primark acted meant that its standing in the high street remained secure at the time, its reputation repaired before many of its customers even noticed it was tarnished [6]\n2008年，英国广播公司(BBC)《全景》(Panorama)和《观察家报》(the Observer)的一项调查发现，印度难民营里有童工为Primark生产一些最便宜的服装，随后，时装连锁店Primark解雇了三家印度供应商Primark迅速采取行动，这意味着它在大街上的地位在当时是稳固的，在许多顾客甚至没有注意到它的声誉受损之前，它的声誉就已经恢复了。[6]\n快时尚：服装需求大，公司为满足快时尚，牺牲质量换取数量，将生产转移到东南亚，出现了低工资，童工，化学品使用，缺少健康和安全保障等等。\nFinancial community 金融界 包括支持过你的、正在支持你的、未来可能支持你的机构和个人。\n金融界不喜欢你，你就拿不到投资。很多大企业花大代价说服投资者，他们是成功的、负责任的公司。\n为什么企业【要】承担社会责任？ Altruistic 利他主义上 - the organisation feels that it has a duty to think about the needs of society not simply the need to make a profit 企业认为有责任满足社会需求，不只是挣钱 Pragmatic 务实精神上 - 从社会责任，尤其是环境政策角度上，是增加企业知名度，获取更多收入的机会 大多数企业出于以上两种的综合考量 企业社会责任和消费者需求的变化消费者财富的增加，意味着外部利益对消费者来说变得越来越重要 马斯洛的需求层次理论认为，消费者的动机是他们最低层次的未满足需求 当基本需求得到满足时，更高层次的需求会影响他们的购买行为 为什么企业【不要】承担社会责任？ There are two main criticisms of CSR\nPhilosophical - Companies should concentrate on what they are best at - i.e. making money\nPragmatic - Critics see CSR as being a form of short-term manipulation of stakeholders by companies\n对企业社会责任的批评主要有两种\n**哲学上的：**公司应该专注于他们最擅长的事情——即赚钱\n**务实主义上的：**批评人士认为，企业社会责任是企业短期操纵利益相关者的一种形式\nThe philosophical criticism：公司应该专注于他们最擅长的事情——即赚钱。公司对社会做出贡献的任何尝试都是对其客户征税的一种形式。消费者应该自由地把这笔钱作为“税”花在他们想要支持的东西上，或者政府应该资助所有的社会需求。当企业处于垄断地位，消费者没有选择的时候，这种观点就更为强烈。\nThe pragmatic criticism：实用主义批评批评者认为，企业社会责任是企业对利益相关者进行短期操纵的一种形式。他们认为企业社会责任是为了提高企业绩效，而不仅仅是为了社会利益。公司只支持“时尚”的事业来提升他们的公众形象，而不一定是最值得的事业。例如支持儿童福利慈善机构却忽视心理健康慈善机构。\n其他批评：\nCorporate Governance 公司治理 定义 ensuring that an organisation is run in a responsible manner with due regard to its stakeholders 翻译 确保组织以负责任的方式运作，充分考虑其利益相关者 为什么要搞公司治理 For many years, companies only had to report basic financial information (e.g. profit \u0026amp; loss) As long as the company apparently made a profit, investors were happy In the late 1990s, there was a series of scandals in which executives badly and fraudulently mismanaged companies The financial detail contained in the standard annual reports was insufficient to protect investors from bad management, so Corporate Governance was establishedEBU5402 - Enterprise Management 多年来，公司只需要报告基本的财务信息(如利润和亏损)。只要公司明显盈利，投资者就会高兴。上世纪90年代末，出现了一系列丑闻，高管们对公司的管理不善，而且存在欺诈行为。标准年度报告中包含的财务细节不足以保护投资者免受管理不善的影响，因此建立了公司治理。\n公司治理的关键概念 防止一个或多个个人滥用权力的内部控制制度 - 首席执行官和董事长不应该是同一个人 长期工资与绩效报酬之间的差额 - 高级职员的薪酬方式应该是可见的 承认员工是重要的利益相关者 组织与其审计师之间的关系应该保持一定的距离 - 许多丑闻都是审计人员不尽职的结果 参考英国电信的年度报告，搜索“治理”部分 董事会应包括执行董事和非执行董事 执行董事是一个组织的高级经理 - 执行董事通常是全职的 非执行董事是公司董事会的成员，但不构成执行管理团队的一部分 - 不是公司的雇员，非执行董事通常是兼职的 Ethics vs. Morals Morals 更指代个人，Ethics 指代其应用到的社会系统。也就是说 Ethics 可以作为全体的行为标准，例如国家道德，社会道德，企业道德，职业道德，家庭道德之类的。\nEthics 与 Legal 区分。例如搞广告吸引小孩劝说父母买自家生产的不健康食物，属于不道德但是不算违法。\nBeing ethical premium products 高端产品 以 Ethics 的方式运作会让消费者更愿意购买其产品。例如，消费者更愿意买散养鸡的鸡蛋而不是饲养场的，消费者会为平等交易付钱等等。\ntraining staff 员工培训 不 Ethic 的行为会损害商业，所以要培训员工，让他们了解自己该怎么做，什么是 unethical 的行为。\n例如，要让员工了解自己正在卖的商品，了解用户需求，把自己卖的产品和用户需求联系起来。\nrewards 奖励 使用高效的奖励制度可以阻止员工使用 unethical 的方式谋利。例如提成（Commission-based rewards）制度会让员工更倾向于卖贵的而不是最合适的，是个反例。提成是指基本工资低，根据销售额的百分比发提成。\ntechnology 技术 新技术带来了新道德问题。例如有的企业在库存里用 RFID 标记货物，卖掉之后有人担心这玩意能追踪隐私。\nEcological concerns 环境担忧 消费者越来越担心环境。两个主要的点是：全球变换和资源枯竭。通常认为是企业贪婪导致的，事实上消费者贪婪也是个重要因素。\nglobal warming 全球变暖 通常认为气温在上升，人类活动导致的。结果可能是海平面上升，极端天气，降水改变，农业受影响。不知道会多严重，工业和消费者应该与此抗争。\ndepletion of resources 资源枯竭 很多资源有限，例如石油和煤炭。一旦用完无法再生。其他很多资源被使用的速度大于再生速度，例如某种鱼。资源在被破坏以获取更有利可图的活动。\nEcological responsibility 生态责任 商业一般认为与环保相冲突，市场导向的公司不能忽视客户对环境的关切。即公司也应表现出对环保的关切。\nGreen consumerism 绿色消费 The \u0026lsquo;green\u0026rsquo; (i.e. environmentally aware) consumer can provide businesses with opportunities as well as problems\nThis \u0026lsquo;green\u0026rsquo; demand can be used in marketing strategies e.g. promotion\nBut, this needs to be backed by environmentally sound products and services\nGoing \u0026lsquo;green\u0026rsquo; can save money\nObtaining goods from local suppliers cuts costs in the distribution channel\nUsing recyclable shipping materials may cut costs\n绿色”(即有环保意识的)消费者可以为企业提供机会，也可以为企业带来问题\n这种“绿色”需求可以用于营销策略 - 例如促销 - 但是，这需要以无害环境的产品和服务为后盾\n“绿色环保”可以省钱 - 从当地供应商那里获取货物可以降低分销渠道的成本，使用可回收的运输材料可以降低成本\nLegislation on environmentally sensitive practices has led to increased costs for organisations.Those organisations who acted ahead of the legislation have gained experience and competitive advantage\n对环境敏感的做法的立法导致机构的成本增加。\n那些在立法之前采取行动的组织已经获得了经验和竞争优势\n","date":"2023-12-31T00:00:00Z","permalink":"https://katomelon.github.io/blog/p/2023-em-question-1/","title":"企业管理 第一题 课件整理"},{"content":"\n本章 分两个主要部分：实用AI工具和计算机视觉。前一部分介绍一些框架和工具，内容不多，后一部分介绍计算机视觉，占大头。\n此文档整理总结Week3 Slide中的定义和知识点。\n所有知识点 按PPT内顺序排列。列出PPT和tutorial中所有概念的英文原始释义（若有）和中文翻译，方便背诵。\nscikit-learn 概念 位置 scikit-learn PPT p5-p13 原 Simple and efficient tools for predictive data analysis 译 用于预测数据分析的简单有效的工具 说明 开源免费可商用的数据分析工具，对所有人可用，不限制训练框架（Tensorflow. Pytorch等），支持机器学习方法，数据处理，可视化，基于Numpy、SciPy、matplotlib等库。\n提供分类Classification、回归Regression、聚类Clustering等工具。课件中给出了一些实际的例子。 Tensor 张量 概念 位置 Tensor 张量 PPT p14 原 In deep learning frameworks, they use specialized data structure called Tensor. 译 在深度学习框架中，使用称为张量的专用数据结构。 说明 张量的本质是多维数组（Multidimensional Array）。例如，0阶张量的本质是一个实数或复数，一阶张量是一个矢量，二阶张量是一个矩阵，更高阶数的张量类似，例如三阶张量表示立方体中的数等等。\nGPU与深度学习 概念 位置 GPU与深度学习 PPT p16 原 Deep learning relies on GPUs. 译 深度学习依赖于GPU 说明 严格来说这说法不准确，深度学习依赖密集计算，只不过是跟CPU一比GPU刚好能干这活。GPU毕竟还是专搞图形计算的。\nCPU适用于复杂逻辑控制，例如武器装备，IT等场景；GPU适用于密码学、挖矿、图形计算等需要平行计算的场景。CPU耗能小，GPU耗能高。 TensorFlow 概念 位置 TensorFlow PPT p14-p17 原 An open-source Deep Learning library 译 一款开源的深度学习库 说明 由谷歌贡献。其训练流程：其编程模型可将数字计算（Numeric Computation）过程表达为图表。图表节点为带有数字输入输出的操作，图表的边缘为节点间流动着的张量。 Static Graph \u0026amp; Dynamic Graph 概念 位置 Static Graph \u0026amp; Dynamic Graph PPT p20 \u0026amp; Tutorial Q1 原 The computation process can be view as a graph. 译 计算过程可展示为图表 原 A static graph is one that defines the entire computation graph before performing the computation. When the data is obtained, it is calculated according to the defined calculation graph. 译 静态图是在执行计算之前定义整个计算图的图。获得数据后，根据定义的计算图进行计算。 原 Dynamic graphs, on the other hand, generate computational graphs as they are computed, and the complete graph is known only when the computation is completed. 译 动态图在计算时生成计算图，并且只有在计算完成时才能得到完整的图表。 说明 PyTorch 概念 位置 Pytorch PPT p22 原 Open-source machine learning library 译 开源的机器学习库 说明 由Fackbook AI研究实验室维护，依靠GPU能力，自动计算梯度，更容易测试和开发新产品。\n优点有：类Python（Pythonic）设计理念，简洁；自动计算梯度（Autograd）；有许多已实现好的算法和组件。 Brief History of Computer Vision 概念 位置 Brief History of Computer Vision PPT p31 说明 Human Vision 人类视觉 概念 位置 Human Vision 人类视觉 PPT p42 原 Vision is the process of discovering what is present in the world and where it is by looking 译 视觉是通过观察发现世界上存在什么以及它在哪里的过程 说明 Computer Vision 计算机视觉 概念 位置 Computer Vision 计算机视觉 PPT p42 原 Computer Vision is the study of analysis of pictures and videos in order to achieve results similar to those as by humans 译 计算机视觉是一门对图片和视频进行分析的研究，目的是为了获得与人类（视觉）类似的结果 说明 Image 图像 概念 位置 Image 图像 PPT p44 原 An image is a rectangular grid of data of light values 译 图像是光值（亮度值）数据的矩形网格 说明 此处给出的定义在PPT中指代灰度图像，更严格的定义应该是图像是像素的矩阵网格。像素的值可以是二进制、灰度、颜色或多模态等。（见PPT p48） Main Goal of Computer Vision 概念 位置 Main Goal of Computer Vision PPT p64 原 Need for an algorithm that can write the rules for us so that we don’t have to write it by hand. 译 需要一种可以自己寻找规则的算法，这样就无需手动编写了。 原 Instead of creating object models, hundreds of thousands of pictures were collected from the internet and an algorithm was used. 译 不创建具体的模型，而是从网络上收集大量图片，使用算法（来学习规则） Computer Vision Techniques 概念 位置 Computer Vision Techniques PPT p67 原 Classification, Semantic Segmentation, Object Detection, Instance Segmentation 译 分类，语义分割，目标检测，实例分割 Convolution 卷积 概念 位置 Convolution 卷积 PPT p69 说明 Problems in processing images with fully connected networks：全连接网络图像处理中的问题:Too many parameters in the weight matrix -\u0026gt; overfitting权重矩阵中参数太多-\u0026gt;过拟合Convolutional neural network solution卷积神经网络解决方案Local correlation, parameter sharing本地关联，参数共享卷积的四个基本属性（Basic Properties）： 卷积核（Kernel）、步幅（Stride）、填充（Padding）、通道（Channel）。 Kernel 卷积核 概念 位置 Kernel 卷积核 PPT p71 原 Kernel: also known as receptive field, the sense of convolution operation, intuitively understood as a filter matrix, commonly used convolution kernel size of 3 × 3, 5 × 5 and so on; 译 卷积核：也称感受野，是卷积操作的核心。直观地理解为一个滤波器矩阵，常用的卷积核尺寸有3×3、5×5等。 Stride 步幅 概念 位置 Stride 步幅 PPT p71 原 Stride: the pixels moved by the convolution kernel at each step when traversing the feature map 译 步幅：在遍历特征图时，卷积核在每一步移动的像素数。步幅决定了卷积操作对输入的采样间隔。 Padding 填充 概念 位置 Padding 填充 PPT p71 原 Padding: the way to deal with the boundary of the feature map. To fill the boundary (generally filled with 0), and then perform the convolution operation, which will make the size of the output feature map the same as the size of the input feature map; 译 填充：处理特征图边界的方法。通过在特征图边界填充（通常使用0进行填充），然后进行卷积操作，可以使输出特征图的大小与输入特征图相同。填充有助于保持特征图边缘信息。 Channel 通道 概念 位置 Channel 通道 PPT p71 原 Channel: the number of channels (layers) of the convolution layer. 译 通道：卷积层的通道（层）数 说明 每个通道对输入进行一种特定的卷积操作，多个通道的输出叠加形成最终的输出特征图。 卷积的计算 概念 位置 卷积的计算 PPT p74-p83Tutorial Solution Q4 \u0026amp; Q5 详见专题子页面：(WIP)\nDe-convolution 反卷积 概念 位置 De-convolution 反卷积 PPT p84 原 Equivalent to a transposition computation after converting a convolution kernel to a sparse matrix 译 相当于将卷积核转换为稀疏矩阵后的转置计算 说明 卷积是把图片弄成特征图，反卷积是把特征图弄成图片 Dilated/Atrous Convolution 膨胀卷积 概念 位置 Dilated/Atrous Convolution 膨胀卷积 PPT p86 原 To expand the receptive field, the kernel is \u0026ldquo;inflated\u0026rdquo; by inserting spaces between elements inside the convolutional kernel to form a \u0026ldquo;null convolution\u0026rdquo; (or inflated convolution), and the kernel to be expanded is indicated by the expansion rate parameter L, i.e., L-1 spaces are inserted between the kernel elements. 译 为了扩大感受野，卷积核通过在卷积核内的元素之间插入空白，形成一种“空洞卷积”（或膨胀卷积）。要扩展的卷积核由扩张率参数L指示，即在卷积核元素之间插入L-1个空格。 说明 Pooling 池化 概念 位置 Pooling 池化 PPT p87 原 Pooling is used for regions of the image that do not overlap (this is different from the convolution operation) 译 池化操作用于图像中不重叠的区域（与卷积操作不同） 说明 Flatten 压平 概念 位置 Flatten 压平 PPT p90 原 Flatten refers to the process of taking the two-dimensional feature maps produced by the convolution and pooling layers and transforming them into a one-dimensional vector. 译 \u0026ldquo;压平\u0026quot;指的是将由卷积和池化层产生的二维特征图转化为一维向量的过程。 说明 Fully Connect \u0026amp; Dropout 概念 位置 Fully Connect \u0026amp; Dropout PPT p91 说明 全连接（Fully Connected）：将前一层的所有神经元连接到当前层的每个神经元。用于在神经网络中进行分类或回归。Dropout（丢弃）：在训练期间随机丢弃一部分神经元，防止网络过度依赖某些特定神经元。有助于减少过拟合并提高模型的泛化能力。 Normalization 正交化/归一化 概念 位置 Normalization 正交化/归一化 PPT p92-p93 原 Normalization techniques in Convolutional Neural Networks (CNNs) are crucial for improving training stability, accelerating convergence, and achieving better generalization. 译 在卷积神经网络（CNNs）中，归一化技术对于提高训练稳定性、加速收敛速度以及实现更好的泛化效果至关重要。 说明 Feature extraction 特征提取 概念 位置 Feature extraction 特征提取 PPT p93-p94 原 Feature is a scalar x which is quantitatively describes a property of the Object. 译 特征是一个标量x，定量地描述了对象的属性。 原 A pattern is represented by a set of N features, or attributes, viewed as a N- dimensional feature vector. 译 模式由一组N个特征或属性表示，这些特征或属性被视为N维特征向量。 原 Class is a set of patterns that share some common properties 译 类型是拥有一些公共属性的模式的集合 原 Classification is a mathematical function or algorithm which assigns a feature to one of the classes. 译 分类是一种数学函数或算法，将一个特征分配给一个类型。 说明 Good features:Objects from the same class have similar feature values.同类物体值相同Objects from different classes have different values.不同类物体值不同 Image Classification 图像分类 概念 位置 Image Classification 图像分类 PPT p97 原 Given a set of pixels determine the category of image 译 给定一组像素，确定该图像的类别 Steps to build a computer vision model 概念 位置 Steps to build a computer vision model PPT p98Tutorial Solution Q2 原 Data collection: Collect all possible data that is related to the goal of the model 译 数据收集：收集与模型目标相关的所有可能的数据 原 Data cleaning: Filter thecollected data and remove unclear pictures 译 数据清理：对收集到的数据进行过滤，去除不清晰的图片 原 Data preparation: Resize all the pictures to one common size 译 数据准备：将所有图片调整为一个通用大小 原 Build and train the model: Start coding 译 构建和训练模型：开始写代码 原 classification or recognition (making sense of the visual information) 译 分类或识别：使视觉信息有意义 Deep Learning based Image Classification 概念 位置 Deep Learning based Image Classification PPT p100 说明 Cascade of Non-linear Transformations（非线性变换的级联）：含义： 一系列非线性变换的级联，其中输入数据通过一系列非线性操作或转换，逐渐被映射到更高级别的表示。这在深度学习中很常见，因为深层神经网络的每一层都可以看作对输入数据的一种非线性变换。End-to-End Learning（端到端学习）：含义： 整个系统的学习过程，从输入直接到输出，没有人为设计的中间步骤或特征。在端到端学习中，模型尽可能地从原始输入到最终目标输出进行学习，而无需手工设计中间阶段的特征提取或转换。General Framework (Any Hierarchical Model is Deep)（通用框架，任何分层模型都是深度的）：含义： 深度学习中的一般性框架，其中任何具有分层结构的模型都可以被认为是深度模型。深度模型通常包含多个层次的表示学习，这使得模型能够逐渐学习数据的抽象表示。Q：为什么需要很多层？A：When input has hierarchical structure, the use of a hierarchical architecture is potentially more efficient because intermediate computations can be re-used. DL architectures are efficient also because they use distributed representations which are shared across classes.当输入具有分层结构时，使用分层架构可能更有效，因为中间计算可以被重复利用。深度学习架构也是高效的，因为它们使用分布式表示，这些表示在不同类别之间是共享的。 Performance Metrics 性能指标 概念 位置 Performance Metrics 性能指标 PPT p126 说明 用以评测模型的准确度和精确度。True/False Positive/Negative： 模型正确/错误预测正/负类别的数量。Precision（精确度）：TP/(TP+FP) - what percentage of the positive class is actually positive?表示被模型正确分类为正类别的样本数量与模型所有预测为正类别的样本数量之比。Recall（召回率）： TP/(TP+FN) - what percentage of the positive class gets captured by the model?表示实际正类别样本中被模型正确分类的数量与所有实际正类别样本的数量之比。Accuracy（准确率）：(TP+TN)/(TP+FP+TN+FN) - what percentage of predictions are correct?是模型正确分类的图像数量与总图像数量之比。 Confusion Matrix 混淆矩阵 概念 位置 Confusion Matrix 混淆矩阵 PPT p128 说明 Good for checking where your model is incorrect用于检查模型不正确的地方For multi-class classification it reflects which classes are correlated对于多类别的分类，它反映了哪些类型是相关的 Segmentation 分割 概念 位置 Segmentation 分割 PPT p133 原 Segmentation is the process of breaking an image into groups, based on similarities of the pixels 译 分割是根据像素的相似性将图像分成组的过程 说明 Object Recognition 物体识别 概念 位置 Object Recognition 物体识别 PPT p144Tutorial Solution Q3 原 It is the task of finding and identifying objects in an image or video sequence 译 是在图像或视频序列中寻找和识别物体的任务 说明 步骤：Detection – of separate objectsDescription – of their geometry and positions in 3DClassification – as being one of a known classIdentification – of the particular instanceUnderstanding – of spatial relationships between objects分离物体的检测在3D中描述它们的几何形状和位置分类为已知类别之一特定实例的标识理解物体之间的空间关系应用：It is used in various applications, such as autonomous navigation (recognizing obstacles), augmented reality (overlaying digital information on real-world objects), and robotics (identifying objects for manipulation).它被用于各种应用程序，例如自主导航(识别障碍物)、增强现实(将数字信息覆盖在现实世界的对象上)和机器人(识别用于操作的对象)。 ","date":"2023-12-18T00:00:00Z","permalink":"https://katomelon.github.io/blog/p/intro-to-ai-w3/","title":"人工智能导论 - Week3概念整理"}]