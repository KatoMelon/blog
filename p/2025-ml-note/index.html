<!doctype html><html lang=zh dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="è¯¾ç¨‹å…¨å››å‘¨è¯¾ä»¶çŸ¥è¯†æ•´ç† å®Œæˆäº01.02"><title>æœºå™¨å­¦ä¹  å…¨å››å‘¨ æ¦‚å¿µæ•´ç†</title>
<link rel=canonical href=https://katomelon.github.io/blog/p/2025-ml-note/><link rel=stylesheet href=/blog/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="æœºå™¨å­¦ä¹  å…¨å››å‘¨ æ¦‚å¿µæ•´ç†"><meta property='og:description' content="è¯¾ç¨‹å…¨å››å‘¨è¯¾ä»¶çŸ¥è¯†æ•´ç† å®Œæˆäº01.02"><meta property='og:url' content='https://katomelon.github.io/blog/p/2025-ml-note/'><meta property='og:site_name' content="KatMelon's Blog"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='æœºå™¨å­¦ä¹ '><meta property='article:published_time' content='2025-01-29T00:00:00+00:00'><meta property='article:modified_time' content='2025-01-29T00:00:00+00:00'><meta name=twitter:title content="æœºå™¨å­¦ä¹  å…¨å››å‘¨ æ¦‚å¿µæ•´ç†"><meta name=twitter:description content="è¯¾ç¨‹å…¨å››å‘¨è¯¾ä»¶çŸ¥è¯†æ•´ç† å®Œæˆäº01.02"><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=åˆ‡æ¢èœå•>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/blog/><img src=/blog/img/avatar_hu_ba03ba61d53929d4.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>ğŸ˜™</span></figure><div class=site-meta><h1 class=site-name><a href=/blog>KatMelon's Blog</a></h1><h2 class=site-description>ç»§ç»­ç²¾è¿›.</h2></div></header><ol class=menu-social><li><a href=https://github.com/KatoMelon target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/blog/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>ä¸»é¡µ</span></a></li><li><a href=/blog/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>å½’æ¡£</span></a></li><li><a href=/blog/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>æœç´¢</span></a></li><li><a href=/blog/%E5%85%B3%E4%BA%8E/><svg class="icon icon-tabler icon-tabler-info-circle" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 12a9 9 0 1018 0A9 9 0 003 12"/><path d="M12 9h.01"/><path d="M11 12h1v4h1"/></svg>
<span>å…³äº</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>æš—è‰²æ¨¡å¼</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">ç›®å½•</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#å‰è¨€week1-1>å‰è¨€ã€Week1-1ã€‘</a><ol><li><a href=#æœºå™¨å­¦ä¹ ç»Ÿè®¡å­¦ä¹ çš„å®šä¹‰>æœºå™¨å­¦ä¹ ï¼ˆç»Ÿè®¡å­¦ä¹ ï¼‰çš„å®šä¹‰</a></li><li><a href=#æ•°æ®çš„å®šä¹‰>æ•°æ®çš„å®šä¹‰</a></li><li><a href=#çŸ¥è¯†çš„è¡¨ç¤º>çŸ¥è¯†çš„è¡¨ç¤º</a></li><li><a href=#æ•°æ®ç§‘å­¦>æ•°æ®ç§‘å­¦</a></li><li><a href=#æœºå™¨å­¦ä¹ çš„ä¸¤ç§è§‚ç‚¹>æœºå™¨å­¦ä¹ çš„ä¸¤ç§è§‚ç‚¹</a></li><li><a href=#æœºå™¨å­¦ä¹ çš„ä¸¤ä¸ªé˜¶æ®µ>æœºå™¨å­¦ä¹ çš„ä¸¤ä¸ªé˜¶æ®µ</a></li><li><a href=#åŸºæœ¬æ–¹æ³•è®º>åŸºæœ¬æ–¹æ³•è®º</a></li><li><a href=#æœºå™¨å­¦ä¹ çš„åˆ†ç±»>æœºå™¨å­¦ä¹ çš„åˆ†ç±»</a><ol><li><a href=#ç›‘ç£å­¦ä¹ -supervised-learning>ç›‘ç£å­¦ä¹  Supervised Learning</a></li><li><a href=#æ— ç›‘ç£å­¦ä¹ -unsupervised-learning>æ— ç›‘ç£å­¦ä¹  Unsupervised Learning</a></li></ol></li></ol></li><li><a href=#å›å½’-regression-i-week-1-2>å›å½’ Regression I ã€Week 1-2ã€‘</a><ol><li><a href=#é—®é¢˜å½¢æˆ>é—®é¢˜å½¢æˆ</a></li><li><a href=#å…³è”ä¸å› æœ>å…³è”ä¸å› æœ</a></li><li><a href=#æ¨¡å‹è´¨é‡æŒ‡æ ‡>æ¨¡å‹è´¨é‡æŒ‡æ ‡</a></li><li><a href=#è¯¯å·®æ˜¯è‡ªç„¶å­˜åœ¨çš„>è¯¯å·®æ˜¯è‡ªç„¶å­˜åœ¨çš„</a></li><li><a href=#å°†å›å½’è§†ä½œä¼˜åŒ–é—®é¢˜>å°†å›å½’è§†ä½œä¼˜åŒ–é—®é¢˜</a></li><li><a href=#å›å½’å­¦ä¹ é˜¶æ®µæ¨¡å‹>å›å½’å­¦ä¹ é˜¶æ®µæ¨¡å‹</a></li><li><a href=#ç®€å•çº¿æ€§å›å½’>ç®€å•çº¿æ€§å›å½’</a></li><li><a href=#ç®€å•å¤šé¡¹å¼å›å½’>ç®€å•å¤šé¡¹å¼å›å½’</a></li></ol></li><li><a href=#å›å½’-regression-ii-week-1-3>å›å½’ Regression II ã€Week 1-3ã€‘</a><ol><li><a href=#å¤šå…ƒå›å½’-multiple-regression>å¤šå…ƒå›å½’ Multiple Regression</a></li><li><a href=#å¤šå…ƒçº¿æ€§å›å½’>å¤šå…ƒçº¿æ€§å›å½’</a></li><li><a href=#æœ€å°äºŒä¹˜è§£>æœ€å°äºŒä¹˜è§£</a></li><li><a href=#å…¶ä»–å›å½’æ¨¡å‹>å…¶ä»–å›å½’æ¨¡å‹</a></li><li><a href=#é€»è¾‘å‡½æ•°>é€»è¾‘å‡½æ•°</a></li><li><a href=#å…¶ä»–è´¨é‡æŒ‡æ ‡>å…¶ä»–è´¨é‡æŒ‡æ ‡</a></li><li><a href=#çµæ´»æ€§-flexibility>çµæ´»æ€§ Flexibility</a></li><li><a href=#å¯è§£é‡Šæ€§-interpretability>å¯è§£é‡Šæ€§ interpretability</a></li><li><a href=#æ³›åŒ–-generalisation>æ³›åŒ– Generalisation</a></li><li><a href=#æ¬ æ‹Ÿåˆä¸è¿‡æ‹Ÿåˆ>æ¬ æ‹Ÿåˆä¸è¿‡æ‹Ÿåˆ</a></li></ol></li><li><a href=#æ–¹æ³•è®º-week-1-4>æ–¹æ³•è®º ã€Week 1-4ã€‘</a><ol><li><a href=#ä»ç›®æ ‡é‡‡æ ·æ•°æ®é›†>ä»ç›®æ ‡é‡‡æ ·æ•°æ®é›†</a></li><li><a href=#è¯„ä¼°éƒ¨ç½²æ—¶æ€§èƒ½>è¯„ä¼°éƒ¨ç½²æ—¶æ€§èƒ½</a></li><li><a href=#ä¼˜åŒ–ç†è®º>ä¼˜åŒ–ç†è®º</a></li><li><a href=#è¯¯å·®æ›²é¢>è¯¯å·®æ›²é¢</a></li><li><a href=#æ¢¯åº¦ä¸‹é™-gradient-descent>æ¢¯åº¦ä¸‹é™ Gradient descent</a></li><li><a href=#å­¦ä¹ ç‡>å­¦ä¹ ç‡</a></li><li><a href=#æ¢¯åº¦ä¸‹é™å¼€å§‹ä¸åœæ­¢>æ¢¯åº¦ä¸‹é™å¼€å§‹ä¸åœæ­¢</a></li><li><a href=#å±€éƒ¨è§£ä¸å…¨å±€è§£>å±€éƒ¨è§£ä¸å…¨å±€è§£</a></li><li><a href=#è®­ç»ƒmlæ¨¡å‹>è®­ç»ƒMLæ¨¡å‹</a></li><li><a href=#è®­ç»ƒé›†ä¸æœ€å°å‡æ–¹>è®­ç»ƒé›†ä¸æœ€å°å‡æ–¹</a></li><li><a href=#æš´åŠ›ç©·ä¸¾>æš´åŠ›ç©·ä¸¾</a></li><li><a href=#ç”±æ•°æ®é©±åŠ¨çš„æ¢¯åº¦ä¸‹é™>ç”±æ•°æ®é©±åŠ¨çš„æ¢¯åº¦ä¸‹é™</a></li><li><a href=#å…¶ä»–åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•>å…¶ä»–åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•</a></li><li><a href=#ç»éªŒè¯¯å·®æ›²é¢å’Œè¿‡æ‹Ÿåˆ>ç»éªŒè¯¯å·®æ›²é¢å’Œè¿‡æ‹Ÿåˆ</a></li><li><a href=#æ­£åˆ™åŒ–-regularisation>æ­£åˆ™åŒ– Regularisation</a></li><li><a href=#è®­ç»ƒæœŸé—´æˆæœ¬ä¸è´¨é‡>ï¼ˆè®­ç»ƒæœŸé—´ï¼‰æˆæœ¬ä¸è´¨é‡</a></li><li><a href=#éªŒè¯æ¨¡å‹>éªŒè¯æ¨¡å‹</a></li><li><a href=#éªŒè¯æ–¹æ³•æ•°æ®é›†åˆ‡åˆ†æ–¹æ³•>éªŒè¯æ–¹æ³•ï¼ˆæ•°æ®é›†åˆ‡åˆ†æ–¹æ³•ï¼‰</a></li></ol></li><li><a href=#åˆ†ç±»-classication-i-week-2-1>åˆ†ç±» Classi cation I ã€Week 2-1ã€‘</a><ol><li><a href=#é—®é¢˜å½¢æˆ-1>é—®é¢˜å½¢æˆ</a></li><li><a href=#åœ¨æ ‡ç­¾ç©ºé—´ä¸‹çš„æ•°æ®é›†>åœ¨æ ‡ç­¾ç©ºé—´ä¸‹çš„æ•°æ®é›†</a></li><li><a href=#åœ¨é¢„æµ‹å˜é‡ç©ºé—´ä¸‹çš„æ•°æ®é›†>åœ¨é¢„æµ‹å˜é‡ç©ºé—´ä¸‹çš„æ•°æ®é›†</a></li><li><a href=#åˆ¤å†³åŸŸ>åˆ¤å†³åŸŸ</a></li><li><a href=#çº¿æ€§åˆ†ç±»å™¨>çº¿æ€§åˆ†ç±»å™¨</a></li><li><a href=#åŸºæœ¬è´¨é‡æŒ‡æ ‡>åŸºæœ¬è´¨é‡æŒ‡æ ‡</a></li><li><a href=#é€»è¾‘å›å½’æ¨¡å‹>é€»è¾‘å›å½’æ¨¡å‹</a></li><li><a href=#å‚æ•°åŒ–ä¸éå‚æ•°åŒ–æ–¹æ³•>å‚æ•°åŒ–ä¸éå‚æ•°åŒ–æ–¹æ³•</a></li><li><a href=#æœ€è¿‘é‚»>æœ€è¿‘é‚»</a></li><li><a href=#kè¿‘é‚»>Kè¿‘é‚»</a></li></ol></li><li><a href=#åˆ†ç±»-classication-ii-week-2-2>åˆ†ç±» Classi cation II ã€Week 2-2ã€‘</a><ol><li><a href=#å…ˆéªŒæ¦‚ç‡åéªŒæ¦‚ç‡ä¸è´å¶æ–¯åˆ†ç±»å™¨>å…ˆéªŒæ¦‚ç‡ã€åéªŒæ¦‚ç‡ä¸è´å¶æ–¯åˆ†ç±»å™¨</a></li><li><a href=#å…ˆéªŒæ¦‚ç‡prior-probability>å…ˆéªŒæ¦‚ç‡ï¼ˆPrior Probabilityï¼‰</a></li><li><a href=#åéªŒæ¦‚ç‡posterior-probability>åéªŒæ¦‚ç‡ï¼ˆPosterior Probabilityï¼‰</a></li><li><a href=#è´å¶æ–¯å®šç†bayes-theorem>è´å¶æ–¯å®šç†ï¼ˆBayes&rsquo; Theoremï¼‰</a></li><li><a href=#è´å¶æ–¯åˆ†ç±»å™¨bayesian-classifier>è´å¶æ–¯åˆ†ç±»å™¨ï¼ˆBayesian Classifierï¼‰</a></li><li><a href=#æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨naive-bayes-classifier>æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ï¼ˆNaive Bayes Classifierï¼‰</a></li><li><a href=#åˆ¤åˆ«åˆ†æ-discriminant-analysis>åˆ¤åˆ«åˆ†æ Discriminant analysis</a></li><li><a href=#æ¯”è¾ƒå­¦è¿‡çš„åˆ†ç±»å™¨>æ¯”è¾ƒå­¦è¿‡çš„åˆ†ç±»å™¨</a></li><li><a href=#æ‹“å±•è´å¶æ–¯åˆ†ç±»å™¨>æ‹“å±•è´å¶æ–¯åˆ†ç±»å™¨</a></li><li><a href=#æ··æ·†çŸ©é˜µ>æ··æ·†çŸ©é˜µ</a></li><li><a href=#å…¶ä»–æŒ‡æ ‡>å…¶ä»–æŒ‡æ ‡</a></li><li><a href=#f1-score>F1 Score</a></li><li><a href=#roc>ROC</a></li></ol></li><li><a href=#æ–¹æ³•è®º-methodology-iiweek-3-1>æ–¹æ³•è®º Methodology IIã€week 3-1ã€‘</a><ol><li><a href=#æµæ°´çº¿-pipeline>æµæ°´çº¿ Pipeline</a></li><li><a href=#æ•°æ®æ ‡å‡†åŒ–-normalization>æ•°æ®æ ‡å‡†åŒ– Normalization</a></li><li><a href=#æœ€å°æœ€å¤§å½’ä¸€åŒ–-min-max-normalisation>æœ€å°æœ€å¤§å½’ä¸€åŒ– Min-max normalisation</a></li><li><a href=#æ ‡å‡†åŒ–-standardisation>æ ‡å‡†åŒ– Standardisation</a></li><li><a href=#æ ‡å‡†åŒ–ç›¸å…³>æ ‡å‡†åŒ–ç›¸å…³</a></li><li><a href=#è½¬æ¢-transformation>è½¬æ¢ Transformation</a></li><li><a href=#ä¸»æˆåˆ†åˆ†æ-pca>ä¸»æˆåˆ†åˆ†æ PCA</a></li><li><a href=#éçº¿æ€§å˜æ¢>éçº¿æ€§å˜æ¢</a></li><li><a href=#å¤æ‚æ¨¡å‹kernelæ–¹æ³•-complex-models-and-kernel-methods>å¤æ‚æ¨¡å‹&amp;Kernelæ–¹æ³• Complex models and kernel methods</a></li><li><a href=#é™ç»´-dimensionality-reduction>é™ç»´ Dimensionality reduction</a></li><li><a href=#ç‰¹å¾é€‰æ‹©>ç‰¹å¾é€‰æ‹©</a><ol><li><a href=#filtering>Filtering</a></li><li><a href=#wrapping>Wrapping</a></li></ol></li><li><a href=#ç‰¹å¾æå–-feature-extraction>ç‰¹å¾æå– Feature extraction</a></li><li><a href=#é›†æˆ-ensembles>é›†æˆ Ensembles</a><ol><li><a href=#bagging>Bagging</a></li><li><a href=#decision-trees>Decision trees</a></li><li><a href=#éšæœºæ£®æ—>éšæœºæ£®æ—</a></li><li><a href=#boosting>Boosting</a></li></ol></li></ol></li><li><a href=#ç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ week-3-2>ç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ã€Week 3-2ã€‘</a><ol><li><a href=#æ¨¡å¼ä¸ç»“æ„>æ¨¡å¼ä¸ç»“æ„</a></li><li><a href=#the-curse-of-dimensionality>The Curse of Dimensionality</a></li><li><a href=#ç¥ç»ç½‘ç»œ>ç¥ç»ç½‘ç»œ</a></li><li><a href=#æ„ŸçŸ¥å™¨-perceptron>æ„ŸçŸ¥å™¨ perceptron</a></li><li><a href=#å±‚-layer>å±‚ Layer</a></li><li><a href=#æ¶æ„-architecture>æ¶æ„ Architecture</a></li><li><a href=#ç¥ç»ç½‘ç»œ-1>ç¥ç»ç½‘ç»œ</a></li><li><a href=#æ„ŸçŸ¥å™¨ä½œä¸ºçº¿æ€§åˆ†ç±»å™¨>æ„ŸçŸ¥å™¨ä½œä¸ºçº¿æ€§åˆ†ç±»å™¨</a></li><li><a href=#æ„ŸçŸ¥å™¨å®ç°é€»è¾‘é—¨åŠŸèƒ½>æ„ŸçŸ¥å™¨å®ç°é€»è¾‘é—¨åŠŸèƒ½</a></li><li><a href=#æ„ŸçŸ¥å™¨å®ç°ç½‘æ ¼æ¨¡å¼æ£€æµ‹>æ„ŸçŸ¥å™¨å®ç°ç½‘æ ¼æ¨¡å¼æ£€æµ‹</a></li><li><a href=#ç»“åˆçº¿æ€§åˆ†ç±»å™¨å’Œé€»è¾‘åŠŸèƒ½å¯ä»¥å¾—åˆ°>ç»“åˆçº¿æ€§åˆ†ç±»å™¨å’Œé€»è¾‘åŠŸèƒ½å¯ä»¥å¾—åˆ°ï¼š</a></li><li><a href=#ä»è®¡ç®—è§’åº¦çœ‹å¾…ç¥ç»ç½‘ç»œ>ä»è®¡ç®—è§’åº¦çœ‹å¾…ç¥ç»ç½‘ç»œ</a></li><li><a href=#ç¥ç»ç½‘ç»œæˆæœ¬å‡½æ•°>ç¥ç»ç½‘ç»œæˆæœ¬å‡½æ•°</a></li><li><a href=#æ¢¯åº¦ä¸‹é™ä¸åå‘ä¼ æ’­>æ¢¯åº¦ä¸‹é™ä¸åå‘ä¼ æ’­</a></li><li><a href=#è®­ç»ƒæ¨¡å‹çš„æ³¨æ„äº‹é¡¹>è®­ç»ƒæ¨¡å‹çš„æ³¨æ„äº‹é¡¹</a></li><li><a href=#è¿ç§»å­¦ä¹ >è¿ç§»å­¦ä¹ </a></li><li><a href=#å…¨è¿æ¥å±‚>å…¨è¿æ¥å±‚</a></li><li><a href=#ç½‘æ ¼æ•°æ®ä¸­çš„ç­‰å˜æ€§-equivariance-in-grid-data>ç½‘æ ¼æ•°æ®ä¸­çš„ç­‰å˜æ€§ Equivariance in grid data</a></li><li><a href=#å·ç§¯å±‚>å·ç§¯å±‚</a></li><li><a href=#æ± åŒ–å±‚>æ± åŒ–å±‚</a></li><li><a href=#æ·±åº¦å­¦ä¹ æ¶æ„>æ·±åº¦å­¦ä¹ æ¶æ„</a></li></ol></li><li><a href=#ç»“æ„åˆ†æ-structure-analysisweek-3-4>ç»“æ„åˆ†æ Structure analysisã€Week 3-4ã€‘</a><ol><li><a href=#æ— ç›‘ç£å­¦ä¹ >æ— ç›‘ç£å­¦ä¹ </a></li><li><a href=#æ— ç›‘ç£å­¦ä¹ çš„ç”¨å¤„>æ— ç›‘ç£å­¦ä¹ çš„ç”¨å¤„</a></li><li><a href=#èšç±»åˆ†æ>èšç±»åˆ†æ</a></li><li><a href=#similarity-as-proximity>Similarity as proximity</a></li><li><a href=#è´¨é‡æŒ‡æ ‡>è´¨é‡æŒ‡æ ‡</a></li><li><a href=#k-means-èšç±»>K-means èšç±»</a></li><li><a href=#è‚˜éƒ¨æ³•-elbow-method>è‚˜éƒ¨æ³• Elbow method</a></li><li><a href=#éå‡¸èšç±»-non-convex-clusters>éå‡¸èšç±» Non-convex clusters</a></li><li><a href=#dbscan>DBSCAN</a></li><li><a href=#å±‚æ¬¡èšç±»-hierarchical-clustering>å±‚æ¬¡èšç±» Hierarchical clustering</a></li><li><a href=#ç»„åˆ†åˆ†æ-component-analysis>ç»„åˆ†åˆ†æ component analysis</a></li></ol></li><li><a href=#å¯†åº¦ä¼°è®¡-density-estimation-week-4-2>å¯†åº¦ä¼°è®¡ Density estimation ã€week 4-2ã€‘</a><ol><li><a href=#æ•°æ®åˆ†å¸ƒ>æ•°æ®åˆ†å¸ƒ</a></li><li><a href=#åˆ‡åˆ†å¹¶è®¡æ•°>åˆ‡åˆ†å¹¶è®¡æ•°</a></li><li><a href=#æ¦‚ç‡å¯†åº¦>æ¦‚ç‡å¯†åº¦</a></li><li><a href=#å¯†åº¦ä¼°è®¡>å¯†åº¦ä¼°è®¡</a></li><li><a href=#éå‚æ•°æ–¹æ³•çš„å¯†åº¦ä¼°è®¡>éå‚æ•°æ–¹æ³•çš„å¯†åº¦ä¼°è®¡</a><ol><li><a href=#ç›´æ–¹å›¾-histogram>ç›´æ–¹å›¾ histogram</a></li><li><a href=#æ ¸æ–¹æ³•-kernel-method>æ ¸æ–¹æ³• Kernel method</a></li></ol></li><li><a href=#å‚æ•°æ–¹æ³•çš„å¯†åº¦ä¼°è®¡>å‚æ•°æ–¹æ³•çš„å¯†åº¦ä¼°è®¡</a><ol><li><a href=#é«˜æ–¯åˆ†å¸ƒ>é«˜æ–¯åˆ†å¸ƒ</a></li><li><a href=#ä¸­å¿ƒæé™å®šç†-central-limit-theorem-clt>ä¸­å¿ƒæé™å®šç† Central Limit Theorem, CLT</a></li><li><a href=#å¤šå…ƒé«˜æ–¯åˆ†å¸ƒ>å¤šå…ƒé«˜æ–¯åˆ†å¸ƒ</a></li></ol></li><li><a href=#é«˜æ–¯åˆ†å¸ƒä¼°è®¡>é«˜æ–¯åˆ†å¸ƒä¼°è®¡</a></li><li><a href=#æ··åˆæ¨¡å‹-mixture-models>æ··åˆæ¨¡å‹ Mixture models</a></li><li><a href=#å™ªå£°ä¸ç¦»ç¾¤å€¼å¼‚å¸¸å€¼-noise-and-outliers>å™ªå£°ä¸ç¦»ç¾¤å€¼ï¼ˆå¼‚å¸¸å€¼ï¼‰ Noise and outliers</a></li><li><a href=#åŸºæœ¬å¼‚å¸¸å€¼æ£€æµ‹ç®—æ³•>åŸºæœ¬å¼‚å¸¸å€¼æ£€æµ‹ç®—æ³•</a></li><li><a href=#ç±»åˆ«å¯†åº¦ä¼°è®¡åˆ†ç±»å™¨>ç±»åˆ«å¯†åº¦ä¼°è®¡åˆ†ç±»å™¨</a></li><li><a href=#åº”ç”¨ç±»åˆ«å¯†åº¦åˆ°æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨>åº”ç”¨ç±»åˆ«å¯†åº¦åˆ°æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨</a></li><li><a href=#åº”ç”¨ç±»åˆ«å¯†åº¦åˆ°èšç±»>åº”ç”¨ç±»åˆ«å¯†åº¦åˆ°èšç±»</a></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/blog/categories/%E8%AF%BE%E4%B8%9A/ style=background-color:#2a9d8f;color:#fff>è¯¾ä¸š</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/blog/p/2025-ml-note/>æœºå™¨å­¦ä¹  å…¨å››å‘¨ æ¦‚å¿µæ•´ç†</a></h2><h3 class=article-subtitle>è¯¾ç¨‹å…¨å››å‘¨è¯¾ä»¶çŸ¥è¯†æ•´ç† å®Œæˆäº01.02</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jan 29, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>é˜…è¯»æ—¶é•¿: 44 åˆ†é’Ÿ</time></div></footer></div></header><section class=article-content><p>Hugo çš„ LaTex å…¬å¼æ¸²æŸ“æœ‰ç‚¹æ¯›ç—…ï¼Œè¿™ä¸ªé¡µé¢ä¸Šçš„æ˜¾ç¤ºæ•ˆæœå¯èƒ½å­˜åœ¨é—®é¢˜ã€‚</p><p>å¦‚æœä½ éœ€è¦è¿™ç¯‡ç¬”è®°çš„ PDF ç‰ˆæœ¬ï¼Œ<strong>å¯ <a class=link href=https://sharrybyte.lanzoub.com/iRc9Q2m81guf target=_blank rel=noopener>ç‚¹å‡»è¿™é‡Œ</a> ä¸‹è½½</strong>ã€‚</p><h2 id=å‰è¨€week1-1>å‰è¨€ã€Week1-1ã€‘</h2><h3 id=æœºå™¨å­¦ä¹ ç»Ÿè®¡å­¦ä¹ çš„å®šä¹‰>æœºå™¨å­¦ä¹ ï¼ˆç»Ÿè®¡å­¦ä¹ ï¼‰çš„å®šä¹‰</h3><p>æœºå™¨å­¦ä¹ æˆ–ç»Ÿè®¡å­¦ä¹ é€šå¸¸è¢«å®šä¹‰ä¸ºï¼š Machine or statistical learning is
usually defined as:</p><p>ä»åŸå§‹<strong>æ•°æ®</strong>ä¸­æå–æ¨¡å¼ä»¥è·å–<strong>çŸ¥è¯†</strong>çš„èƒ½åŠ›ã€‚ The ability to acquire
<strong>knowledge</strong> by extracting patterns from raw <strong>data</strong>. (Goodfellow,
Bengio, Courville)</p><p>ç”¨äº<strong>å»ºæ¨¡</strong>å’Œ<strong>ç†è§£</strong>å¤æ‚<strong>æ•°æ®é›†</strong>çš„ä¸€ç»„å·¥å…·ã€‚ A set of tools for
<strong>modeling</strong> and <strong>understanding</strong> complex <strong>datasets</strong>. (James, Witten,
Hastie, Tibshirani)</p><h3 id=æ•°æ®çš„å®šä¹‰>æ•°æ®çš„å®šä¹‰</h3><p>æ•°æ®æ˜¯<strong>è§‚å¯Ÿæˆ–æµ‹é‡</strong>çš„å…·ä½“åŒ–ã€‚ <strong>Data</strong> is the materialisation of an
<strong>observation</strong> or a <strong>measurement</strong>.</p><p>æ•°æ®é›†æ˜¯æŒ‰ç…§ä¸€ç»„<strong>é¢„å®šä¹‰å±æ€§æè¿°çš„é¡¹</strong>é›†åˆæ ¼å¼åŒ–çš„æ•°æ®ã€‚ <strong>Datasets</strong>
are data formatted as collections of <strong>items</strong> described by a set of
pre-defined <strong>attributes</strong>.</p><p>åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬çš„æ•°æ®æ€»æ˜¯<strong>ä»¥æ•°æ®é›†çš„å½¢å¼è¡¨ç¤º</strong>ã€‚ In machine
learning, our data is always <strong>represented as a dataset.</strong></p><h3 id=çŸ¥è¯†çš„è¡¨ç¤º>çŸ¥è¯†çš„è¡¨ç¤º</h3><p>çŸ¥è¯†å¯ä»¥è¡¨ç¤ºä¸º Knowledge can be represented as</p><p><strong>å‘½é¢˜ï¼ˆé™ˆè¿°ã€å®šå¾‹ï¼‰</strong> <strong>Proposition (statement, law)</strong></p><p><strong>å™è¿°ï¼ˆæè¿°ã€æ•…äº‹ï¼‰</strong> <strong>Narrative (description, story)</strong></p><p><strong>æ¨¡å‹ï¼ˆæ•°å­¦æˆ–è®¡ç®—æœºï¼‰</strong> <strong>Model (mathematical or computer)</strong></p><p>æ¨¡å‹æè¿°äº†å±æ€§ä¹‹é—´çš„å…³ç³»ã€‚<br>Models describe relationships between attributes.</p><p>æ•°å­¦æ¨¡å‹å’Œè®¡ç®—æœºæ¨¡å‹æ˜¯ç­‰ä»·çš„ï¼š<br>Mathematical and computer models are equivalent:</p><p>æ•°å­¦æ¨¡å‹å¯ä»¥å®ç°ä¸ºè®¡ç®—æœºç¨‹åºã€‚<br>Mathematical models can be implemented as computer programs.</p><p>æ¯ä¸ªè®¡ç®—æœºæ¨¡å‹éƒ½æœ‰å¯¹åº”çš„æ•°å­¦è¡¨è¾¾å¼ã€‚<br>Every computer model has a corresponding mathematical expression.</p><h3 id=æ•°æ®ç§‘å­¦>æ•°æ®ç§‘å­¦</h3><p>ç§‘å­¦ä¸åœ¨äºä½¿ç”¨å¤æ‚çš„ä»ªå™¨ã€æ•°å­¦æˆ–æŠ€æœ¯ï¼Œè€Œåœ¨äºè¯„ä¼°æˆ‘ä»¬çš„çŸ¥è¯†ã€‚ Science is
not about using sophisticated instrumentation, maths or techniques,
science is about evaluating our knowledge.</p><p>æˆ‘ä»¬åœ¨æ­¤è¯„ä¼°ä¸­ä½¿ç”¨æ•°æ®ä¸å…¬è®¤çš„çŸ¥è¯†ã€‚ We use data together with accepted
knowledge in this evaluation.</p><p>å‘½é¢˜æœ¬èº«å¹¶æ— ç§‘å­¦æˆ–ä¸ç§‘å­¦ä¹‹åˆ†ï¼Œå…³é”®åœ¨äºæˆ‘ä»¬å¦‚ä½•è¯„ä¼°å®ƒä»¬ã€‚ Propositions
are not scientific or unscientific, but the way we evaluate them.</p><p>å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œå°±æ²¡æœ‰æœºå™¨å­¦ä¹ ã€‚ If there is no data, there is no machine
learning.</p><p>è¿™å¹¶ä¸æ„å‘³ç€æœºå™¨å­¦ä¹ ä»…ä»…æ˜¯å…³äºæ•°æ®çš„ã€‚ This doesn&rsquo;t mean machine
learning is all about data.</p><p>å…·ä½“æ¥è¯´ï¼Œä¸å­˜åœ¨ä¸­ç«‹æˆ–å®¢è§‚çš„æ•°æ®æ¥æ­ç¤ºçœŸç›¸ã€‚ Specifically, there is no
such thing as neutral or objective data that speaks the truth.</p><p>æˆ‘ä»¬éœ€è¦éµå¾ª<strong>ä¸¥æ ¼çš„æ–¹æ³•è®º</strong>ã€‚ We need to follow a <strong>rigorous
methodology</strong>.</p><h3 id=æœºå™¨å­¦ä¹ çš„ä¸¤ç§è§‚ç‚¹>æœºå™¨å­¦ä¹ çš„ä¸¤ç§è§‚ç‚¹</h3><p>è®¸å¤šæœºå™¨å­¦ä¹ ä»ä¸šè€…é‡‡ç”¨<strong>æ•°æ®é›†ä¼˜å…ˆ</strong>çš„è§‚ç‚¹ï¼šæˆ‘ä»¬ä»æ•°æ®é›†å¼€å§‹ï¼Œç„¶åï¼ˆåˆ¶å®šé—®é¢˜å¹¶æœ€ç»ˆï¼‰ç”Ÿæˆæ¨¡å‹ã€‚
Many machine learning professionals adopt a <strong>dataset-first</strong> view: we
start with a dataset, then (formulate a problem and finally) produce a
model.</p><p>åœ¨CBU5201ä¸­ï¼Œ<strong>æˆ‘ä»¬é‡‡ç”¨éƒ¨ç½²ä¼˜å…ˆï¼ˆé—®é¢˜ä¼˜å…ˆï¼‰çš„æ–¹æ³•</strong>ï¼šæˆ‘ä»¬ä»é—®é¢˜å¼€å§‹ï¼Œç„¶åè·å–æ•°æ®é›†ï¼Œæœ€åç”Ÿæˆæ¨¡å‹ã€‚
In CBU5201 we use a <strong>deployment-first (problem-first) approach</strong>: we
start with a problem, then secure a dataset and finally produce a model.</p><p>å› æ­¤ï¼Œæˆ‘ä»¬å°†æœºå™¨å­¦ä¹ å®šä¹‰ä¸ºï¼šä¸€å¥—ä½¿ç”¨æ•°æ®ç”¨äºè§£å†³ç§‘å­¦ã€å·¥ç¨‹å’Œå•†ä¸šé—®é¢˜çš„å·¥å…·å’Œæ–¹æ³•è®º&mldr;
Accordingly, we define machine learning as: A set of tools together with
a methodology for solving scientific, engineering and business problems
using data</p><h3 id=æœºå™¨å­¦ä¹ çš„ä¸¤ä¸ªé˜¶æ®µ>æœºå™¨å­¦ä¹ çš„ä¸¤ä¸ªé˜¶æ®µ</h3><p>æ¨¡å‹å¯ä»¥è¢«æ„å»ºã€é”€å”®å’Œéƒ¨ç½²ä»¥æä¾›<strong>ä»·å€¼</strong>ã€‚<br>Models can be built, sold, and deployed to deliver <strong>value</strong>.</p><p>åœ¨æ¨¡å‹çš„ç”Ÿå‘½å‘¨æœŸä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åŒºåˆ†ä¸¤ä¸ªé˜¶æ®µï¼š<br>During the life of a model, we can distinguish two stages:</p><ol><li><p>å­¦ä¹ é˜¶æ®µï¼šæ¨¡å‹è¢«æ„å»ºã€‚</p><p>Learning stage: The model is built.</p></li><li><p>éƒ¨ç½²é˜¶æ®µï¼šæ¨¡å‹è¢«ä½¿ç”¨ã€‚</p><p>Deployment stage: The model is used.</p></li></ol><h3 id=åŸºæœ¬æ–¹æ³•è®º>åŸºæœ¬æ–¹æ³•è®º</h3><p>åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å…³æ³¨çš„æ˜¯æ‰¾åˆ°åœ¨éƒ¨ç½²æ—¶æœ‰æ•ˆçš„æ¨¡å‹ã€‚å› æ­¤ï¼Œé™¤äº†æ„å»ºæ¨¡å‹å¤–ï¼Œæˆ‘ä»¬è¿˜éœ€è¦éªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚<br>In machine learning, we are interested in finding models that work
during deployment. Hence, in addition to building a model, we need to
check it works.</p><p>åŸºæœ¬çš„æœºå™¨å­¦ä¹ æ–¹æ³•è®ºåŒ…æ‹¬ä¸¤ä¸ªç‹¬ç«‹çš„ä»»åŠ¡ï¼š<br>Basic machine learning methodologies include two separate tasks:</p><p><strong>è®­ç»ƒ</strong>ï¼šä½¿ç”¨æ•°æ®å’Œè´¨é‡æŒ‡æ ‡åˆ›å»ºæ¨¡å‹ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥è¯´æˆ‘ä»¬å°†æ¨¡å‹æ‹Ÿåˆåˆ°æ•°æ®é›†ã€‚<br><strong>Training</strong>: A model is created using data and a quality metric. We
also say that we fit a model to a dataset.</p><p><strong>æµ‹è¯•</strong>ï¼šä½¿ç”¨æ–°çš„ã€æœªè§è¿‡çš„æ•°æ®è¯„ä¼°æ¨¡å‹åœ¨éƒ¨ç½²æ—¶çš„æ€§èƒ½ã€‚<br><strong>Testing</strong>: The performance of the model during deployment is assessed
using new, unseen data.</p><p>å¦‚æœæ²¡æœ‰ä¸¥æ ¼çš„æ–¹æ³•è®ºï¼Œæ¨¡å‹å¾ˆå¯èƒ½å‡ ä¹æ²¡æœ‰ç”¨å¤„ã€‚<br>Without rigorous methodologies, models are very likely to be of little
use.</p><h3 id=æœºå™¨å­¦ä¹ çš„åˆ†ç±»>æœºå™¨å­¦ä¹ çš„åˆ†ç±»</h3><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101163308940.png width=1101 height=561 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101163308940_hu_90b05bcdea14de6c.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101163308940_hu_f4d7d938ac3f895a.png 1024w" loading=lazy alt=image-20250101163308940 class=gallery-image data-flex-grow=196 data-flex-basis=471px></p><h4 id=ç›‘ç£å­¦ä¹ -supervised-learning>ç›‘ç£å­¦ä¹  Supervised Learning</h4><p>åœ¨ç›‘ç£å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬ç»™å®šä¸€ä¸ªæ–°é¡¹ç›®ï¼Œå…¶ä¸­ä¸€ä¸ªå±æ€§çš„å€¼å¯¹æˆ‘ä»¬æ¥è¯´æ˜¯æœªçŸ¥çš„ã€‚<br>In supervised learning, we are given a new item such that the value of
one of its attributes is unknown to us.</p><p>æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡ä»å·²çŸ¥é¡¹ç›®çš„é›†åˆä¸­å­¦ä¹ æ¥ä¼°è®¡ç¼ºå¤±çš„å€¼ã€‚<br>Our goal is to estimate the missing value by learning from a collection
of known items.</p><p>æŒ‘æˆ˜åœ¨äºæ„å»ºä¸€ä¸ªæ¨¡å‹ï¼Œè¯¥æ¨¡å‹å°†ä¸€ä¸ªå±æ€§xï¼ˆç§°ä¸ºé¢„æµ‹å˜é‡ï¼‰æ˜ å°„åˆ°å¦ä¸€ä¸ªå±æ€§yï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºæ ‡ç­¾ï¼‰ï¼Œä½¿ç”¨ä¸€ä¸ªå¸¦æ ‡ç­¾çš„æ•°æ®é›†ã€‚<br>The challenge is then to build a model that maps one attribute x, known
as the predictor, to another attribute y, which we call the label, using
a dataset of labelled examples.</p><p>ç›‘ç£å­¦ä¹ æ ¹æ®æ ‡ç­¾çš„ç±»å‹è¿›ä¸€æ­¥åˆ†ä¸ºä¸¤ç±»ï¼š Supervised learning is further
divided into two categories depending on the type of label:</p><ul><li><strong>åˆ†ç±»</strong>ï¼šæ ‡ç­¾æ˜¯<strong>ç¦»æ•£</strong>å˜é‡ã€‚ <strong>Classification</strong>: The label is a
discrete variable.</li><li><strong>å›å½’</strong>ï¼šæ ‡ç­¾æ˜¯<strong>è¿ç»­</strong>å˜é‡ã€‚ <strong>Regression</strong>: The label is a
continuous variable.</li></ul><h4 id=æ— ç›‘ç£å­¦ä¹ -unsupervised-learning>æ— ç›‘ç£å­¦ä¹  Unsupervised Learning</h4><p>åœ¨æ— ç›‘ç£å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬è‡´åŠ›äº<strong>å‘ç°æ•°æ®é›†çš„æ½œåœ¨ç»“æ„</strong>ã€‚<br>In unsupervised learning, we set out to <strong>find the underlying
structure</strong> of our dataset.</p><p>è¿™æœ‰åŠ©äºæˆ‘ä»¬è·å¾—ç†è§£ã€è¯†åˆ«å¼‚å¸¸ã€å‹ç¼©æ•°æ®å¹¶å‡å°‘å¤„ç†æ—¶é—´ã€‚<br>This can be useful to gain understanding, identify anomalies, compress
our data and reduce processing time.</p><p>æ•°æ®é›†çš„åŸºç¡€ç»“æ„å¯ä»¥é€šè¿‡<strong>ç»“æ„åˆ†æ</strong>æ¥ç ”ç©¶ï¼Œå…¶ä¸­åŒ…æ‹¬ï¼š The underlying
structure of a dataset can be studied using <strong>structure analysis</strong>,
which includes:</p><ul><li><strong>èšç±»åˆ†æ</strong>ï¼šä¸“æ³¨äºæ•°æ®ç‚¹çš„åˆ†ç»„ã€‚ <strong>Cluster analysis</strong>: Focuses on
groups of data points.</li><li><strong>æˆåˆ†åˆ†æ</strong>ï¼šè¯†åˆ«æ„Ÿå…´è¶£çš„æ–¹å‘ã€‚ <strong>Component analysis</strong>: Identifies
directions of interest.</li><li><strong>å¯†åº¦ä¼°è®¡æŠ€æœ¯</strong>æä¾›äº†æè¿°æ ·æœ¬åœ¨å±æ€§ç©ºé—´ä¸­åˆ†å¸ƒçš„ç»Ÿè®¡æ¨¡å‹ã€‚ <strong>Density
estimation techniques</strong> provide statistical models that describe the
distribution of samples in the attribute space.</li></ul><p>æ— ç›‘ç£å­¦ä¹ çš„ä¸€äº›åº”ç”¨å®ä¾‹ï¼š</p><ul><li><p>å®¢æˆ·ç»†åˆ†ã€‚ Customer segmentation.</p></li><li><p>ç¤¾äº¤ç¤¾åŒºæ£€æµ‹ã€‚ Social community detection.</p></li><li><p>æ¨èç³»ç»Ÿã€‚ Recommendation systems.</p></li><li><p>è¿›åŒ–åˆ†æã€‚ Evolutionary analysis.</p></li></ul><h2 id=å›å½’-regression-i-week-1-2>å›å½’ Regression I ã€Week 1-2ã€‘</h2><h3 id=é—®é¢˜å½¢æˆ>é—®é¢˜å½¢æˆ</h3><p>å›å½’æ˜¯ä¸€ä¸ª<strong>ç›‘ç£å­¦ä¹ </strong>é—®é¢˜ï¼šæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä½¿ç”¨å‰©ä½™å±æ€§ï¼ˆ<strong>é¢„æµ‹å˜é‡</strong>ï¼‰æ¥é¢„æµ‹ä¸€ä¸ªå±æ€§ï¼ˆ<strong>æ ‡ç­¾</strong>ï¼‰çš„å€¼ã€‚
Regression is a <strong>supervised</strong> problem: Our goal is to predict the value
of one attribute (<strong>label</strong>) using the remaining attributes
(<strong>predictors</strong>).</p><p>æ ‡ç­¾æ˜¯ä¸€ä¸ª<strong>è¿ç»­</strong>å˜é‡ã€‚ The label is a <strong>continuous</strong> variable.</p><p>æˆ‘ä»¬çš„ä»»åŠ¡æ˜¯<strong>æ‰¾åˆ°æœ€ä½³æ¨¡å‹</strong>ï¼Œè¯¥æ¨¡å‹å¯ä»¥ä¸ºç»™å®šçš„é¢„æµ‹å˜é‡é›†åˆ†é…å”¯ä¸€çš„æ ‡ç­¾ã€‚
Our job is then to <strong>find the best model</strong> that assigns a unique label
to a given set of predictors.</p><p>æˆ‘ä»¬ä½¿ç”¨ç”±<strong>æ ‡è®°æ ·æœ¬</strong>ç»„æˆçš„<strong>æ•°æ®é›†</strong>ã€‚ We use <strong>datasets</strong> consisting
of <strong>labelled samples</strong>.</p><h3 id=å…³è”ä¸å› æœ>å…³è”ä¸å› æœ</h3><p>é¢„æµ‹æ¨¡å‹æœ‰æ—¶è¢«é€šè¿‡<strong>å› æœè§†è§’</strong>è§£é‡Šï¼šé¢„æµ‹å› å­æ˜¯<strong>åŸå› </strong>ï¼Œæ ‡ç­¾æ˜¯å…¶<strong>ç»“æœ</strong>ã€‚ç„¶è€Œè¿™æ˜¯<strong>ä¸æ­£ç¡®çš„</strong>ã€‚
Prediction models are sometimes interpreted through a causal lens: the
predictor is the cause, the label its effect. However this is not
correct.</p><p>æˆ‘ä»¬æ„å»ºé¢„æµ‹å™¨çš„èƒ½åŠ›æºäºå±æ€§ä¹‹é—´çš„<strong>å…³è”</strong>ï¼Œè€Œé<strong>å› æœ</strong>å…³ç³»ã€‚ Our
ability to build predictors is due to association between attributes,
rather than causation.</p><p>æ•°æ®é›†ä¸­çš„ä¸¤ä¸ªå±æ€§å‡ºç°å…³è”çš„åŸå› å¯èƒ½æ˜¯ï¼š Two attributes in a dataset
appear associated due to:</p><ul><li><p>ä¸€ä¸ªå±æ€§å¯¼è‡´å¦ä¸€ä¸ªå±æ€§ï¼ˆç›´æ¥æˆ–é—´æ¥ï¼‰ã€‚ If one causes the other
(directly or indirectly).</p></li><li><p>ä¸¤è€…æœ‰å…±åŒçš„åŸå› ã€‚ When both have a common cause.</p></li><li><p>ç”±äºæˆ‘ä»¬æ”¶é›†æ ·æœ¬çš„æ–¹å¼ï¼ˆæŠ½æ ·ï¼‰ã€‚ Due to the way we collect samples
(sampling).</p></li></ul><p>å…³é”®ä¿¡æ¯ï¼šåœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬ä¸æ„å»ºå› æœæ¨¡å‹ï¼ Take-home message: In
machine learning we don&rsquo;t build causal models!</p><h3 id=æ¨¡å‹è´¨é‡æŒ‡æ ‡>æ¨¡å‹è´¨é‡æŒ‡æ ‡</h3><p>To find the best model, we need a notion of model quality.<br>ä¸ºäº†æ‰¾åˆ°æœ€ä½³æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ¨¡å‹è´¨é‡çš„æ¦‚å¿µã€‚</p><p>The squared error $ e_i^2 = (y_i - <code>\hat{y}</code>{=tex}_i)^2 $ is a
common quantity used in regression to encapsulate the notion of single
prediction quality.<br>å¹³æ–¹è¯¯å·® $ e_i^2 = (y_i - <code>\hat{y}</code>{=tex}_i)^2 $
æ˜¯å›å½’ä¸­å¸¸ç”¨çš„é‡ï¼Œç”¨äºæ¦‚æ‹¬å•ä¸ªé¢„æµ‹è´¨é‡çš„æ¦‚å¿µã€‚</p><p>Two quality metrics based on the squared error are the sum of squared
errors (SSE) and the mean squared error (MSE), which can be computed
using a dataset as:<br>åŸºäºå¹³æ–¹è¯¯å·®çš„ä¸¤ä¸ªè´¨é‡æŒ‡æ ‡æ˜¯è¯¯å·®å¹³æ–¹å’Œï¼ˆSSEï¼‰å’Œå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ï¼Œå¯ä»¥ä½¿ç”¨æ•°æ®é›†è®¡ç®—å¦‚ä¸‹ï¼š</p><p>$E_{SSE} = e_1^2 + e_2^2 + \cdots + e_N^2 = \sum_{i=1}^{N}e_i^2$</p><p>$E_{MSE} = \frac{1}{N} \sum_{i=1}^{N}e_i^2$</p><h3 id=è¯¯å·®æ˜¯è‡ªç„¶å­˜åœ¨çš„>è¯¯å·®æ˜¯è‡ªç„¶å­˜åœ¨çš„</h3><p>åœ¨å›å½’é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬éœ€è¦æ³¨æ„ï¼š<br>When considering a regression problem, we need to be aware:</p><ul><li><p>é€‰æ‹©çš„<strong>é¢„æµ‹å˜é‡å¯èƒ½ä¸åŒ…æ‹¬æ‰€æœ‰å†³å®šæ ‡ç­¾çš„å› ç´ </strong>ã€‚<br>The chosen predictors might not include all factors that determine
the label.</p></li><li><p>é€‰æ‹©çš„<strong>æ¨¡å‹å¯èƒ½æ— æ³•è¡¨ç¤º</strong>å“åº”ä¸é¢„æµ‹å˜é‡ä¹‹é—´çš„çœŸå®å…³ç³»ï¼ˆæ¨¡å¼ï¼‰ã€‚<br>The chosen model might not be able to represent the true
relationship between response and predictor (the pattern).</p></li><li><p><strong>éšæœºæœºåˆ¶</strong>ï¼ˆå™ªéŸ³ï¼‰å¯èƒ½å­˜åœ¨ã€‚<br>Random mechanisms (noise) might be present.</p></li></ul><p>åœ¨æ•°å­¦ä¸Šï¼Œæˆ‘ä»¬å°†è¿™ç§å·®å¼‚è¡¨ç¤ºä¸ºï¼š<br>Mathematically, we represent this discrepancy as:</p><p>$y = \hat{y} + e = f(x) + e$</p><p>çœŸå®æ ‡ç­¾ y ä¸æ¨¡å‹é¢„æµ‹ f (x) ä¹‹é—´æ€»ä¼šå­˜åœ¨ä¸€äº›è¯¯å·®ï¼ˆè¯¯å·® eï¼‰ã€‚<br>There will always be some discrepancy (error e) between the true label y
and our model prediction f(x).</p><h3 id=å°†å›å½’è§†ä½œä¼˜åŒ–é—®é¢˜>å°†å›å½’è§†ä½œä¼˜åŒ–é—®é¢˜</h3><p>å³ï¼šæ‰¾å‡ºæœ€å°MSEçš„æ¨¡å‹ã€‚</p><p>$f_{best}(x)=\arg\min_f\frac{1}{N}\sum_{i=1}^N\left(y_i-f(x_i)\right)^2$</p><h3 id=å›å½’å­¦ä¹ é˜¶æ®µæ¨¡å‹>å›å½’å­¦ä¹ é˜¶æ®µæ¨¡å‹</h3><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101165809920.png width=891 height=393 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101165809920_hu_640672088ebbbf32.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101165809920_hu_ac2f2f294f86617d.png 1024w" loading=lazy alt=image-20250101165809920 class=gallery-image data-flex-grow=226 data-flex-basis=544px></p><p>**å…ˆéªŒçŸ¥è¯†ï¼š**çº¿æ€§ã€å¤šé¡¹å¼ç­‰ç±»å‹çš„æ¨¡å‹<br>Type of model (linear, polynomial, etc).</p><p>**æ•°æ®ï¼š**æ ‡è®°æ ·æœ¬ï¼ˆç‰¹å¾å’ŒçœŸå€¼æ ‡ç­¾ï¼‰<br>Labelled samples (predictors and true label).</p><p>**æ¨¡å‹ï¼š**åŸºäºç‰¹å¾é¢„æµ‹æ ‡ç­¾<br>Predicts a label based on the predictors.</p><h3 id=ç®€å•çº¿æ€§å›å½’>ç®€å•çº¿æ€§å›å½’</h3><p>ç®€å•<strong>çº¿æ€§</strong>å›å½’æ¨¡å‹é€šè¿‡ä»¥ä¸‹æ•°å­¦è¡¨è¾¾å¼å®šä¹‰ï¼š In simple linear
regression, models are defined by the mathematical expression:</p><p>$f(x)=w_0+wx_1$</p><p>å› æ­¤ï¼Œé¢„æµ‹æ ‡ç­¾ $\hat{y}$ å¯ä»¥è¡¨ç¤ºä¸º Hence, the predicted label $\hat{y}$
can be expressed as</p><p>$\hat{y_i}f(x_i)=w_0+w_1x_i$</p><p>å› æ­¤ï¼Œä¸€ä¸ªçº¿æ€§æ¨¡å‹æœ‰<strong>ä¸¤ä¸ªå‚æ•°woï¼ˆæˆªè·ï¼‰å’Œw1ï¼ˆæ–œç‡ï¼‰</strong>ï¼Œ<strong>éœ€è¦è°ƒæ•´è¿™äº›å‚æ•°ä»¥è¾¾åˆ°æœ€é«˜è´¨é‡</strong>ã€‚
A linear model has therefore two parameters wo (intercept) and w1
(gradient), which need to be tuned to achieve the highest quality.</p><p>åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬ç”¨<strong>æ•°æ®é›†</strong>æ¥è°ƒæ•´è¿™äº›å‚æ•°ã€‚ In machine learning, we
use a dataset to tune the parameters.</p><p>æˆ‘ä»¬è¯´æˆ‘ä»¬åœ¨<strong>è®­ç»ƒæ¨¡å‹æˆ–å°†æ¨¡å‹æ‹Ÿåˆåˆ°è®­ç»ƒæ•°æ®é›†ä¸Š</strong>ã€‚ We say that we
train the model or fit the model to the training dataset.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101170257131.png width=870 height=630 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101170257131_hu_724f823b6c8a2a7b.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101170257131_hu_24276787377ce61e.png 1024w" loading=lazy alt=image-20250101170257131 class=gallery-image data-flex-grow=138 data-flex-basis=331px></p><h3 id=ç®€å•å¤šé¡¹å¼å›å½’>ç®€å•å¤šé¡¹å¼å›å½’</h3><p>å¤šé¡¹å¼å›å½’æ¨¡å‹çš„ä¸€èˆ¬å½¢å¼æ˜¯ï¼š The general form of a polynomial regression
model is:</p><p>$f(x_i) = w_0 + w_1x_i + w_2x_i^2 + \cdots + w_Dx_i^D$</p><p>è¿™é‡Œï¼Œ$D$ æ˜¯å¤šé¡¹å¼çš„é˜¶æ•°ã€‚ where $D$ is the degree of the polynomial.</p><p>å¤šé¡¹å¼å›å½’å®šä¹‰äº†<strong>ä¸€ç»„åŒ…å«å¤šç§æ¨¡å‹çš„æ—ã€‚</strong> Polynomial regression defines
<strong>a family of families of models.</strong></p><p>å¯¹äºæ¯ä¸ª $D$ å€¼ï¼Œæˆ‘ä»¬éƒ½æœ‰ä¸€ä¸ªä¸åŒçš„æ—ï¼š$D=1$ å¯¹åº”äºçº¿æ€§çš„æ—ï¼Œ$D=2$
å¯¹åº”äºäºŒæ¬¡çš„ï¼Œ$D=3$ å¯¹åº”äºä¸‰æ¬¡çš„ï¼Œä»¥æ­¤ç±»æ¨ã€‚ For each value of $D$, we
have a different family: $D=1$ corresponds to the linear family, $D=2$
to the quadratic, $D=3$ to the cubic, and so on.</p><p>æˆ‘ä»¬ç§° $D$ ä¸ºä¸€ä¸ª<strong>è¶…å‚æ•°</strong>ã€‚ We call $D$ a <strong>hyperparameter</strong>.</p><p>è¿™æ„å‘³ç€å…¶è®¾å®šçš„å€¼ä¼šäº§ç”Ÿä¸€ä¸ªä¸åŒçš„æ—ï¼Œå…·æœ‰ä¸åŒçš„å‚æ•°åˆé›†ã€‚ What it means
is that setting its value results in a different family, with a
different collection of parameters.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101170649577.png width=832 height=628 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101170649577_hu_1e1da9d4476ae2c8.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101170649577_hu_ddc72f9a2e7ffa72.png 1024w" loading=lazy alt=image-20250101170649577 class=gallery-image data-flex-grow=132 data-flex-basis=317px></p><h2 id=å›å½’-regression-ii-week-1-3>å›å½’ Regression II ã€Week 1-3ã€‘</h2><h3 id=å¤šå…ƒå›å½’-multiple-regression>å¤šå…ƒå›å½’ Multiple Regression</h3><p>å¤šå…ƒå›å½’æ‹¥æœ‰ä¸¤ä¸ªæˆ–æ›´å¤šçš„é¢„æµ‹å˜é‡ã€‚</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101211003056.png width=985 height=577 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101211003056_hu_812167a41b289121.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101211003056_hu_7f869214f1058f.png 1024w" loading=lazy alt=image-20250101211003056 class=gallery-image data-flex-grow=170 data-flex-basis=409px></p><p>æ•°å­¦è¡¨ç¤ºä¸ºï¼š</p><p>$\boldsymbol{x}<em>i=[1,x</em>{i,1},x_{i,2},\ldots,x_{i,K}]^T$</p><p>$\hat{y_i}=f(x_i)$</p><h3 id=å¤šå…ƒçº¿æ€§å›å½’>å¤šå…ƒçº¿æ€§å›å½’</h3><p>$f(x_i)=w^Tx_i=w_0+w_1x_{i,1}+\cdots+w_Kx_{i,K}$</p><p>where $w = [w_0, w_1,&mldr;,w_K]^T$ is the model&rsquo;s parameter vector.</p><p>Note that we can use the same vector notation for simple linear
regression models, by defining $\boldsymbol{w} = [w_0, w_1]^T$ and
$\boldsymbol{x}_i = [1,x_i]^T.$</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101212011866.png width=999 height=657 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101212011866_hu_f3e23a89ae562e81.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101212011866_hu_42f7cb9e725d2923.png 1024w" loading=lazy alt=image-20250101212011866 class=gallery-image data-flex-grow=152 data-flex-basis=364px></p><p>è®­ç»ƒæ•°æ®é›†å¯ä»¥è¡¨ç¤ºä¸ºè®¾è®¡çŸ©é˜µXï¼š In multiple linear regression, the
training dataset can be represented by the design matrix X:</p><p>$$\begin{gathered}\mathbf{X}\quad=\quad[\boldsymbol{x}<em>1,\ldots,\boldsymbol{x}<em>N]^T=\begin{bmatrix}\boldsymbol{x}<em>1^T\\boldsymbol{x}<em>2^T\\vdots\\boldsymbol{x}<em>N^T\end{bmatrix}=\begin{bmatrix}1&amp;x</em>{1,1}&amp;x</em>{1,2}&\ldots&amp;x</em>{1,K}\1&amp;x</em>{2,1}&amp;x</em>{2,2}&\ldots&amp;x_{2,K}\\vdots&\vdots&\vdots&\ddots&\vdots\1&amp;x_{N,1}&amp;x_{N,2}&\ldots&amp;x_{N,K}\end{bmatrix}\end{gathered}$$</p><p>å’Œæ ‡ç­¾çŸ©é˜µy and the label vector y:</p><p>$$\boldsymbol{y}\quad=\quad[y_1,\ldots,y_N]^T=\begin{bmatrix}y_1\y_2\\vdots\y_N\end{bmatrix}$$</p><h3 id=æœ€å°äºŒä¹˜è§£>æœ€å°äºŒä¹˜è§£</h3><p>å¯¹è®­ç»ƒæ•°æ®é›†ï¼Œçº¿æ€§æ¨¡å‹çš„æœ€å°MSEçš„è§£æè§£æœ‰å¦‚ä¸‹è¡¨ç¤ºï¼š</p><p>$\boldsymbol{w}_{best}=\left(\mathbf{X}^{T}\mathbf{X}\right)^{-1}\mathbf{X}^{T}y$</p><p>$(\mathbf{X}^T\mathbf{X})^{-1}$ åœ¨Xä¸­çš„æ‰€æœ‰åˆ—çº¿æ€§ç‹¬ç«‹æ—¶å­˜åœ¨ã€‚</p><h3 id=å…¶ä»–å›å½’æ¨¡å‹>å…¶ä»–å›å½’æ¨¡å‹</h3><p>çº¿æ€§æ¨¡å‹å’Œå¤šé¡¹å¼æ¨¡å‹å¹¶ä¸æ˜¯å”¯ä¸€å¯ç”¨çš„é€‰é¡¹ã€‚ Linear and polynomial models
are not the only options available.</p><p>å…¶ä»–å¯ä»¥ä½¿ç”¨çš„æ¨¡å‹å®¶æ—åŒ…æ‹¬ï¼š Other families of models that can be used
include:</p><ul><li><p>æŒ‡æ•°æ¨¡å‹ Exponential</p></li><li><p>æ­£å¼¦æ¨¡å‹ Sinusoids</p></li><li><p>å¾„å‘åŸºå‡½æ•° Radial basis functions</p></li><li><p>æ ·æ¡æ¨¡å‹ Splines</p></li></ul><p>æ•°å­¦å…¬å¼æ˜¯ç›¸åŒçš„ï¼Œåªæœ‰f(â‹…)çš„è¡¨è¾¾å¼å‘ç”Ÿå˜åŒ–ã€‚ The mathematical
formulation is identical and only the expression for f(â‹…) changes.</p><h3 id=é€»è¾‘å‡½æ•°>é€»è¾‘å‡½æ•°</h3><p>åœ¨æŸäº›é—®é¢˜ä¸­ï¼Œæ ‡ç­¾ä»£è¡¨ä¸€ä¸ªæ¯”ä¾‹æˆ–æ¦‚ç‡ï¼Œå³ä¸€ä¸ªä»‹äº0å’Œ1ä¹‹é—´çš„é‡ã€‚<br>In some problems, the label represents a proportion or a probability,
i.e.Â a quantity between 0 and 1.</p><p>æ­¤å¤–ï¼Œè¿™ä¸ªé‡å¯èƒ½ä¼šéšç€é¢„æµ‹å˜é‡çš„å¢åŠ è€Œå¢åŠ ã€‚<br>Moreover, this quantity might increase as the predictor increases.</p><p>åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé€»è¾‘å‡½æ•°å¯èƒ½ä¼šå¾ˆæœ‰ç”¨ã€‚<br>In such cases, the logistic function can be useful.</p><p>$p(d)=\frac{e^d}{1+e^d}=\frac{1}{1+e^{-d}}$</p><h3 id=å…¶ä»–è´¨é‡æŒ‡æ ‡>å…¶ä»–è´¨é‡æŒ‡æ ‡</h3><p>å‡æ–¹æ ¹RMSEã€è¡¨å¾é¢„æµ‹å€¼è¯¯å·®çš„æ ‡å‡†å·®ã€‘ï¼š$E_{RMSE}=\sqrt{\frac{1}{N}\sum e_i^2}$</p><p>MAEã€è¡¨å¾é¢„æµ‹å€¼çš„è¯¯å·®çš„ç»å¯¹å€¼å¹³å‡ã€‘ï¼š$E_{MAE}=\frac{1}{N}\sum|e_i|$</p><p>Ræ–¹ã€æµ‹é‡å“åº”ä¸­å¯ä»é¢„æµ‹å˜é‡é¢„æµ‹çš„æ–¹å·®æ¯”ä¾‹ã€‘ï¼š$E_R=1-\frac{\sum e_i^2}{\sum(y_i-\bar{y})^2}\mathrm{,where~}\bar{y}=\frac{1}{N}\sum y_i$</p><h3 id=çµæ´»æ€§-flexibility>çµæ´»æ€§ Flexibility</h3><p>æ¨¡å‹å…è®¸æˆ‘ä»¬é€šè¿‡è°ƒæ•´å…¶å‚æ•°ç”Ÿæˆå¤šç§å½¢çŠ¶ã€‚ Models allow us to generate
multiple shapes by tuning their parameters.</p><p>æˆ‘ä»¬é€šè¿‡æ¨¡å‹çš„è‡ªç”±åº¦æˆ–å¤æ‚åº¦æ¥æè¿°å…¶ç”Ÿæˆä¸åŒå½¢çŠ¶çš„èƒ½åŠ›ï¼Œå³å…¶çµæ´»æ€§ã€‚ We
talk about the degrees of freedom or the complexity of a model to
describe its ability to generate different shapes, i.e.Â its flexibility.</p><p>æ¨¡å‹çš„çµæ´»æ€§ä¸å¯è§£é‡Šæ€§å’Œå‡†ç¡®æ€§ç›¸å…³ï¼Œå¹¶ä¸”åœ¨ä¸¤è€…ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚ The
flexibility of a model is related to its interpretability and accuracy,
and there is a trade-off between the two.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101213256579.png width=1123 height=571 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101213256579_hu_cdc21b8924e72898.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101213256579_hu_17cd1d540fbee5e7.png 1024w" loading=lazy alt=image-20250101213256579 class=gallery-image data-flex-grow=196 data-flex-basis=472px></p><p>æ¨¡å‹çš„è®­ç»ƒæ•°æ®é›†è´¨é‡ä¸å…¶çµæ´»æ€§ç›¸å…³ã€‚ The quality of a model on a
training dataset is also related to its flexibility.</p><p>åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œçµæ´»æ€§è¾ƒé«˜çš„æ¨¡å‹äº§ç”Ÿçš„è¯¯å·®é€šå¸¸è¾ƒä½ã€‚ During training, the
error produced by flexible models is in general lower.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101213247882.png width=1128 height=486 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101213247882_hu_720c007e84a4246b.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101213247882_hu_6821174f2d8fd75b.png 1024w" loading=lazy alt=image-20250101213247882 class=gallery-image data-flex-grow=232 data-flex-basis=557px></p><h3 id=å¯è§£é‡Šæ€§-interpretability>å¯è§£é‡Šæ€§ interpretability</h3><p>æ¨¡å‹çš„å¯è§£é‡Šæ€§å¯¹äºæˆ‘ä»¬äººç±»ä»¥å®šæ€§çš„æ–¹å¼ç†è§£é¢„æµ‹å™¨å¦‚ä½•æ˜ å°„åˆ°æ ‡ç­¾è‡³å…³é‡è¦ã€‚
Model interpretability is crucial for us, as humans, to understand in a
qualitative manner how a predictor is mapped to a label.</p><p>ä¸çµæ´»çš„æ¨¡å‹é€šå¸¸äº§ç”Ÿæ›´ç®€å•ä¸”æ›´å®¹æ˜“è§£é‡Šçš„è§£å†³æ–¹æ¡ˆã€‚ Inflexible models
produce solutions that are usually simpler and easier to interpret.</p><h3 id=æ³›åŒ–-generalisation>æ³›åŒ– Generalisation</h3><p>æ³›åŒ–èƒ½åŠ›æ˜¯æŒ‡æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤ŸæˆåŠŸåœ°å°†å­¦ä¹ é˜¶æ®µæ‰€æŒæ¡çš„çŸ¥è¯†åº”ç”¨åˆ°å®é™…éƒ¨ç½²ä¸­çš„èƒ½åŠ›ã€‚
Generalisation is the ability of our model to successfully translate
what we was learnt during the learning stage to deployment.</p><h3 id=æ¬ æ‹Ÿåˆä¸è¿‡æ‹Ÿåˆ>æ¬ æ‹Ÿåˆä¸è¿‡æ‹Ÿåˆ</h3><p>é€šè¿‡æ¯”è¾ƒæ¨¡å‹åœ¨è®­ç»ƒå’Œéƒ¨ç½²æœŸé—´çš„è¡¨ç°ï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ä¸‰ç§ä¸åŒçš„è¡Œä¸ºï¼š By
comparing the performance of models during training and deployment, we
can observe three different behaviours:</p><ul><li>æ¬ æ‹Ÿåˆï¼šè®­ç»ƒå’Œéƒ¨ç½²æ—¶äº§ç”Ÿè¾ƒå¤§çš„è¯¯å·®ã€‚æ¨¡å‹æ— æ³•æ•æ‰åˆ°æ½œåœ¨çš„æ¨¡å¼ã€‚è¿‡äºç®€å•çš„æ¨¡å‹ä¼šå¯¼è‡´æ¬ æ‹Ÿåˆã€‚
Underfitting: Large training and deployment errors are produced. The
model is unable to capture the underlying pattern. Rigid models lead
to underfitting.</li><li>è¿‡æ‹Ÿåˆï¼šè®­ç»ƒæ—¶äº§ç”Ÿè¾ƒå°çš„è¯¯å·®ï¼Œéƒ¨ç½²æ—¶äº§ç”Ÿè¾ƒå¤§çš„è¯¯å·®ã€‚æ¨¡å‹åœ¨è®°å¿†æ— å…³çš„ç»†èŠ‚ã€‚è¿‡äºå¤æ‚çš„æ¨¡å‹å’Œä¸è¶³çš„æ•°æ®ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆã€‚
Overfitting: Small errors are produced during training, large errors
during deployment. The model is memorising irrelevant details. Too
complex models and not enough data lead to overfitting.</li><li>æ°åˆ°å¥½å¤„ï¼šè®­ç»ƒå’Œéƒ¨ç½²æ—¶äº§ç”Ÿè¾ƒä½çš„è¯¯å·®ã€‚æ¨¡å‹èƒ½å¤Ÿé‡ç°æ½œåœ¨çš„æ¨¡å¼å¹¶å¿½ç•¥æ— å…³çš„ç»†èŠ‚ã€‚
Just right: Low training and deployment errors. The model is capable
of reproducing the underlying pattern and ignores irrelevant
details.</li></ul><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101213503808.png width=1111 height=723 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101213503808_hu_b060a0e9d8877103.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101213503808_hu_1bd3dcd72b7f24ce.png 1024w" loading=lazy alt=image-20250101213503808 class=gallery-image data-flex-grow=153 data-flex-basis=368px></p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101213509344.png width=1090 height=729 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101213509344_hu_87f51c6ab0ef8eae.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101213509344_hu_9465aa898cce3629.png 1024w" loading=lazy alt=image-20250101213509344 class=gallery-image data-flex-grow=149 data-flex-basis=358px></p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101213516014.png width=1071 height=730 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101213516014_hu_1ea0e0725aae7d93.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101213516014_hu_e13741355f03b4a6.png 1024w" loading=lazy alt=image-20250101213516014 class=gallery-image data-flex-grow=146 data-flex-basis=352px></p><p>æ³›åŒ–èƒ½åŠ›åªèƒ½é€šè¿‡<strong>æ¯”è¾ƒè®­ç»ƒå’Œéƒ¨ç½²æ€§èƒ½</strong>æ¥è¯„ä¼°ï¼Œè€Œ<strong>ä¸ä»…ä»…æ˜¯çœ‹æ¯ä¸ªæ¨¡å‹å¯¹è®­ç»ƒæ•°æ®</strong>çš„æ‹Ÿåˆç¨‹åº¦ã€‚|
Generalisation can only be assessed by comparing training and deployment
performance, not by just looking at how each model fits the training
data.</p><h2 id=æ–¹æ³•è®º-week-1-4>æ–¹æ³•è®º ã€Week 1-4ã€‘</h2><h3 id=ä»ç›®æ ‡é‡‡æ ·æ•°æ®é›†>ä»ç›®æ ‡é‡‡æ ·æ•°æ®é›†</h3><ul><li>æ•°æ®é›†éœ€è¦<strong>å…·æœ‰ä»£è¡¨æ€§</strong>ï¼Œå³æä¾›ç›®æ ‡ç¾¤ä½“çš„å®Œæ•´ç”»é¢ã€‚<br>Datasets are representative, i.e., provide a complete picture of the
target population.\</li><li>é‡‡æ ·éœ€æ¨¡æ‹Ÿéƒ¨ç½²æœŸé—´ç”Ÿæˆæ ·æœ¬çš„æœºåˆ¶ï¼šæ ·æœ¬éœ€è¦ç‹¬ç«‹æå–ã€‚<br>Sampling mimics the mechanism that generates samples during
deployment: Samples need to be extracted independently.</li></ul><p>æ ·æœ¬éœ€è¦<strong>ç‹¬ç«‹ä¸”åŒåˆ†å¸ƒï¼ˆiidï¼‰</strong>ã€‚<br>Samples need to be <strong>independent and identically distributed (iid).</strong></p><h3 id=è¯„ä¼°éƒ¨ç½²æ—¶æ€§èƒ½>è¯„ä¼°éƒ¨ç½²æ—¶æ€§èƒ½</h3><p>æ¯ä¸ªæœºå™¨å­¦ä¹ é¡¹ç›®éƒ½éœ€è¦åŒ…å«ä¸€ä¸ª<strong>åœ¨éƒ¨ç½²æœŸé—´</strong>è¯„ä¼°æ¨¡å‹æ€§èƒ½çš„ç­–ç•¥ã€‚<br>Every machine learning project needs to include a strategy to evaluate
the performance of a model during deployment.</p><p>åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæ€§èƒ½è¯„ä¼°ç­–ç•¥åŒ…æ‹¬ï¼š<br>In machine learning, a performance evaluation strategy includes:</p><ol><li><p>ç”¨äºé‡åŒ–æ€§èƒ½çš„<strong>è´¨é‡æŒ‡æ ‡ã€‚</strong>\</p></li><li><p>A quality metric used to quantify the performance.</p></li><li><p>å¦‚ä½•åˆ©ç”¨<strong>æ•°æ®</strong>æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚\</p></li><li><p>How data will be used to assess the performance of a model.</p></li></ol><p>æ€§èƒ½è¯„ä¼°ç­–ç•¥å¿…é¡»åœ¨åˆ›å»ºæ¨¡å‹<strong>ä¹‹å‰</strong>è®¾è®¡ï¼Œä»¥é¿å…é™·å…¥æ•°æ®é™·é˜±ï¼Œä¾‹å¦‚ç¡®è®¤åå·®ã€‚<br>The performance evaluation strategy has to be designed before creating a
model to avoid falling into our own data-traps, such as confirmation
bias.</p><p>æˆ‘ä»¬ä½¿ç”¨æ•°æ®çš„å­é›†ï¼Œå³<strong>æµ‹è¯•æ•°æ®é›†</strong>ï¼Œæ¥è®¡ç®—æµ‹è¯•<strong>éƒ¨ç½²æ€§èƒ½</strong>ï¼Œä½œä¸ºçœŸå®æ€§èƒ½çš„<strong>ä¼°è®¡</strong>ã€‚
We use a subset of data, the test dataset, to compute the test
deployment performance as an estimation of the true performance.</p><p>æµ‹è¯•æ•°æ®é›†æ˜¯éšæœºæŠ½å–çš„ï¼Œå› æ­¤<strong>æµ‹è¯•æ€§èƒ½</strong>æœ¬èº«ä¹Ÿæ˜¯<strong>éšæœº</strong>çš„ï¼Œå› ä¸ºä¸åŒçš„æ•°æ®é›†é€šå¸¸ä¼šäº§ç”Ÿä¸åŒçš„å€¼ã€‚
Test datasets are extracted randomly. Hence, the test performance is
itself random, as different datasets generally produce different values.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101214245630.png width=1035 height=487 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101214245630_hu_e6b8c267126d6488.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101214245630_hu_a133878816762abd.png 1024w" loading=lazy alt=image-20250101214245630 class=gallery-image data-flex-grow=212 data-flex-basis=510px></p><p>æ¨¡å‹ç”±ä¸åŒå›¢é˜Ÿæ„å»ºï¼Œå¯ä»¥æ ¹æ®å®ƒä»¬çš„æµ‹è¯•æ€§èƒ½è¿›è¡Œæ¯”è¾ƒã€‚ Models built by
different teams can be compared based on their test performances.</p><p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæµ‹è¯•æ€§èƒ½æ˜¯ä¸€ä¸ªéšæœºé‡ï¼Œå› æ­¤æŸäº›æ¨¡å‹å¯èƒ½<strong>å¶ç„¶è¡¨ç°å¾—æ›´å¥½</strong>ï¼
Caution should be used, as the test performance is a random quantity,
hence some models might appear to be superior by chance!</p><h3 id=ä¼˜åŒ–ç†è®º>ä¼˜åŒ–ç†è®º</h3><p>ä¼˜åŒ–ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨æ‰€æœ‰å€™é€‰æ¨¡å‹ä¸­è¯†åˆ«å‡ºåœ¨ç›®æ ‡ç¾¤ä½“ä¸Šå®ç°æœ€é«˜è´¨é‡çš„æ¨¡å‹ï¼Œå³<strong>æœ€ä¼˜æ¨¡å‹</strong>
Optimization allows us to identify among all the candidate models the
one that achieves the highest quality on the target population, i.e.Â the
optimal model.</p><h3 id=è¯¯å·®æ›²é¢>è¯¯å·®æ›²é¢</h3><p>è¯¯å·®æ›²é¢ï¼ˆä¹Ÿç§°ä¸ºè¯¯å·®ã€ç›®æ ‡ã€æŸå¤±æˆ–æˆæœ¬å‡½æ•°ï¼‰ç”¨E(w)è¡¨ç¤ºï¼Œå®ƒå°†æ¯ä¸ªå€™é€‰æ¨¡å‹wæ˜ å°„åˆ°å…¶è¯¯å·®ã€‚
The error surface (a.k.a. error, objective, loss or cost function)
denoted by E(w) maps each candidate model w to its error.</p><p>æˆ‘ä»¬å‡è®¾å¯ä»¥é€šè¿‡ç›®æ ‡ç¾¤ä½“çš„ç†æƒ³æè¿°æ¥è·å¾—å®ƒã€‚ We will assume that we can
obtain it using the ideal description of our target population.</p><p>æœ€ä¼˜æ¨¡å‹å¯ä»¥é€šè¿‡æœ€ä½è¯¯å·®æ¥è¯†åˆ«ã€‚<br>The optimal model can be identified as the one with the lowest error.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101214643302.png width=781 height=532 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101214643302_hu_f773ba410bf82ee0.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101214643302_hu_19288c523b5f45d1.png 1024w" loading=lazy alt=image-20250101214643302 class=gallery-image data-flex-grow=146 data-flex-basis=352px></p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101214635667.png width=1059 height=441 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101214635667_hu_6e80887887da7e7c.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101214635667_hu_7ca06b608f778284.png 1024w" loading=lazy alt=image-20250101214635667 class=gallery-image data-flex-grow=240 data-flex-basis=576px></p><p>è¯¯å·®è¡¨é¢çš„æ¢¯åº¦ï¼ˆæ–œç‡ï¼‰åœ¨æœ€ä¼˜æ¨¡å‹å¤„ä¸ºé›¶ã€‚<br>The gradient (slope) of the error surface is zero at the optimal model.</p><p>å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ‰¾åˆ°æ¢¯åº¦ä¸ºé›¶çš„ä½ç½®æ¥å¯»æ‰¾æœ€ä¼˜æ¨¡å‹ã€‚<br>Hence, we can look for it by identifying where the gradient is zero.</p><h3 id=æ¢¯åº¦ä¸‹é™-gradient-descent>æ¢¯åº¦ä¸‹é™ Gradient descent</h3><p>æ¢¯åº¦ä¸‹é™æ˜¯ä¸€ç§æ•°å€¼ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡<strong>è¿­ä»£æ›´æ–°</strong>æ¨¡å‹å‚æ•°ï¼Œåˆ©ç”¨è¯¯å·®æ›²é¢çš„æ¢¯åº¦è¿›è¡Œè°ƒæ•´ã€‚
Gradient descent is a numerical optimization method where we iteratively
update our model parameters using the gradient of the error surface.</p><p>æ¢¯åº¦æä¾›äº†è¯¯å·®å¢åŠ æœ€å¿«çš„æ–¹å‘ã€‚åˆ©ç”¨æ¢¯åº¦ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä»¥ä¸‹æ›´æ–°è§„åˆ™ï¼š The
gradient provides the direction along which the error increases the
most. Using the gradient, we can create the following update rule:</p><p>$w_\mathrm{new}=w_\mathrm{old}-\epsilon\nabla E(w_{old})$</p><p>å…¶ä¸­$\epsilon$è¢«ç§°ä¸º<strong>å­¦ä¹ ç‡æˆ–æ­¥é•¿</strong>ã€‚ where $\epsilon$ is known as the
learning rate or step size.</p><p>åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬è°ƒæ•´æ¨¡å‹çš„å‚æ•°wã€‚å› æ­¤ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¹Ÿè¢«ç§°ä¸º<strong>å‚æ•°è°ƒä¼˜</strong>ã€‚
With every iteration, we adjust the parameters w of our model. This is
why this process is also known as parameter tuning.</p><h3 id=å­¦ä¹ ç‡>å­¦ä¹ ç‡</h3><p>å­¦ä¹ ç‡$\epsilon$æ§åˆ¶æˆ‘ä»¬åœ¨æ¯æ¬¡æ¢¯åº¦ä¸‹é™è¿­ä»£ä¸­æ”¹å˜æ¨¡å‹å‚æ•°wçš„ç¨‹åº¦ã€‚<br>The learning rate $\epsilon$ controls how much we change the parameters
w of our model in each iteration of gradient descent.</p><ul><li>è¾ƒå°çš„$\epsilon$å€¼ä¼šå¯¼è‡´æ¨¡å‹æ”¶æ•›åˆ°æœ€ä¼˜è§£çš„é€Ÿåº¦è¾ƒæ…¢ã€‚<br>Small values of $\epsilon$ result in slow convergence to the optimal
model.\</li><li>è¾ƒå¤§çš„$\epsilon$å€¼å¯èƒ½ä¼šä½¿æ¨¡å‹é”™è¿‡æœ€ä¼˜è§£ã€‚<br>Large values of $\epsilon$ risk overshooting the optimal model.</li></ul><p>å¯ä»¥é‡‡ç”¨è‡ªé€‚åº”æ–¹æ³•ï¼Œä½¿å­¦ä¹ ç‡é€æ¸å‡å°ã€‚<br>Adaptive approaches can be implemented, where the value of the learning
rate decreases progressively.</p><h3 id=æ¢¯åº¦ä¸‹é™å¼€å§‹ä¸åœæ­¢>æ¢¯åº¦ä¸‹é™å¼€å§‹ä¸åœæ­¢</h3><p>æ¢¯åº¦ä¸‹é™å¼€å§‹æ—¶ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªåˆå§‹æ¨¡å‹ã€‚åˆå§‹æ¨¡å‹çš„é€‰æ‹©å¯èƒ½è‡³å…³é‡è¦ã€‚åˆå§‹å‚æ•°
w é€šå¸¸<strong>éšæœºé€‰æ‹©</strong>ï¼ˆä½†åœ¨åˆç†çš„å€¼èŒƒå›´å†…ï¼‰ã€‚<br>To start gradient descent, we need an initial model. The choice of the
initial model can be crucial. The initial parameters w are usually
chosen randomly (but within a sensible range of values).</p><p>ä¸€èˆ¬æ¥è¯´ï¼Œæ¢¯åº¦ä¸‹é™ä¸ä¼šè¾¾åˆ°æœ€ä¼˜æ¨¡å‹ï¼Œå› æ­¤éœ€è¦è®¾è®¡ä¸€ä¸ªåœæ­¢ç­–ç•¥ã€‚å¸¸è§çš„é€‰æ‹©åŒ…æ‹¬ï¼š<br>In general, gradient descent will not reach the optimal model, hence it
is necessary to design a stopping strategy. Common choices include:</p><ul><li><p>è¿­ä»£æ¬¡æ•°ã€‚<br>Number of iterations.\</p></li><li><p>å¤„ç†æ—¶é—´ã€‚<br>Processing time.</p></li><li><p>è¯¯å·®å€¼ã€‚<br>Error value.</p></li><li><p>è¯¯å·®å€¼çš„ç›¸å¯¹å˜åŒ–ã€‚<br>Relative change of the error value.</p></li></ul><h3 id=å±€éƒ¨è§£ä¸å…¨å±€è§£>å±€éƒ¨è§£ä¸å…¨å±€è§£</h3><p>è¯¯å·®è¡¨é¢å¯ä»¥æ˜¯å¤æ‚çš„ï¼Œå¹¶ä¸”å…·æœ‰ Error surfaces can be complex and have</p><ul><li>å±€éƒ¨æœ€ä¼˜ï¼ˆåœ¨æŸä¸ªåŒºåŸŸå†…è¯¯å·®æœ€å°çš„æ¨¡å‹ï¼‰ã€‚ local optima (the model
with the lowest error within a region).</li><li>å…¨å±€æœ€ä¼˜ï¼ˆåœ¨æ‰€æœ‰æ¨¡å‹ä¸­è¯¯å·®æœ€å°çš„æ¨¡å‹ï¼‰ã€‚ Global optima (the model
with the lowest error among all the models).</li></ul><p>æ¢¯åº¦ä¸‹é™å¯èƒ½ä¼šé™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚ä¸ºäº†é¿å…è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬å¯ä»¥ä»å¤šä¸ªåˆå§‹æ¨¡å‹å¼€å§‹é‡å¤è¯¥è¿‡ç¨‹ï¼Œå¹¶é€‰æ‹©æœ€ä½³ç»“æœã€‚
Gradient descent can get stuck in local optima. To avoid them, we can
repeat the procedure from several initial models and select the best.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101215408022.png width=739 height=421 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101215408022_hu_5090d0fbfb9e5e69.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101215408022_hu_e16f5e2a3606fe44.png 1024w" loading=lazy alt=image-20250101215408022 class=gallery-image data-flex-grow=175 data-flex-basis=421px></p><h3 id=è®­ç»ƒmlæ¨¡å‹>è®­ç»ƒMLæ¨¡å‹</h3><p>ç°åœ¨é—®é¢˜æ˜¯ï¼Œè¯¯å·®æ›²é¢ä¸æ˜¯ç»™å®šçš„ï¼Œæˆ‘ä»¬åªæœ‰æ•°æ®ã€‚</p><p>æˆ‘ä»¬ä½¿ç”¨æ•°æ®çš„ä¸€ä¸ªå­é›†ï¼Œå³è®­ç»ƒæ•°æ®é›†ï¼Œæ¥ï¼ˆéšå¼æˆ–æ˜¾å¼ï¼‰é‡å»ºä¼˜åŒ–è¿‡ç¨‹ä¸­æ‰€éœ€çš„è¯¯å·®è¡¨é¢ã€‚æˆ‘ä»¬å°†å…¶ç§°ä¸ºç»éªŒè¯¯å·®è¡¨é¢ã€‚
We use a subset of data, known as the training dataset, to (implicitly
or explicitly) reconstruct the error surface needed during optimisation.
We will call this the empirical error surface.</p><p>ç»éªŒè¯¯å·®å’ŒçœŸå®è¯¯å·®è¡¨é¢é€šå¸¸ä¸åŒã€‚<br>The empirical and true error surfaces are in general different.</p><p>å› æ­¤ï¼Œå®ƒä»¬çš„æœ€ä¼˜æ¨¡å‹å¯èƒ½ä¸åŒï¼Œå³è®­ç»ƒæ•°æ®é›†ä¸Šçš„æœ€ä½³æ¨¡å‹å¯èƒ½ä¸æ˜¯æ€»ä½“ä¸Šçš„æœ€ä½³æ¨¡å‹ã€‚<br>Hence, their optimal models might differ, i.e.Â the best model for the
training dataset might not be the best for the population.</p><h3 id=è®­ç»ƒé›†ä¸æœ€å°å‡æ–¹>è®­ç»ƒé›†ä¸æœ€å°å‡æ–¹</h3><p>å¯ä»¥åœ¨è®­ç»ƒé›†ä¸Šä½¿ç”¨æœ€å°å‡æ–¹ä½œä¸ºæŒ‡æ ‡æ¥ä¼˜åŒ–æ¨¡å‹ã€‚</p><p>$\hat{y}=\mathrm{X}w$</p><p>å¾—åˆ°å‡æ–¹å‡½æ•°</p><p>$\begin{aligned}E_{MSE}(w)&=\quad\frac{1}{N}\left(y-\hat{y}\right)^T\left(y-\hat{y}\right)\&=\quad\frac{1}{N}\left(y-\mathrm{X}w\right)^T\left(y-\mathrm{X}w\right)\end{aligned}$</p><p>å¾—åˆ°å‡æ–¹å‡½æ•°çš„æ¢¯åº¦ï¼š</p><p>$\nabla E_{MSE}(w)=\frac{-2}{N}\mathrm{X}^T\left(y-\mathrm{X}w\right)$</p><p>æ­¤æ¢¯åº¦åœ¨$w=\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^Ty.$æ—¶ä¸ºé›¶ã€‚</p><h3 id=æš´åŠ›ç©·ä¸¾>æš´åŠ›ç©·ä¸¾</h3><p>é€šå¸¸ï¼Œæˆ‘ä»¬æ— æ³•è·å¾—è§£æè§£ã€‚<br>In general, we will not have analytical solutions.</p><p>æˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨è®­ç»ƒæ•°æ®ä¸Šè¯„ä¼°æ¯ä¸ªæ¨¡å‹æ¥é‡å»ºç»éªŒè¯¯å·®è¡¨é¢ã€‚<br>We can reconstruct the empirical error surface by evaluating each model
on training data.</p><p>è¿™è¢«ç§°ä¸ºæš´åŠ›æœç´¢æˆ–ç©·ä¸¾æœç´¢ã€‚<br>This is called brute-force or exhaustive search.</p><p>æ–¹æ³•ç®€å•ï¼Œä½†é€šå¸¸ä¸å®ç”¨ã€‚<br>Simple, but often impractical.</p><h3 id=ç”±æ•°æ®é©±åŠ¨çš„æ¢¯åº¦ä¸‹é™>ç”±æ•°æ®é©±åŠ¨çš„æ¢¯åº¦ä¸‹é™</h3><p>æ¢¯åº¦ä¸‹é™å¯ä»¥é€šè¿‡ä½¿ç”¨è®­ç»ƒæ•°æ®é›†æ¥ä¼°è®¡æ¢¯åº¦æ¥å®ç°ã€‚<br>Gradient descent can be implemented by estimating the gradient using our
training dataset.</p><p>åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œä½¿ç”¨è®­ç»ƒæ•°æ®é›†çš„ä¸€ä¸ªå­é›†ï¼ˆæ‰¹æ¬¡ï¼‰æ¥è®¡ç®—è¯¯å·®è¡¨é¢çš„æ¢¯åº¦ã€‚<br>During each iteration, a subset (<strong>batch</strong>) of the training dataset is
used to compute the gradient of the error surface.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101221104144.png width=961 height=376 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101221104144_hu_fdd8f278caf01f70.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101221104144_hu_b235a6b195806c37.png 1024w" loading=lazy alt=image-20250101221104144 class=gallery-image data-flex-grow=255 data-flex-basis=613px></p><p>æ ¹æ®æ¯æ¬¡è¿­ä»£ä¸­ä½¿ç”¨çš„æ•°æ®é‡ï¼Œé€šå¸¸ï¼ˆå°½ç®¡å¹¶ä¸ååˆ†æœ‰ç”¨ï¼‰å¯ä»¥åŒºåˆ†ä»¥ä¸‹å‡ ç§æ–¹æ³•ï¼š
Depending on the amount of data used in each iteration, it is common
(although not really useful) to distinguish between:</p><ul><li><p>æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆä½¿ç”¨æ•´ä¸ªè®­ç»ƒæ•°æ®é›†ï¼‰ã€‚ Batch gradient descent (the
whole training dataset is used).</p></li><li><p>éšæœºï¼ˆæˆ–åœ¨çº¿ï¼‰æ¢¯åº¦ä¸‹é™ï¼ˆä½¿ç”¨ä¸€ä¸ªæ ·æœ¬ï¼‰ã€‚ Stochastic (or online)
gradient descent (one sample is used).</p></li><li><p>å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆä½¿ç”¨è®­ç»ƒæ•°æ®é›†ä¸­çš„ä¸€å°éƒ¨åˆ†ï¼‰ã€‚ Mini-batch gradient
descent (a small subset from the training dataset is used).</p></li></ul><p>æ›´å®ç”¨çš„æ–¹æ³•æ˜¯è®¨è®º<strong>æ‰¹é‡å¤§å°</strong>ï¼ˆä¸€ä¸ªä»‹äº1å’Œè®­ç»ƒæ•°æ®é›†å¤§å°ä¹‹é—´çš„æ•°å­—ï¼‰ã€‚æ— è®ºæ‰¹é‡å¤§å°çš„å€¼å¦‚ä½•ï¼Œæˆ‘ä»¬éƒ½ä¼šä½¿ç”¨<strong>éšæœºæ¢¯åº¦ä¸‹é™</strong>è¿™ä¸ªæœ¯è¯­æ¥æŒ‡ä»£è¿™ç§æ–¹æ³•ã€‚
It is more useful to talk about the batch size (a number between 1 and
the size of the training dataset). Irrespective of the value of the
batch size, we will use the term stochastic gradient descent for this
approach.</p><p>å°æ‰¹é‡ä¼šäº§ç”Ÿç»éªŒè¯¯å·®è¡¨é¢æ¢¯åº¦çš„å™ªå£°ç‰ˆæœ¬ï¼Œè¿™æœ‰åŠ©äºé€ƒç¦»å±€éƒ¨æœ€å°å€¼ã€‚ Small
batches produce noisy versions of the gradient of the empirical error
surface, which can help to escape local minima.</p><h3 id=å…¶ä»–åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•>å…¶ä»–åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•</h3><p>éšæœºæ¢¯åº¦ä¸‹é™æ˜¯æœ€å¸¸ç”¨çš„ä¼˜åŒ–ç®—æ³•ï¼Œå°½ç®¡æœ‰æ—¶å¯èƒ½è¾ƒæ…¢ã€‚<br>Stochastic gradient descent is the most used optimisation algorithm,
although it can sometimes be slow.</p><p>å…¶ä»–æµè¡Œçš„åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–ç®—æ³•åŒ…æ‹¬ï¼š<br>Other popular gradient-based optimisation algorithms include:</p><ul><li><strong>åŠ¨é‡æ³•</strong>å®šä¹‰äº†æ›´æ–°æ­¥éª¤çš„é€Ÿåº¦ï¼ˆæ–¹å‘å’Œå¤§å°ï¼‰ï¼Œè¿™å–å†³äºè¿‡å»çš„æ¢¯åº¦ã€‚<br><strong>Momentum</strong> defines a velocity (direction and speed) for the update
step, which depends on past gradients.\</li><li><strong>RMSProp</strong>é€šè¿‡ä½¿ç”¨è¿‡å»çš„æ¢¯åº¦æ¥ç¼©æ”¾å­¦ä¹ ç‡ã€‚<br><strong>RMSProp</strong> adapts the learning rate by scaling them using the past
gradients.\</li><li><strong>Adam</strong>ç»“åˆäº†åŠ¨é‡æ³•å’ŒRMSPropçš„ä¸€äº›ç‰¹æ€§ã€‚<br><strong>Adam</strong> combines some features from the Momentum and RMSProp
approaches.</li></ul><h3 id=ç»éªŒè¯¯å·®æ›²é¢å’Œè¿‡æ‹Ÿåˆ>ç»éªŒè¯¯å·®æ›²é¢å’Œè¿‡æ‹Ÿåˆ</h3><p>ç»éªŒè¯¯å·®æ›²é¢å’ŒçœŸå®è¯¯å·®æ›²é¢é€šå¸¸æ˜¯ä¸åŒçš„ã€‚<br>The empirical and true error surfaces are in general different.</p><p>å½“ä½¿ç”¨å°æ•°æ®é›†å’Œå¤æ‚æ¨¡å‹æ—¶ï¼Œä¸¤è€…ä¹‹é—´çš„å·®å¼‚å¯èƒ½éå¸¸å¤§ï¼Œå¯¼è‡´è®­ç»ƒå‡ºçš„æ¨¡å‹åœ¨ç»éªŒè¯¯å·®æ›²é¢ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†åœ¨çœŸå®è¯¯å·®æ›²é¢ä¸Šè¡¨ç°å¾ˆå·®ã€‚<br>When small datasets and complex models are used, the differences between
the two can be very large, resulting in trained models that work very
well for the empirical error surface but very poorly for the true error
surface.</p><p>è¿™å½“ç„¶æ˜¯ä»å¦ä¸€ä¸ªè§’åº¦æ¥çœ‹<strong>è¿‡æ‹Ÿåˆ</strong>é—®é¢˜ã€‚<br>This is, of course, another way of looking at <strong>overfitting</strong>.</p><p>é€šè¿‡å¢åŠ è®­ç»ƒæ•°æ®é›†çš„è§„æ¨¡ï¼Œç»éªŒè¯¯å·®æ›²é¢ä¼šæ¥è¿‘çœŸå®è¯¯å·®æ›²é¢ï¼Œä»è€Œé™ä½è¿‡æ‹Ÿåˆçš„é£é™©ã€‚<br>By increasing the size of the training dataset, empirical error surfaces
become closer to the true error surface and the risk of overfitting
decreases.</p><p><strong>åˆ‡å‹¿</strong>ä½¿ç”¨ç›¸åŒçš„æ•°æ®è¿›è¡Œæµ‹è¯•å’Œè®­ç»ƒæ¨¡å‹ã€‚<br>Never use the same data for testing and training a model.</p><p>æµ‹è¯•æ•°æ®é›†éœ€è¦ä¿æŒ<strong>ä¸å¯è®¿é—®</strong>ï¼Œä»¥é¿å…åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ˆæœ‰æ„æˆ–æ— æ„åœ°ï¼‰ä½¿ç”¨å®ƒã€‚<br>The test dataset needs to remain inaccessible to avoid using it
(inadvertently or not) during training.</p><h3 id=æ­£åˆ™åŒ–-regularisation>æ­£åˆ™åŒ– Regularisation</h3><p>æ­£åˆ™åŒ–é€šè¿‡æ·»åŠ ä¸€ä¸ªçº¦æŸæ¨¡å‹å‚æ•°å–å€¼çš„é¡¹æ¥ä¿®æ”¹ç»éªŒè¯¯å·®è¡¨é¢ã€‚
Regularisation modifies the empirical error surface by adding a term
that constrains the values that the model parameters can take on.</p><p>$E_R(w)=E(w)+\lambda w^Tw$</p><p>ä¾‹å¦‚ï¼ŒMSEçš„æ­£åˆ™åŒ–ï¼š</p><p>$E_{MSE+R}=\frac{1}{N}\sum_{i=1}^Ne_i^2+\lambda\sum_{i=1}^Kw_k^2$</p><p>è§£ä¸ºï¼š$w=\left(\mathbf{X}^T\mathbf{X}+N\lambda\mathbf{I}\right)^{-1}\mathbf{X}^Ty$</p><p>éšç€æ­£åˆ™åŒ–å¼ºåº¦çš„å¢åŠ ï¼Œæ‰€å¾—è§£çš„å¤æ‚åº¦é™ä½ï¼Œè¿‡æ‹Ÿåˆçš„é£é™©ä¹Ÿéšä¹‹å‡å°ã€‚ As
the regularization strength increases, the complexity of the resulting
solution decreases, and so does the risk of overfitting.</p><h3 id=è®­ç»ƒæœŸé—´æˆæœ¬ä¸è´¨é‡>ï¼ˆè®­ç»ƒæœŸé—´ï¼‰æˆæœ¬ä¸è´¨é‡</h3><p>æ­£åˆ™åŒ–æä¾›äº†ä¸€ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨äº†ä¸€ç§è´¨é‡æ¦‚å¿µï¼ˆEMSE+Rï¼‰ï¼Œè€Œåœ¨éƒ¨ç½²æœŸé—´ä½¿ç”¨äº†ä¸åŒçš„è´¨é‡æ¦‚å¿µï¼ˆEMSEï¼‰ã€‚è¿™å¬èµ·æ¥æ˜¯ä¸æ˜¯æœ‰ç‚¹å¥‡æ€ªï¼Ÿ
Regularisation provides an example where we use a notion of quality
during training (EMSE+R) that is different from the notion of quality
during deployment (EMSE). Doesn&rsquo;t it sound strange?</p><p>æˆ‘ä»¬çš„ç›®æ ‡å§‹ç»ˆæ˜¯ç”Ÿæˆä¸€ä¸ªåœ¨éƒ¨ç½²æœŸé—´è¾¾åˆ°æœ€é«˜è´¨é‡çš„æ¨¡å‹ã€‚æˆ‘ä»¬å¦‚ä½•å®ç°å®ƒï¼Œåˆ™æ˜¯å¦ä¸€ä¸ªé—®é¢˜ã€‚
Our goal is always to produce a model that achieves the highest quality
during deployment. How we achieve it, is a different question.</p><p>è¯¸å¦‚è¿‡æ‹Ÿåˆç­‰å› ç´ å¯èƒ½å¯¼è‡´æ¨¡å‹åœ¨è®­ç»ƒæœŸé—´è¡¨ç°æœ€ä¼˜ï¼Œä½†åœ¨éƒ¨ç½²æœŸé—´å´å¹¶éå¦‚æ­¤ã€‚ä¸€ä¸ªè®¾è®¡è‰¯å¥½çš„è®­ç»ƒæœŸé—´è´¨é‡æ¦‚å¿µå¯ä»¥ç”Ÿæˆåœ¨éƒ¨ç½²æœŸé—´è¡¨ç°æ›´å¥½çš„æ¨¡å‹ã€‚
Factors such as overfitting might result in models that are optimal
during training, but not deployment. A well-designed notion of quality
during training can produce models that perform better during
deployment.</p><p>æˆ‘ä»¬é€šå¸¸å°†è®­ç»ƒæœŸé—´çš„è´¨é‡æ¦‚å¿µç§°ä¸º<strong>æˆæœ¬æˆ–ç›®æ ‡å‡½æ•°</strong>ï¼Œä»¥åŒºåˆ«äº<strong>ç›®æ ‡è´¨é‡æŒ‡æ ‡</strong>ã€‚
We usually call our notion of quality during training cost or objective
function, to distinguish it from the target quality metric.</p><h3 id=éªŒè¯æ¨¡å‹>éªŒè¯æ¨¡å‹</h3><p>éªŒè¯æ–¹æ³•å…è®¸æˆ‘ä»¬ä½¿ç”¨æ•°æ®æ¥è¯„ä¼°å’Œé€‰æ‹©ä¸åŒçš„æ¨¡å‹å®¶æ—ã€‚ç”¨äºéªŒè¯çš„ç›¸åŒæ•°æ®éšåå¯ä»¥ç”¨äºè®­ç»ƒæœ€ç»ˆæ¨¡å‹ã€‚<br>Validation methods allow us to use data for assessing and selecting
different families of models. The same data used for validation can then
be used to train a final model.</p><p>éªŒè¯æ¶‰åŠæ¯ä¸ªæ¨¡å‹æ—çš„ä¸€è½®æˆ–å¤šè½®è®­ç»ƒå’Œæ€§èƒ½ä¼°è®¡ï¼Œéšåè¿›è¡Œæ€§èƒ½å¹³å‡ã€‚<br>Validation involves one or more training and performance estimation
rounds per model family followed by performance averaging.</p><p>éªŒè¯é›†æ–¹æ³•æ˜¯æœ€ç®€å•çš„æ–¹æ³•ã€‚å®ƒå°†å¯ç”¨æ•°æ®é›†éšæœºåˆ†ä¸º<strong>è®­ç»ƒé›†å’ŒéªŒè¯é›†</strong>ï¼ˆæˆ–ä¿ç•™é›†ï¼‰ã€‚
The validation set approach is the simplest method. It randomly splits
the available dataset into a training and a validation (or hold-out)
dataset.</p><p>æ¨¡å‹ä½¿ç”¨è®­ç»ƒéƒ¨åˆ†è¿›è¡Œæ‹Ÿåˆï¼ŒéªŒè¯éƒ¨åˆ†ç”¨äºä¼°è®¡å…¶æ€§èƒ½ã€‚ Models are fitted
with the training part and the validation part is used to estimate its
performance.</p><h3 id=éªŒè¯æ–¹æ³•æ•°æ®é›†åˆ‡åˆ†æ–¹æ³•>éªŒè¯æ–¹æ³•ï¼ˆæ•°æ®é›†åˆ‡åˆ†æ–¹æ³•ï¼‰</h3><p>éªŒè¯é›†æ–¹æ³•æ¶‰åŠä¸€æ¬¡è®­ç»ƒè½®æ¬¡ã€‚ç„¶è€Œï¼Œæ¨¡å‹ä½¿ç”¨è¾ƒå°‘çš„æ ·æœ¬è¿›è¡Œè®­ç»ƒï¼Œæœ€ç»ˆæ€§èƒ½ç”±äºéšæœºåˆ†å‰²è€Œå…·æœ‰é«˜åº¦å¯å˜æ€§ã€‚<br>The validation set approach involves one training round. Models are
however trained with fewer samples and the final performance is highly
variable due to random splitting.</p><ul><li>**ç•™ä¸€æ³•äº¤å‰éªŒè¯ï¼ˆLOOCVï¼ŒLeave-one-out
cross-validationï¼‰**éœ€è¦ä¸æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°é‡ç›¸åŒçš„è®­ç»ƒè½®æ¬¡ï¼Œç„¶è€Œåœ¨æ¯ä¸€è½®ä¸­å‡ ä¹æ‰€æœ‰çš„æ ·æœ¬éƒ½ç”¨äºè®­ç»ƒã€‚å®ƒå§‹ç»ˆæä¾›ç›¸åŒçš„æ€§èƒ½ä¼°è®¡ã€‚<br>LOOCV requires as many training rounds as samples there are in the
dataset, however in every round almost all the samples are used for
training. It always provides the same performance estimation.<br><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101222359889.png width=829 height=303 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101222359889_hu_9be530ca4907d0e5.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101222359889_hu_1e1aa5f751168cdb.png 1024w" loading=lazy alt=image-20250101222359889 class=gallery-image data-flex-grow=273 data-flex-basis=656px></li><li><strong>kæŠ˜äº¤å‰éªŒè¯</strong>æ˜¯æœ€æµè¡Œçš„æ–¹æ³•ï¼ˆå°†æ•°æ®é›†åˆ‡åˆ†æˆKä»½ï¼‰ã€‚å®ƒæ¯”ç•™ä¸€æ³•äº¤å‰éªŒè¯æ¶‰åŠæ›´å°‘çš„è®­ç»ƒè½®æ¬¡ã€‚ä¸éªŒè¯é›†æ–¹æ³•ç›¸æ¯”ï¼Œæ€§èƒ½ä¼°è®¡çš„å˜å¼‚æ€§æ›´å°ï¼Œå¹¶ä¸”æ›´å¤šçš„æ ·æœ¬ç”¨äºè®­ç»ƒã€‚<br>k-fold is the most popular approach. It involves fewer training
rounds than LOOCV. Compared to the validation set approach, the
performance estimation is less variable and more samples are used
for training.
<img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101222409990.png width=801 height=292 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101222409990_hu_9a29de16415910e0.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101222409990_hu_5dd49184b8d1822c.png 1024w" loading=lazy alt=image-20250101222409990 class=gallery-image data-flex-grow=274 data-flex-basis=658px></li></ul><h2 id=åˆ†ç±»-classication-i-week-2-1>åˆ†ç±» Classi cation I ã€Week 2-1ã€‘</h2><h3 id=é—®é¢˜å½¢æˆ-1>é—®é¢˜å½¢æˆ</h3><p>åœ¨æœºå™¨å­¦ä¹ åˆ†ç±»é—®é¢˜ä¸­ï¼š<br>In a machine learning classification problem:</p><p>æˆ‘ä»¬ä½¿ç”¨æ•°æ®é›† ${(x_i,y_i):1\leq i\leq N}$ æ„å»ºæ¨¡å‹ $\hat{y}=f(x)$ã€‚<br>We build a model $\hat{y}=f(x)$ using a dataset
${(x_i,y_i):1\leq i\leq N}$.</p><p>æˆ‘ä»¬æœ‰ä¸€ä¸ªæ¨¡å‹è´¨é‡çš„æ¦‚å¿µã€‚<br>We have a notion of model quality.</p><p>å¯¹ $(x_i,y_i)$ å¯ä»¥ç†è§£ä¸º"æ ·æœ¬ $i$ å±äºç±»åˆ« $y_i$"ï¼Œæˆ–è€…"æ ·æœ¬ $i$
çš„æ ‡ç­¾æ˜¯ $y_i$"ã€‚<br>The pair $(x_i,y_i)$ can be read as &ldquo;sample $i$ belongs to class $y_i$&rdquo;,
or &ldquo;the label of sample $i$ is $y_i$&rdquo;.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101223110302.png width=943 height=201 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101223110302_hu_fcabed87352e6084.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101223110302_hu_d264986f78993b75.png 1024w" loading=lazy alt=image-20250101223110302 class=gallery-image data-flex-grow=469 data-flex-basis=1125px></p><p>åˆ†ç±»é—®é¢˜æœ‰äºŒåˆ†ç±»ã€å¤šç±»åˆ†ç±»ç­‰ç±»å‹ã€‚æ•°æ®é›†æˆä¸ºé¢„æµ‹å˜é‡å’Œæ ‡ç­¾å¯¹ã€‚</p><h3 id=åœ¨æ ‡ç­¾ç©ºé—´ä¸‹çš„æ•°æ®é›†>åœ¨æ ‡ç­¾ç©ºé—´ä¸‹çš„æ•°æ®é›†</h3><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101223627095.png width=1156 height=478 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101223627095_hu_1854d82c44994ef5.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101223627095_hu_d2dcba9cc45db58c.png 1024w" loading=lazy alt=image-20250101223627095 class=gallery-image data-flex-grow=241 data-flex-basis=580px></p><h3 id=åœ¨é¢„æµ‹å˜é‡ç©ºé—´ä¸‹çš„æ•°æ®é›†>åœ¨é¢„æµ‹å˜é‡ç©ºé—´ä¸‹çš„æ•°æ®é›†</h3><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101223643659.png width=1083 height=483 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101223643659_hu_c3b370d203b7d474.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101223643659_hu_19fcd60425d41a40.png 1024w" loading=lazy alt=image-20250101223643659 class=gallery-image data-flex-grow=224 data-flex-basis=538px></p><h3 id=åˆ¤å†³åŸŸ>åˆ¤å†³åŸŸ</h3><p>åœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨é¢„æµ‹ç©ºé—´ä¸­çš„<strong>å†³ç­–åŒºåŸŸ</strong>çš„æ¦‚å¿µã€‚<br>In classification problems, we use the notion of <strong>decision regions</strong> in
the predictor space.</p><ul><li>å†³ç­–åŒºåŸŸç”±<strong>ä¸ç›¸åŒæ ‡ç­¾ç›¸å…³è”çš„ç‚¹</strong>ç»„æˆã€‚<br>A decision region is made up of points that are associated to the
same label.\</li><li>å¯ä»¥é€šè¿‡è¯†åˆ«å…¶<strong>è¾¹ç•Œ</strong>æ¥å®šä¹‰åŒºåŸŸã€‚<br>Regions can be defined by identifying their boundaries.\</li><li>åˆ†ç±»ä¸­çš„è§£å†³æ–¹æ¡ˆæ¨¡å‹æ˜¯å°†é¢„æµ‹ç©ºé—´<strong>åˆ’åˆ†ä¸ºç”±å†³ç­–è¾¹ç•Œåˆ†éš”çš„</strong>å†³ç­–åŒºåŸŸã€‚<br>A solution model in classification is a partition of the predictor
space into decision regions separated by decision boundaries.</li></ul><h3 id=çº¿æ€§åˆ†ç±»å™¨>çº¿æ€§åˆ†ç±»å™¨</h3><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101223938774.png width=1128 height=400 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101223938774_hu_31ca125b11ddf54d.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101223938774_hu_ea880e8ccf9f8f12.png 1024w" loading=lazy alt=image-20250101223938774 class=gallery-image data-flex-grow=282 data-flex-basis=676px></p><p>çº¿æ€§åˆ†ç±»å™¨ä½¿ç”¨å†³ç­–åŒºåŸŸä¹‹é—´çš„çº¿æ€§è¾¹ç•Œï¼š<br>Linear classifiers use linear boundaries between decision regions:</p><p>çº¿æ€§è¾¹ç•Œç”±çº¿æ€§æ–¹ç¨‹ $ w^T x = 0 $ å®šä¹‰ã€‚<br>Linear boundaries are defined by the linear equation $ w^T x = 0 $.</p><p>æ‰©å±•å‘é‡ $ x = [1, x_1, x_2&mldr;]^T $ åŒ…å«é¢„æµ‹å˜é‡ï¼Œ$ w $
æ˜¯ç³»æ•°å‘é‡ã€‚<br>The extended vector $ x = [1, x_1, x_2&mldr;]^T $ contains the
predictors and $ w $ is the coefficients vector.</p><p>ä¸ºäº†å¯¹æ ·æœ¬è¿›è¡Œåˆ†ç±»ï¼Œæˆ‘ä»¬åªéœ€ç¡®å®šå®ƒä½äºè¾¹ç•Œçš„å“ªä¸€ä¾§ã€‚<br>To classify a sample we simply identify the side of the boundary where
it lies.</p><p>å¦‚æœæˆ‘ä»¬çŸ¥é“çº¿æ€§è¾¹ç•Œçš„ç³»æ•°å‘é‡ $\mathbf{w}$ï¼Œåˆ†ç±»ä¸€ä¸ªæ ·æœ¬å°±éå¸¸ç®€å•ï¼š If
we know the coefficients vector $\mathbf{w}$ of a linear boundary,
classifying a sample is very simple:</p><ul><li><p>æ„å»ºæ‰©å±•å‘é‡ $\mathbf{x}_i$ã€‚</p></li><li><p>Build the extended vector $\mathbf{x}_i$.</p></li><li><p>è®¡ç®— $\mathbf{w}^T \mathbf{x}_i$ã€‚</p></li><li><p>Compute $\mathbf{w}^T \mathbf{x}_i$.</p></li><li><p><strong>ä½¿ç”¨ä»¥ä¸‹äº‹å®è¿›è¡Œåˆ†ç±»ï¼š</strong></p></li><li><p><strong>Classify using the following facts:</strong></p><ul><li><p>å¦‚æœ $\mathbf{w}^T \mathbf{x}_i > 0$ï¼Œæˆ‘ä»¬åœ¨è¾¹ç•Œçš„ä¸€ä¾§ã€‚</p></li><li><p>If $\mathbf{w}^T \mathbf{x}_i > 0$, we are on one side of the
boundary.</p></li><li><p>å¦‚æœ $\mathbf{w}^T \mathbf{x}_i &lt; 0$ï¼Œæˆ‘ä»¬åœ¨è¾¹ç•Œçš„å¦ä¸€ä¾§ï¼</p></li><li><p>If $\mathbf{w}^T \mathbf{x}_i &lt; 0$, we are on the other!</p></li><li><p>å¦‚æœ $\mathbf{w}^T \mathbf{x}_i = 0$&mldr;
æˆ‘ä»¬åœ¨å“ªé‡Œï¼Ÿï¼ˆæ°åœ¨è¾¹ç•Œä¸Šï¼‰</p></li><li><p>If $\mathbf{w}^T \mathbf{x}_i = 0$&mldr; where are we?</p></li></ul></li></ul><h3 id=åŸºæœ¬è´¨é‡æŒ‡æ ‡>åŸºæœ¬è´¨é‡æŒ‡æ ‡</h3><p>é€šè¿‡æ¯”è¾ƒé¢„æµ‹å€¼å’ŒçœŸå®æ ‡ç­¾ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ•°æ®é›†ä¸­è¯†åˆ«å‡ºï¼š By comparing
predictions and true labels, we can identify in a dataset:</p><ul><li>æ¯ä¸ªç±»åˆ«ä¸­æ­£ç¡®åˆ†ç±»çš„æ ·æœ¬ï¼ˆçœŸå®é¢„æµ‹ï¼‰ã€‚ The correctly classified
samples (true predictions) in each class.</li><li>æ¯ä¸ªç±»åˆ«ä¸­é”™è¯¯åˆ†ç±»çš„æ ·æœ¬ï¼ˆé”™è¯¯é¢„æµ‹ï¼‰ã€‚ The incorrectly classified
samples (false predictions) in each class.</li></ul><p>ä¸¤ä¸ªå¸¸è§ä¸”ç­‰æ•ˆçš„è´¨é‡æŒ‡æ ‡æ˜¯<strong>å‡†ç¡®ç‡Aå’Œé”™è¯¯ç‡ï¼ˆæˆ–è¯¯åˆ†ç±»ç‡ï¼‰E = 1 âˆ’
A</strong>ï¼Œå®šä¹‰ä¸ºï¼š Two common and equivalent notions of quality are the
accuracy A and the error (or misclassification) rate E = 1 âˆ’ A, defined
as:</p><p><strong>A = æ­£ç¡®åˆ†ç±»çš„æ ·æœ¬æ•° / æ€»æ ·æœ¬æ•°</strong> <strong>A = #correctly classified samples
/ #samples</strong></p><p><strong>E = é”™è¯¯åˆ†ç±»çš„æ ·æœ¬æ•° / æ€»æ ·æœ¬æ•°</strong> <strong>E = #incorrectly classified
samples / #samples</strong></p><h3 id=é€»è¾‘å›å½’æ¨¡å‹>é€»è¾‘å›å½’æ¨¡å‹</h3><p>ç»™å®šä¸€ä¸ªçº¿æ€§è¾¹ç•Œ $\mathbf{w}$ å’Œä¸€ä¸ªé¢„æµ‹å‘é‡ $\mathbf{x}_i$ï¼Œé‡
$\mathbf{w}^T \mathbf{x}_i$ å¯ä»¥è§£é‡Šä¸ºæ ·æœ¬åˆ°è¾¹ç•Œçš„<strong>è·ç¦»</strong>ã€‚ Given a
linear boundary $\mathbf{w}$ and a predictor vector $\mathbf{x}_i$, the
quantity $\mathbf{w}^T \mathbf{x}_i$ can be interpreted as the distance
from the sample to the boundary.</p><p>å¦‚æœæˆ‘ä»¬åœ¨é€»è¾‘å‡½æ•°ä¸­è®¾ $d = \mathbf{w}^T \mathbf{x}_i$ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š_
_If we set $d = \mathbf{w}^T \mathbf{x}_i$ in the logistic function, we
get:</p><p>$ p(<code>\mathbf{w}</code>{=tex}^T <code>\mathbf{x}</code>{=tex}_i) =
<code>\frac{e^{\mathbf{w}^T \mathbf{x}_i}}{1 + e^{\mathbf{w}^T \mathbf{x}_i}}</code>{=tex}
$</p><p>å¯¹äºå›ºå®šçš„ $\mathbf{w}$ï¼Œæˆ‘ä»¬ç®€è®°ä¸º $p(\mathbf{x}_i)$ï¼š_ _For a fixed
$\mathbf{w}$, we will simply denote it as $p(\mathbf{x}_i)$ to simplify
the notation:</p><ul><li><p>å½“ $\mathbf{w}^T \mathbf{x} \rightarrow \infty$ æ—¶ï¼Œé€»è¾‘å‡½æ•°
$p(\mathbf{x}_i) \rightarrow 1$</p></li><li><p>When $\mathbf{w}^T \mathbf{x} \rightarrow \infty$, the logistic
function $p(\mathbf{x}_i) \rightarrow 1$</p></li><li><p>å½“ $\mathbf{w}^T \mathbf{x} \rightarrow -\infty$ æ—¶ï¼Œé€»è¾‘å‡½æ•°
$p(\mathbf{x}_i) \rightarrow 0$</p></li><li><p>When $\mathbf{w}^T \mathbf{x} \rightarrow -\infty$, the logistic
function $p(\mathbf{x}_i) \rightarrow 0$</p></li></ul><p>çº¿æ€§åˆ†ç±»å™¨ $w$ å¯¹æ ·æœ¬è¿›è¡Œæ ‡è®°ï¼š<br>A linear classifier $w$ labels samples:</p><ul><li><p>å¦‚æœ $w^T x_i > 0$ï¼Œåˆ™æ ‡è®°ä¸º"æ­£ç±»"ã€‚\</p></li><li><p>If $w^T x_i > 0$, then label as &ldquo;positive&rdquo;.</p></li><li><p>å¦‚æœ $w^T x_i &lt; 0$ï¼Œåˆ™æ ‡è®°ä¸º"è´Ÿç±»"ã€‚\</p></li><li><p>If $w^T x_i &lt; 0$, then label as &ldquo;negative&rdquo;.</p></li></ul><p>å…³é”®ç‚¹ï¼š<br>Key points to notice:</p><ul><li><p>å¦‚æœ $w^T x_i = 0$ï¼ˆç‚¹ $x_i$ åœ¨è¾¹ç•Œä¸Šï¼‰ï¼Œåˆ™ $p(x_i) = 0.5$ã€‚\</p></li><li><p>If $w^T x_i = 0$ (the point $x_i$ is on the boundary), then
$p(x_i) = 0.5$.</p></li><li><p>å¦‚æœ $w^T x_i > 0$ï¼ˆç‚¹ $x_i$
åœ¨æ­£ç±»åŒºåŸŸï¼‰ï¼Œåˆ™éšç€è¿œç¦»è¾¹ç•Œï¼Œ$p(x_i) \rightarrow 1$ã€‚\</p></li><li><p>If $w^T x_i > 0$ (the point $x_i$ is in the positive region), then
$p(x_i) \rightarrow 1$ as it moves away from the boundary.</p></li><li><p>å¦‚æœ $w^T x_i &lt; 0$ï¼ˆç‚¹ $x_i$
åœ¨è´Ÿç±»åŒºåŸŸï¼‰ï¼Œåˆ™éšç€è¿œç¦»è¾¹ç•Œï¼Œ$p(x_i) \rightarrow 0$ã€‚\</p></li><li><p>If $w^T x_i &lt; 0$ (the point $x_i$ is in the negative region), then
$p(x_i) \rightarrow 0$ as it moves away from the boundary.</p></li></ul><p>å…³é”®ç»“è®ºï¼š<br>Here is the crucial point:</p><ul><li><p>$p(x_i)$ æ˜¯åˆ†ç±»å™¨å¯¹ $y_i$ ä¸ºæ­£ç±»çš„ç¡®ä¿¡åº¦ã€‚\</p></li><li><p>$p(x_i)$ is the classifier&rsquo;s certainty that $y_i$ is positive.</p></li><li><p>$1 - p(x_i)$ æ˜¯åˆ†ç±»å™¨å¯¹ $y_i$ ä¸ºè´Ÿç±»çš„ç¡®ä¿¡åº¦ã€‚\</p></li><li><p>$1 - p(x_i)$ is the classifier&rsquo;s certainty that $y_i$ is negative.</p></li></ul><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101225652001.png width=1069 height=432 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101225652001_hu_4359cf8753a07f27.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101225652001_hu_540d474513a9790d.png 1024w" loading=lazy alt=image-20250101225652001 class=gallery-image data-flex-grow=247 data-flex-basis=593px></p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101225827468.png width=1062 height=627 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101225827468_hu_8e9f5a725457862b.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250101225827468_hu_364cb985c0abb993.png 1024w" loading=lazy alt=image-20250101225827468 class=gallery-image data-flex-grow=169 data-flex-basis=406px></p><h3 id=å‚æ•°åŒ–ä¸éå‚æ•°åŒ–æ–¹æ³•>å‚æ•°åŒ–ä¸éå‚æ•°åŒ–æ–¹æ³•</h3><p>çº¿æ€§åˆ†ç±»å™¨å±äºå‚æ•°åŒ–æ–¹æ³•å®¶æ—ï¼šå‡è®¾ä¸€ç§å½¢çŠ¶ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯çº¿æ€§çš„ï¼‰ï¼Œå¹¶ä½¿ç”¨æˆ‘ä»¬çš„æ•°æ®é›†åœ¨æ‰€æœ‰å…·æœ‰é¢„é€‰å½¢çŠ¶çš„è¾¹ç•Œä¸­æ‰¾åˆ°æœ€ä½³è¾¹ç•Œã€‚<br>Linear classifiers belong to the family of parametric approaches: a
shape is assumed (in this case linear) and our dataset is used to find
the best boundary amongst all the boundaries with the preselected shape.</p><p>éå‚æ•°åŒ–æ–¹æ³•æä¾›äº†ä¸€ç§æ›´çµæ´»çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå› ä¸ºå®ƒä»¬ä¸å‡è®¾ä»»ä½•ç±»å‹çš„è¾¹ç•Œã€‚
Non-parametric approaches offer a more flexible alternative, as they do
not assume any type of boundary.</p><h3 id=æœ€è¿‘é‚»>æœ€è¿‘é‚»</h3><p>æ–°æ ·æœ¬è¢«åˆ†é…ä¸º<strong>æœ€æ¥è¿‘ï¼ˆæœ€ç›¸ä¼¼ï¼‰çš„è®­ç»ƒæ ·æœ¬çš„æ ‡ç­¾</strong>ã€‚<br>New samples are assigned the label of the closest (most similar)
training sample.</p><ul><li>è¾¹ç•Œæ²¡æœ‰æ˜ç¡®å®šä¹‰ï¼ˆå°½ç®¡å®ƒä»¬å­˜åœ¨å¹¶ä¸”å¯ä»¥è·å–ï¼‰ã€‚<br>Boundaries are not defined explicitly (although they exist and can
be obtained).\</li><li>æ•´ä¸ªè®­ç»ƒæ•°æ®é›†éœ€è¦è¢«<strong>è®°å¿†</strong>ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬æœ‰æ—¶è¯´æœ€è¿‘é‚»æ˜¯ä¸€ç§<strong>åŸºäºå®ä¾‹çš„æ–¹æ³•</strong>ã€‚<br>The whole training dataset needs to be memorised. That&rsquo;s why
sometimes we say NN is an instance-based method.</li></ul><h3 id=kè¿‘é‚»>Kè¿‘é‚»</h3><p>Kè¿‘é‚»ç®—æ³•ï¼ˆkNNï¼‰æ˜¯æœ€è¿‘é‚»ç®—æ³•çš„ä¸€ä¸ªç®€å•æ‰©å±•ï¼Œå…¶è¿‡ç¨‹å¦‚ä¸‹ï¼š<br>K-Nearest Neighbors (kNN) is a simple extension of the nearest neighbors
algorithm, which proceeds as follows:</p><ul><li><p>ç»™å®šä¸€ä¸ªæ–°æ ·æœ¬xï¼š<br>Given a new sample x:\</p></li><li><p>æˆ‘ä»¬è®¡ç®—å®ƒä¸æ‰€æœ‰è®­ç»ƒæ ·æœ¬xiçš„è·ç¦»ã€‚<br>We calculate the distance to all the training samples xi.</p></li><li><p>æå–Kä¸ªæœ€è¿‘çš„æ ·æœ¬ï¼ˆé‚»å±…ï¼‰ã€‚<br>Extract the K closest samples (neighbors).\</p></li><li><p>è·å–å±äºæ¯ä¸ªç±»åˆ«çš„é‚»å±…æ•°é‡ã€‚<br>Obtain the number of neighbors that belong to each class.\</p></li><li><p>å°†æ ·æœ¬xçš„æ ‡ç­¾åˆ†é…ä¸ºé‚»å±…ä¸­æœ€å¸¸è§çš„ç±»åˆ«ã€‚<br>Assign the label of the most popular class among the neighbors.</p></li></ul><p>KNNçš„ç‰¹ç‚¹ï¼š</p><ul><li><p>å­˜åœ¨ä¸€ä¸ªéšå¼çš„è¾¹ç•Œï¼Œå°½ç®¡å®ƒä¸ç”¨äºåˆ†ç±»æ–°æ ·æœ¬ã€‚<br>There is always an implicit boundary, although it is not used to
classify new samples.</p></li><li><p>éšç€Kçš„å¢åŠ ï¼Œè¾¹ç•Œå˜å¾—ä¸é‚£ä¹ˆå¤æ‚ã€‚æˆ‘ä»¬ä»è¿‡æ‹Ÿåˆï¼ˆå°Kï¼‰è½¬å‘æ¬ æ‹Ÿåˆï¼ˆå¤§Kï¼‰åˆ†ç±»å™¨ã€‚<br>As K increases, the boundary becomes less complex. We move away from
overfitting (small K) to underfitting (large K) classifiers.\</p></li><li><p>åœ¨äºŒåˆ†ç±»é—®é¢˜ä¸­ï¼ŒKçš„å€¼é€šå¸¸æ˜¯ä¸€ä¸ªå¥‡æ•°ã€‚è¿™æ˜¯ä¸ºäº†é˜²æ­¢æ ·æœ¬çš„æœ€è¿‘é‚»ä¸­ä¸€åŠå±äºæ¯ä¸ªç±»åˆ«çš„æƒ…å†µã€‚<br>In binary problems, the value of K is usually an odd number. The
idea is to prevent situations where half of the nearest neighbours
of a sample belong to each class.\</p></li><li><p>kNNå¯ä»¥è½»æ¾åœ°åº”ç”¨äºå¤šåˆ†ç±»åœºæ™¯ã€‚<br>kNN can be easily implemented in multi-class scenarios.</p></li></ul><h2 id=åˆ†ç±»-classication-ii-week-2-2>åˆ†ç±» Classi cation II ã€Week 2-2ã€‘</h2><h3 id=å…ˆéªŒæ¦‚ç‡åéªŒæ¦‚ç‡ä¸è´å¶æ–¯åˆ†ç±»å™¨>å…ˆéªŒæ¦‚ç‡ã€åéªŒæ¦‚ç‡ä¸è´å¶æ–¯åˆ†ç±»å™¨</h3><h3 id=å…ˆéªŒæ¦‚ç‡prior-probability>å…ˆéªŒæ¦‚ç‡ï¼ˆPrior Probabilityï¼‰</h3><p>å…ˆéªŒæ¦‚ç‡æ˜¯æŒ‡åœ¨æ²¡æœ‰ä»»ä½•é¢å¤–ä¿¡æ¯çš„æƒ…å†µä¸‹ï¼ŒæŸä¸ªäº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡ã€‚å®ƒé€šå¸¸åŸºäºå·²æœ‰çš„çŸ¥è¯†æˆ–ç»éªŒã€‚ä¾‹å¦‚ï¼Œåœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼Œå…ˆéªŒæ¦‚ç‡
$P(C)$ è¡¨ç¤ºç±»åˆ« $C$ åœ¨æ²¡æœ‰ä»»ä½•ç‰¹å¾ä¿¡æ¯çš„æƒ…å†µä¸‹å‡ºç°çš„æ¦‚ç‡ã€‚</p><h3 id=åéªŒæ¦‚ç‡posterior-probability>åéªŒæ¦‚ç‡ï¼ˆPosterior Probabilityï¼‰</h3><p>åéªŒæ¦‚ç‡æ˜¯æŒ‡åœ¨è§‚å¯Ÿåˆ°æŸäº›è¯æ®æˆ–ç‰¹å¾åï¼ŒæŸä¸ªäº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡ã€‚åœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼ŒåéªŒæ¦‚ç‡
$P(C|X)$ è¡¨ç¤ºåœ¨è§‚å¯Ÿåˆ°ç‰¹å¾ $X$ åï¼Œç±»åˆ« $C$ å‡ºç°çš„æ¦‚ç‡ã€‚</p><h3 id=è´å¶æ–¯å®šç†bayes-theorem>è´å¶æ–¯å®šç†ï¼ˆBayes&rsquo; Theoremï¼‰</h3><p>è´å¶æ–¯å®šç†æ˜¯è®¡ç®—åéªŒæ¦‚ç‡çš„åŸºç¡€ï¼Œå…¶å…¬å¼ä¸ºï¼š $$
P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}
$$ å…¶ä¸­ï¼š - $P(C|X)$ æ˜¯åéªŒæ¦‚ç‡ï¼Œå³åœ¨è§‚å¯Ÿåˆ°ç‰¹å¾ $X$ åç±»åˆ« $C$
çš„æ¦‚ç‡ã€‚ - $P(X|C)$ æ˜¯ä¼¼ç„¶ï¼ˆLikelihoodï¼‰ï¼Œå³åœ¨ç±»åˆ« $C$ ä¸‹è§‚å¯Ÿåˆ°ç‰¹å¾ $X$
çš„æ¦‚ç‡ã€‚ - $P(C)$ æ˜¯å…ˆéªŒæ¦‚ç‡ï¼Œå³ç±»åˆ« $C$ çš„æ¦‚ç‡ã€‚ - $P(X)$
æ˜¯è¯æ®ï¼ˆEvidenceï¼‰ï¼Œå³ç‰¹å¾ $X$ çš„æ¦‚ç‡ï¼Œé€šå¸¸å¯ä»¥é€šè¿‡å…¨æ¦‚ç‡å…¬å¼è®¡ç®—ï¼š $$
P(X) = \sum_{i} P(X|C_i) \cdot P(C_i)
$$</p><h3 id=è´å¶æ–¯åˆ†ç±»å™¨bayesian-classifier>è´å¶æ–¯åˆ†ç±»å™¨ï¼ˆBayesian Classifierï¼‰</h3><p>è´å¶æ–¯åˆ†ç±»å™¨æ˜¯ä¸€ç§åŸºäºè´å¶æ–¯å®šç†çš„åˆ†ç±»æ–¹æ³•ã€‚å®ƒçš„åŸºæœ¬æ€æƒ³æ˜¯ï¼šç»™å®šä¸€ä¸ªæ ·æœ¬çš„ç‰¹å¾
$X$ï¼Œè®¡ç®—æ¯ä¸ªç±»åˆ« $C$ çš„åéªŒæ¦‚ç‡
$P(C|X)$ï¼Œç„¶åå°†æ ·æœ¬åˆ†é…åˆ°åéªŒæ¦‚ç‡æœ€å¤§çš„ç±»åˆ«ã€‚</p><p>å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š 1. <strong>è®¡ç®—å…ˆéªŒæ¦‚ç‡</strong>ï¼šæ ¹æ®è®­ç»ƒæ•°æ®ä¼°è®¡æ¯ä¸ªç±»åˆ«çš„å…ˆéªŒæ¦‚ç‡
$P(C)$ã€‚ 2. <strong>è®¡ç®—ä¼¼ç„¶</strong>ï¼šä¼°è®¡åœ¨ç»™å®šç±»åˆ« $C$ ä¸‹ç‰¹å¾ $X$ çš„ä¼¼ç„¶
$P(X|C)$ã€‚ 3. <strong>è®¡ç®—è¯æ®</strong>ï¼šè®¡ç®—ç‰¹å¾ $X$ çš„æ¦‚ç‡ $P(X)$ã€‚ 4.
<strong>è®¡ç®—åéªŒæ¦‚ç‡</strong>ï¼šä½¿ç”¨è´å¶æ–¯å®šç†è®¡ç®—æ¯ä¸ªç±»åˆ«çš„åéªŒæ¦‚ç‡ $P(C|X)$ã€‚ 5.
<strong>åˆ†ç±»å†³ç­–</strong>ï¼šå°†æ ·æœ¬åˆ†é…åˆ°åéªŒæ¦‚ç‡æœ€å¤§çš„ç±»åˆ«ã€‚</p><h3 id=æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨naive-bayes-classifier>æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ï¼ˆNaive Bayes Classifierï¼‰</h3><p>æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨æ˜¯è´å¶æ–¯åˆ†ç±»å™¨çš„ä¸€ç§ç®€åŒ–ç‰ˆæœ¬ï¼Œå®ƒå‡è®¾ç‰¹å¾ä¹‹é—´æ˜¯æ¡ä»¶ç‹¬ç«‹çš„ã€‚å³ï¼š
$$
P(X|C) = \prod_{i=1}^{n} P(x_i|C)
$$ å…¶ä¸­ $x_i$ æ˜¯ç‰¹å¾ $X$ çš„ç¬¬ $i$ ä¸ªåˆ†é‡ã€‚</p><p>æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨çš„åéªŒæ¦‚ç‡å…¬å¼ä¸ºï¼š $$
P(C|X) = \frac{P(C) \cdot \prod_{i=1}^{n} P(x_i|C)}{P(X)}
$$
ç”±äº $P(X)$
å¯¹äºæ‰€æœ‰ç±»åˆ«æ˜¯ç›¸åŒçš„ï¼Œå› æ­¤åœ¨åˆ†ç±»å†³ç­–æ—¶å¯ä»¥å¿½ç•¥å®ƒï¼Œåªéœ€æ¯”è¾ƒåˆ†å­éƒ¨åˆ†ï¼š $$
C_{\text{pred}} = \arg\max_{C} P(C) \cdot \prod_{i=1}^{n} P(x_i|C)
$$</p><ul><li><strong>å…ˆéªŒæ¦‚ç‡</strong> $P(C)$ æ˜¯åœ¨æ²¡æœ‰ä»»ä½•ç‰¹å¾ä¿¡æ¯çš„æƒ…å†µä¸‹ç±»åˆ« $C$ çš„æ¦‚ç‡ã€‚</li><li><strong>åéªŒæ¦‚ç‡</strong> $P(C|X)$ æ˜¯åœ¨è§‚å¯Ÿåˆ°ç‰¹å¾ $X$ åç±»åˆ« $C$ çš„æ¦‚ç‡ã€‚</li><li><strong>è´å¶æ–¯åˆ†ç±»å™¨</strong> é€šè¿‡è®¡ç®—åéªŒæ¦‚ç‡æ¥è¿›è¡Œåˆ†ç±»å†³ç­–ã€‚</li><li><strong>æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨</strong> å‡è®¾ç‰¹å¾ä¹‹é—´æ¡ä»¶ç‹¬ç«‹ï¼Œç®€åŒ–äº†è®¡ç®—ã€‚</li></ul><p>å¯ä»¥è¾¾åˆ°<strong>æœ€ä½³å‡†ç¡®ç‡</strong></p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102134946803.png width=597 height=151 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102134946803_hu_3b7989d60e7e947.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102134946803_hu_1465a32b30f9b21e.png 1024w" loading=lazy alt=image-20250102134946803 class=gallery-image data-flex-grow=395 data-flex-basis=948px></p><h3 id=åˆ¤åˆ«åˆ†æ-discriminant-analysis>åˆ¤åˆ«åˆ†æ Discriminant analysis</h3><p>åœ¨åˆ¤åˆ«åˆ†æä¸­ï¼Œæˆ‘ä»¬å‡è®¾å„ç±»åˆ«çš„å¯†åº¦æ˜¯<strong>é«˜æ–¯åˆ†å¸ƒ</strong>ã€‚å¦‚æœæœ‰ä¸€ä¸ªé¢„æµ‹å˜é‡
$x$ï¼Œåˆ™ç±»åˆ«Açš„å¯†åº¦ä¸ºï¼š $$
p(x|y = A) = \frac{1}{\sigma_A\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x-\mu_A}{\sigma_A}\right)^2}
$$ å…¶ä¸­ $\mu_A$ æ˜¯<strong>å‡å€¼</strong>ï¼Œ$\sigma^2_A$ æ˜¯é«˜æ–¯åˆ†å¸ƒçš„<strong>æ–¹å·®</strong>ã€‚</p><p>å¦‚æœæœ‰ $K$ ä¸ªé¢„æµ‹å˜é‡ï¼Œé«˜æ–¯ç±»çš„å¯†åº¦è¡¨ç¤ºä¸ºï¼š $$
p(x|y = A) = \frac{1}{(2\pi)^{p/2}|\Sigma_A|^{1/2}}e^{-\frac{1}{2}(x-\mu_A)^T\Sigma_A^{-1}(x-\mu_A)}
$$ è¿™é‡Œ $x = [x_1, \dots, x_K]^T$ åŒ…å«æ‰€æœ‰çš„é¢„æµ‹å˜é‡ï¼Œ$\mu_A$
æ˜¯<strong>å‡å€¼</strong>ï¼Œ$\Sigma_A$ æ˜¯<strong>åæ–¹å·®çŸ©é˜µ</strong>ã€‚</p><p>å¯¹äºç±»åˆ«Açš„å¯†åº¦ï¼Œå¯ä»¥å¾—åˆ°ç±»ä¼¼çš„è¡¨è¾¾å¼ã€‚</p><p>å½“ç±»å¯†åº¦æ»¡è¶³é«˜æ–¯åˆ†å¸ƒçš„æ—¶å€™ï¼Œåˆ©ç”¨ä¸¤ä¸ªåˆ†ç±»çš„åæ–¹å·®çŸ©é˜µåˆ¤æ–­ï¼Œå¦‚æœç›¸ç­‰å°±æ˜¯Linear
Discriminant Analysisï¼ˆçº¿æ€§åˆ¤åˆ«åˆ†æï¼‰(LDR)ï¼Œå¦‚æœä¸ç›¸ç­‰å°±æ˜¯Quadratic
Discriminant Analysisï¼ˆäºŒæ¬¡åˆ¤åˆ«åˆ†æï¼‰(QDA)</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102134419572.png width=1222 height=721 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102134419572_hu_252af9bba2d77966.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102134419572_hu_75ffb1523b291b62.png 1024w" loading=lazy alt=image-20250102134419572 class=gallery-image data-flex-grow=169 data-flex-basis=406px></p><h3 id=æ¯”è¾ƒå­¦è¿‡çš„åˆ†ç±»å™¨>æ¯”è¾ƒå­¦è¿‡çš„åˆ†ç±»å™¨</h3><p><strong>è¾¹ç•Œå½¢çŠ¶</strong>ï¼šé€»è¾‘å›å½’å’ŒLDAæ„å»ºçº¿æ€§è¾¹ç•Œï¼ŒQDAæ„å»ºäºŒæ¬¡è¾¹ç•Œã€‚kNNä¸æ–½åŠ ä»»ä½•ç‰¹å®šå½¢çŠ¶</p><p><strong>ç¨³å®šæ€§</strong>ï¼šå¯¹äºæ ·æœ¬è¾ƒå°‘çš„æƒ…å†µï¼Œé€»è¾‘å›å½’å¯èƒ½éå¸¸ä¸ç¨³å®šï¼Œè€ŒDAæ–¹æ³•äº§ç”Ÿç¨³å®šçš„è§£å†³æ–¹æ¡ˆã€‚</p><p><strong>å¼‚å¸¸å€¼</strong>ï¼šé€»è¾‘å›å½’å¯¹ä½äºè¾¹ç•Œä¹‹å¤–çš„æ ·æœ¬è¡¨ç°å‡ºç¨³å¥æ€§ï¼Œè€ŒLDAå’ŒQDAå¯èƒ½ä¼šå—åˆ°å½±å“ã€‚</p><p><strong>å¤šåˆ†ç±»</strong>ï¼šåœ¨åˆ¤åˆ«åˆ†æä¸­ï¼Œå¤šåˆ†ç±»é—®é¢˜å¯ä»¥å¾ˆå®¹æ˜“è¢«å®ç°ã€‚</p><p><strong>å…ˆéªŒçŸ¥è¯†</strong>ï¼šå¯ä»¥å®¹æ˜“åœ°é€šè¿‡è´å¶æ–¯æ–¹æ³•æ•´åˆè¿›å»ã€‚</p><h3 id=æ‹“å±•è´å¶æ–¯åˆ†ç±»å™¨>æ‹“å±•è´å¶æ–¯åˆ†ç±»å™¨</h3><p>å³åŠ å…¥é”™åˆ¤çš„ä»£ä»·ä½œä¸ºå‚æ•°</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102135149963.png width=856 height=151 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102135149963_hu_d501ad0d36276d8c.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102135149963_hu_e7d981df410bf4b3.png 1024w" loading=lazy alt=image-20250102135149963 class=gallery-image data-flex-grow=566 data-flex-basis=1360px></p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102135206828.png width=564 height=144 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102135206828_hu_172a43df3a71895f.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102135206828_hu_9400429fd62a3d32.png 1024w" loading=lazy alt=image-20250102135206828 class=gallery-image data-flex-grow=391 data-flex-basis=940px></p><p>æ”¹å˜Tä¼šæ”¹å˜åˆ†ç±»å™¨è¾¹ç•Œã€‚å¾—åˆ°çš„ç»“æœæ˜¯èƒ½å¤Ÿè¾¾åˆ°<strong>æœ€ä½é”™åˆ¤æˆæœ¬</strong>ã€‚</p><h3 id=æ··æ·†çŸ©é˜µ>æ··æ·†çŸ©é˜µ</h3><p>åœ¨<strong>ç±»åˆ«æ•æ„Ÿ</strong>é—®é¢˜ä¸­ï¼Œå‡†ç¡®ç‡å’Œé”™è¯¯ç‡å¹¶ä¸æ˜¯æœ€åˆé€‚çš„è´¨é‡æŒ‡æ ‡ï¼Œå› ä¸ºä¸åŒç±»åˆ«çš„æ ·æœ¬è¢«é”™è¯¯åˆ†ç±»çš„æˆæœ¬æ˜¯ä¸åŒçš„ã€‚<br>In class-sensitive problems, accuracy and error rate are not the most
suitable quality metrics, as the cost of misclassifying samples from
different classes varies.</p><p>é™¤äº†ä½¿ç”¨æ¯ä¸ªç±»åˆ«çš„<strong>é”™è¯¯åˆ†ç±»æˆæœ¬</strong>å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥è¯„ä¼°åˆ†ç±»å™¨åœ¨å¤„ç†æ¯ä¸ªç±»åˆ«æ—¶çš„è¡¨ç°ã€‚è¿™æ­£æ˜¯<strong>æ··æ·†çŸ©é˜µæˆ–åˆ—è”è¡¨</strong>æ‰€å±•ç¤ºçš„ä¿¡æ¯ã€‚<br>In addition to using the misclassification cost for each class, we can
assess how well the classifier deals with each class individually. This
is precisely the information that a confusion or contingency matrix
shows.</p><p>å¯¹è§’çº¿ä¸Šæ˜¯æ­£ç¡®åˆ†ç±»çš„ç»“æœï¼Œå…¶ä»–ä½ç½®æ˜¯é”™è¯¯åˆ†ç±»çš„ç»“æœã€‚</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102140945441.png width=1167 height=664 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102140945441_hu_1f6db31151a4e8d7.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102140945441_hu_4185eea8e27c7684.png 1024w" loading=lazy alt=image-20250102140945441 class=gallery-image data-flex-grow=175 data-flex-basis=421px></p><p>å¯¹äºäºŒåˆ†ç±»ä»»åŠ¡ï¼š</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102141211002.png width=1093 height=537 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102141211002_hu_a08d2e37b48fe2d0.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102141211002_hu_4cbade5af3df39fa.png 1024w" loading=lazy alt=image-20250102141211002 class=gallery-image data-flex-grow=203 data-flex-basis=488px></p><h3 id=å…¶ä»–æŒ‡æ ‡>å…¶ä»–æŒ‡æ ‡</h3><p>é”™è¯¯ç‡å’Œå‡†ç¡®ç‡æ˜¯æ— æ³•è®©æˆ‘ä»¬ç ”ç©¶åˆ†ç±»å™¨å¦‚ä½•å¤„ç†æ¯ä¸ªç±»åˆ«çš„æ€§èƒ½æŒ‡æ ‡ã€‚ Error
rate and accuracy are performance rates that do not allow us to
investigate how a classifier treats each class.</p><ul><li><strong>çµæ•åº¦ï¼ˆå¬å›ç‡æˆ–çœŸé˜³æ€§ç‡ï¼‰</strong>ï¼šTP/(TP+FN) Sensitivity (recall or
true positive rate): TP/(TP+FN)</li><li><strong>ç‰¹å¼‚æ€§ï¼ˆçœŸé˜´æ€§ç‡ï¼‰</strong>ï¼šTN/(TN+FP) Specificity (true negative rate):
TN/(TN+FP)</li><li><strong>ç²¾ç¡®ç‡ï¼ˆé˜³æ€§é¢„æµ‹å€¼ï¼‰</strong>ï¼šTP/(TP+FP) Precision (positive predictive
value): TP/(TP+FP)</li></ul><p>è¿™äº›æŒ‡æ ‡å¯ä»¥ç”¨ä½œè´¨é‡æŒ‡æ ‡ã€‚ These rates can be used as quality metrics.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102141815329.png width=883 height=487 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102141815329_hu_4622f71cca86559e.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102141815329_hu_800520391f04aa48.png 1024w" loading=lazy alt=image-20250102141815329 class=gallery-image data-flex-grow=181 data-flex-basis=435px></p><p>å•ç‹¬æå‡ä¸€ä¸ªè´¨é‡æŒ‡æ ‡å¾ˆå®¹æ˜“ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬å°†æ¯ä¸ªæ ·æœ¬éƒ½æ ‡è®°ä¸ºæ­£ä¾‹ï¼Œå°±èƒ½è¾¾åˆ°å®Œç¾çš„çµæ•åº¦ã€‚
Improving one quality metric individually is easy. For instance, if we
label every sample as positive, we would achieve a perfect sensitivity.</p><p>é—®é¢˜åœ¨äºï¼Œ<strong>æå‡ä¸€ä¸ªè´¨é‡æŒ‡æ ‡ä¼šæŸå®³å…¶ä»–æŒ‡æ ‡</strong>ã€‚ The problem is that
improving one quality metric deteriorates others.</p><p>æˆ‘ä»¬é€šå¸¸åŒæ—¶è€ƒè™‘æˆå¯¹çš„è´¨é‡æŒ‡æ ‡ï¼š<br>We usually consider pairs of quality metrics simultaneously:</p><ul><li>æ•æ„Ÿæ€§å’Œç‰¹å¼‚æ€§ã€‚<br>Sensitivity and specificity.\</li><li>ç²¾ç¡®ç‡å’Œå¬å›ç‡ã€‚<br>Precision and recall.</li></ul><h3 id=f1-score>F1 Score</h3><p>F1åˆ†æ•°æ˜¯å¦ä¸€ä¸ªå¹¿æ³›ä½¿ç”¨çš„æ€§èƒ½æŒ‡æ ‡ï¼Œå®ƒæä¾›äº†ç²¾ç¡®ç‡å’Œå¬å›ç‡ä¹‹é—´çš„å¹³å‡å€¼ã€‚<br>The F1-score is another widely used performance metric that provides an
average between precision and recall. $$
F_1 = 2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}}
$$</p><h3 id=roc>ROC</h3><p>ROCï¼ˆå—è¯•è€…å·¥ä½œç‰¹å¾ï¼‰å¹³é¢ç”¨äºè¡¨ç¤ºåˆ†ç±»å™¨åœ¨æ•æ„Ÿæ€§å’Œ1-ç‰¹å¼‚æ€§æ–¹é¢çš„æ€§èƒ½ã€‚
The ROC (Receiver Operating Characteristic) plane is used to represent
the performance of a classifier in terms of its sensitivity and
1-specificity.</p><p>æˆ‘ä»¬å¸Œæœ›çµæ•åº¦æ¥è¿‘1ï¼Œ1-ç‰¹å¼‚æ€§æ¥è¿‘0ï¼ˆå·¦ä¸Šè§’ï¼‰ã€‚ We would like the
sensitivity to be close to 1 and the 1-specificity to be close to 0 (top
left corner).</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102142554532.png width=1201 height=538 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102142554532_hu_bbd75093cbb01d9d.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102142554532_hu_8a25ca7b22812579.png 1024w" loading=lazy alt=image-20250102142554532 class=gallery-image data-flex-grow=223 data-flex-basis=535px></p><p>æˆ‘ä»¬<strong>æ— æ³•åŒæ—¶ä½¿ç”¨ä¸¤ä¸ªæŒ‡æ ‡</strong>å¯¹åˆ†ç±»å™¨è¿›è¡Œæ’åºã€‚<br>We <strong>cannot rank classifiers</strong> using two metrics simultaneously.</p><p>é€šå¸¸çš„åšæ³•æ˜¯å›ºå®šå…¶ä¸­ä¸€ä¸ªæŒ‡æ ‡çš„æœ€å°å€¼ï¼Œå¹¶ä¼˜åŒ–å¦ä¸€ä¸ªæŒ‡æ ‡ï¼Œä¾‹å¦‚ï¼šåœ¨ç‰¹å¼‚æ€§è‡³å°‘ä¸º70%çš„æƒ…å†µä¸‹è·å¾—æœ€é«˜çš„çµæ•åº¦ã€‚<br>The usual practice is to fix a minimum value for one of the metrics and
optimise the other, for instance: obtain the highest sensitivity with a
minimum specificity of 70%.</p><p>**AUCï¼ˆæ›²çº¿ä¸‹é¢ç§¯ï¼‰**æ˜¯è¡¡é‡å¯æ ¡å‡†åˆ†ç±»å™¨æ€§èƒ½çš„æŒ‡æ ‡ã€‚<br><strong>The AUC (Area Under the Curve)</strong> is a measure of goodness for a
classifier that can be calibrated.</p><p>å¥½çš„åˆ†ç±»å™¨çš„AUCæ¥è¿‘1ï¼Œå·®çš„åˆ†ç±»å™¨æ¥è¿‘0.5ã€‚ Good classifiers will have AUC
close to 1, bad classifiers close to 0.5.</p><h2 id=æ–¹æ³•è®º-methodology-iiweek-3-1>æ–¹æ³•è®º Methodology IIã€week 3-1ã€‘</h2><h3 id=æµæ°´çº¿-pipeline>æµæ°´çº¿ Pipeline</h3><p>ç®¡é“ï¼ˆpipelineï¼‰æè¿°äº†ä¸€ç³»åˆ—æ“ä½œçš„é¡ºåºã€‚ The term pipeline describes a
sequence of operations.</p><p>ï¼ˆæœ‰ç›‘ç£çš„ï¼‰æœºå™¨å­¦ä¹ ç®¡é“æ˜¯ä½¿ç”¨ä¸€ç»„é¢„æµ‹å˜é‡ï¼ˆè¾“å…¥ï¼‰ç”Ÿæˆé¢„æµ‹ï¼ˆè¾“å‡ºï¼‰çš„æ“ä½œåºåˆ—ã€‚
A (supervised) machine learning pipeline is the sequence of operations
that produce a prediction (output) using a set of predictors (input).</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102143514981.png width=870 height=205 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102143514981_hu_dfe537ba7cda2587.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102143514981_hu_3160230faf7e40dc.png 1024w" loading=lazy alt=image-20250102143514981 class=gallery-image data-flex-grow=424 data-flex-basis=1018px></p><p>æˆ‘ä»¬å¯ä»¥ä»æ¯å¼ å›¾ç‰‡ä¸­æå–ç‰¹å¾ï¼ˆä¾‹å¦‚è¼ç‰‡é•¿åº¦å’Œå®½åº¦ï¼ŒèŠ±ç“£é•¿åº¦å’Œå®½åº¦ï¼‰ï¼Œå¹¶æ„å»ºä¸€ä¸ªä»¥è¿™äº›ç‰¹å¾ä¸ºè¾“å…¥çš„çº¿æ€§æ¨¡å‹ã€‚
We can extract features from each picture (e.g., sepal length and width,
petal length and width) and build a linear model that takes these
features as input.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102144222071.png width=1159 height=192 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102144222071_hu_12d1634194aff8bc.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102144222071_hu_1a8a7cd0a96871ce.png 1024w" loading=lazy alt=image-20250102144222071 class=gallery-image data-flex-grow=603 data-flex-basis=1448px></p><p>æœºå™¨å­¦ä¹ è§£å†³æ–¹æ¡ˆä¸ä»…ä»…æ˜¯ä¸€ä¸ªæ¨¡å‹ã€‚<br>Machine learning solutions are more than just one model.</p><p>å®ƒä»¬å¯ä»¥åŒ…å«å¤šä¸ªé˜¶æ®µï¼Œå½¢æˆä¸€ä¸ª<strong>å¤„ç†ç®¡é“</strong>ï¼š<br>They can include several stages that form a processing pipeline:</p><ul><li><p>è½¬æ¢é˜¶æ®µï¼ˆå¤„ç†è¾“å…¥æ•°æ®ï¼‰ã€‚<br>Transformation stages (where input data is processed).</p></li><li><p>å¤šä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹å¹¶è¡Œè¿è¡Œã€‚<br>Several machine learning models running in parallel.</p></li><li><p>æœ€ç»ˆèšåˆé˜¶æ®µï¼ˆå°†å„ä¸ªè¾“å‡ºç»„åˆæˆä¸€ä¸ªå•ä¸€è¾“å‡ºï¼‰ã€‚<br>A final aggregation stage (where individual outputs are combined to
produce one single output).</p></li></ul><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102144322597.png width=942 height=397 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102144322597_hu_7071efa58b85d857.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102144322597_hu_9eb723523cef3866.png 1024w" loading=lazy alt=image-20250102144322597 class=gallery-image data-flex-grow=237 data-flex-basis=569px></p><ul><li><p>è®­ç»ƒåï¼Œç®¡é“çš„å‚æ•°<strong>ä¿æŒä¸å˜</strong>ã€‚<br>After training, the parameters of a pipeline remain fixed.\</p></li><li><p>ç®¡é“å¯ä»¥è¢«æµ‹è¯•å’Œéƒ¨ç½²ã€‚<br>Pipelines can be tested and deployed.</p></li><li><p>é¢„æµ‹çš„è´¨é‡å–å†³äºæ‰€æœ‰ç®¡é“é˜¶æ®µã€‚<br>The quality of the prediction depends on all the pipeline stages.</p></li></ul><h3 id=æ•°æ®æ ‡å‡†åŒ–-normalization>æ•°æ®æ ‡å‡†åŒ– Normalization</h3><p>è·ç¦»çš„æ¦‚å¿µæ˜¯è®¸å¤šæœºå™¨å­¦ä¹ æŠ€æœ¯çš„åŸºç¡€ï¼š<br>The notion of distance is behind many machine learning techniques:</p><ul><li>åœ¨å›å½’ä¸­ï¼Œé¢„æµ‹è¯¯å·® 4 e_i = y_i - <code>\hat{y}</code>{=tex}_i $
å¯ä»¥è¢«è§†ä¸ºä¸€ç§è·ç¦»ã€‚<br>In regression, the prediction error $ e_i = y_i -
<code>\hat{y}</code>{=tex}_i $ can be seen as a distance.\</li><li>åœ¨åˆ†ç±»ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†æ ·æœ¬ä¸è¾¹ç•Œä¹‹é—´çš„è·ç¦»ï¼ˆé€»è¾‘å›å½’ï¼‰ï¼Œä»¥åŠæ ·æœ¬ä¹‹é—´çš„è·ç¦»ï¼ˆkNNï¼‰ã€‚<br>In classification, we used the distance between samples and
boundaries (logistic regression), and between samples (kNN).\</li><li>åœ¨èšç±»ä¸­ï¼ŒK-means èšç±»æ˜¯åŸºäºæ ·æœ¬ä¸åŸå‹ä¹‹é—´çš„è·ç¦»åˆ›å»ºçš„ã€‚<br>In clustering, K-means clusters were created based on the distance
between samples and prototypes.\</li><li>åœ¨å¯†åº¦ä¼°è®¡ä¸­ï¼Œæ ‡å‡†å·®é‡åŒ–äº†æ ·æœ¬ä¸æ ·æœ¬å‡å€¼ä¹‹é—´çš„å¹³å‡è·ç¦»ã€‚<br>In density estimation, the standard deviation quantifies the average
distance between samples and the sample mean.</li></ul><p>æ•°å€¼è¡¨ç¤ºå¯¹æˆ‘ä»¬çš„æ•°æ®æœ‰å½±å“ï¼Œå¯èƒ½ä¼šå½±å“æœ€ç»ˆæ¨¡å‹å’Œç®—æ³•çš„æ€§èƒ½ã€‚<br>The numerical representations of our data can have an impact on the
final model and the performance of our algorithms.</p><h3 id=æœ€å°æœ€å¤§å½’ä¸€åŒ–-min-max-normalisation>æœ€å°æœ€å¤§å½’ä¸€åŒ– Min-max normalisation</h3><p>äº§ç”Ÿåœ¨0åˆ°1èŒƒå›´å†…çš„å€¼</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102152255868.png width=1083 height=370 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102152255868_hu_ec5b806a03423e7d.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102152255868_hu_9cad07c216b45c99.png 1024w" loading=lazy alt=image-20250102152255868 class=gallery-image data-flex-grow=292 data-flex-basis=702px></p><h3 id=æ ‡å‡†åŒ–-standardisation>æ ‡å‡†åŒ– Standardisation</h3><p>äº§ç”Ÿçš„ç»“æœå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102152454685.png width=937 height=361 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102152454685_hu_ca1a4557cab363b9.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102152454685_hu_a7d8e6f80009861f.png 1024w" loading=lazy alt=image-20250102152454685 class=gallery-image data-flex-grow=259 data-flex-basis=622px></p><h3 id=æ ‡å‡†åŒ–ç›¸å…³>æ ‡å‡†åŒ–ç›¸å…³</h3><ul><li><p>åœ¨æµ‹è¯•å’Œéƒ¨ç½²è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åº”é¢„æ–™åˆ°ä¼šå‡ºç°è¶…å‡ºèŒƒå›´çš„å€¼ï¼ˆä¾‹å¦‚ï¼Œåœ¨æœ€å°-æœ€å¤§å½’ä¸€åŒ–ä¸­ï¼Œz
= 1:2ï¼‰ã€‚<br>During test and deployment, we should expect out-of-range values
(e.g., z = 1:2 in min-max normalization).\</p></li><li><p>å¼‚å¸¸å€¼å¯èƒ½ä¼šäº§ç”Ÿè´Ÿé¢å½±å“ï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªæ¯”ç¬¬äºŒå¤§å€¼å¤§10å€çš„å¼‚å¸¸å€¼ä¼šå°†æœ€å°-æœ€å¤§å½’ä¸€åŒ–å‹ç¼©åˆ°[0;
0.1]ï¼‰ã€‚<br>Outliers can have a negative impact (e.g., an outlier 10 times
larger than the second largest value will squeeze min-max
normalization to [0; 0.1]).</p></li><li><p>å­˜åœ¨éçº¿æ€§ç¼©æ”¾é€‰é¡¹ï¼Œä¾‹å¦‚ä½¿ç”¨é€»è¾‘å‡½æ•°çš„Softmaxç¼©æ”¾æˆ–å¯¹æ•°ç¼©æ”¾ã€‚<br>Non-linear scaling options exist, for instance, softmax scaling,
which uses the logistic function, or logarithmic scaling.\</p></li><li><p>åŸå§‹æ•°æ®é›†ä¸­çš„å€¼å¯èƒ½å…·æœ‰ç›¸å…³æ€§ã€‚<br>The original values in your dataset might be relevant.</p></li><li><p>é€šå¸¸ï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘æ„å¤–çš„æ•ˆæœå’Œæ‰­æ›²ã€‚<br>In general, we need to consider unintended effects and distortions.</p></li><li><p>è®¸å¤šæœºå™¨å­¦ä¹ ç®—æ³•åœ¨ç¼©æ”¾åå¯èƒ½ä¼šäº§ç”Ÿä¸åŒçš„è§£å†³æ–¹æ¡ˆã€‚<br>Many machine learning algorithms might produce different solutions
after scaling.</p></li></ul><h3 id=è½¬æ¢-transformation>è½¬æ¢ Transformation</h3><p>æ•°æ®è½¬æ¢æ˜¯é€šè¿‡æ”¹å˜æ ·æœ¬çš„è¡¨ç¤ºæ–¹å¼æ¥è¿›è¡Œçš„æ•°æ®æ“ä½œã€‚<br>Transformations are data manipulations that change the way that we
represent our samples.</p><p>å®ƒä»¬å¯ä»¥è¢«è§†ä¸ºå°†æ ·æœ¬ä»ä¸€ä¸ªç©ºé—´ç§»åŠ¨åˆ°å¦ä¸€ä¸ªç©ºé—´ã€‚<br>They can be seen as moving samples from one space to another.</p><p><strong>å½’ä¸€åŒ–æ˜¯å•ç‹¬å¯¹æ¯ä¸ªå±æ€§è¿›è¡Œæ“ä½œçš„ä¸€ç§è½¬æ¢ç¤ºä¾‹</strong>ï¼šå®ƒåº”è¯¥ä½œä¸ºæˆ‘ä»¬æµæ°´çº¿ä¸­çš„å¦ä¸€ä¸ªé˜¶æ®µã€‚
Normalisation is one example of a transformation that operates on each
attribute separately: it should be included as another stage in our
pipeline.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102152723679.png width=1074 height=352 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102152723679_hu_27b4266ee226a376.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102152723679_hu_b1cc4152a3ec27e9.png 1024w" loading=lazy alt=image-20250102152723679 class=gallery-image data-flex-grow=305 data-flex-basis=732px></p><p>åœ¨æŸäº›å˜æ¢ä¸­ï¼ŒåŸå§‹ç©ºé—´å’Œç›®æ ‡ç©ºé—´å…·æœ‰<strong>ç›¸åŒçš„ç»´åº¦</strong>ã€‚<br>In some transformations, the original and destination spaces have the
same number of dimensions.</p><ul><li>çº¿æ€§å˜æ¢å¯ä»¥çœ‹ä½œæ˜¯æ—‹è½¬å’Œç¼©æ”¾ã€‚<br>A linear transformation can be seen as a rotation and scaling.\</li><li>éçº¿æ€§å˜æ¢æ²¡æœ‰å”¯ä¸€çš„æè¿°ã€‚<br>There is no unique description for non-linear transformations.</li></ul><p>å¦‚æœç›®æ ‡ç©ºé—´çš„ç»´åº¦å°‘äºåŸå§‹ç©ºé—´ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º<strong>é™ç»´</strong>ã€‚<br>If the destination space has fewer dimensions than the original one, we
talk about dimensionality reduction.</p><ul><li>ç‰¹åˆ«æ˜¯åœ¨ç‰¹å¾é€‰æ‹©åï¼Œç›®æ ‡ç©ºé—´ç”±åŸå§‹å±æ€§çš„å­é›†å®šä¹‰ã€‚<br>In particular, after feature selection, the destination space is
defined by a subset of the original attributes.\</li><li>åœ¨ç‰¹å¾æå–ä¸­ï¼Œæ–°å±æ€§è¢«å®šä¹‰ä¸ºå¯¹åŸå§‹å±æ€§çš„æ“ä½œã€‚<br>In feature extraction, the new attributes are defined as operations
on the original attributes.</li></ul><h3 id=ä¸»æˆåˆ†åˆ†æ-pca>ä¸»æˆåˆ†åˆ†æ PCA</h3><p>ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰è¯†åˆ«æ ·æœ¬å¯¹é½çš„<strong>æ–¹å‘</strong>ã€‚<br>Principal components analysis (PCA) identifies the directions along
which samples are aligned.</p><p>è¿™äº›æ–¹å‘å®šä¹‰äº†ä¸€ä¸ªä¸åŸå§‹ç©ºé—´ç»´åº¦ç›¸åŒçš„ç›®æ ‡ç©ºé—´ã€‚<br>These directions define a destination space with the same number of
dimensions as the original space.</p><p>ä½¿ç”¨æ•°æ®é›†ï¼ŒPCAæ„å»ºäº†ä¸€ä¸ª<strong>çº¿æ€§å˜æ¢</strong>ï¼Œå¹¶ä¸ºæ¯ä¸ªæˆåˆ†åˆ†é…äº†ä¸€ä¸ªåˆ†æ•°ã€‚<br>Using a dataset, PCA builds a linear transformation and additionally
assigns a score to each component.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102154025457.png width=1132 height=480 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102154025457_hu_a4d41e69688b5bce.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102154025457_hu_db30806a65df4596.png 1024w" loading=lazy alt=image-20250102154025457 class=gallery-image data-flex-grow=235 data-flex-basis=566px></p><h3 id=éçº¿æ€§å˜æ¢>éçº¿æ€§å˜æ¢</h3><p>ä¸€ç§é€‚ç”¨äºå…ˆå‰ç¤ºä¾‹çš„è§£å†³æ–¹æ¡ˆå¯ä»¥æ˜¯ä¸€ä¸ªç”±ä»¥ä¸‹éƒ¨åˆ†ç»„æˆçš„ç®¡é“ï¼š A solution
for the previous example could be a pipeline consisting of:</p><ul><li><p>ä¸€ä¸ªåˆé€‚çš„éçº¿æ€§å˜æ¢ã€‚ A suitable non-linear transformation.</p></li><li><p>éšåæ˜¯ç¬¬äºŒå±‚çº¿æ€§åˆ†ç±»å™¨ã€‚ Followed by a second layer of linear
classifiers.</p></li><li><p>æœ€åæ˜¯ä¸€ä¸ªå®ç°é€»è¾‘å‡½æ•°çš„å•å…ƒã€‚ A final unit implementing a logical
function.</p></li></ul><p>è¯¥ç®¡é“åœ¨åŸå§‹ç©ºé—´ä¸­äº§ç”Ÿåœ†å½¢è¾¹ç•Œã€‚ This pipeline produces circular
boundaries in the original space.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102154438703.png width=991 height=460 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102154438703_hu_30710b5f2341c61f.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102154438703_hu_d8969fb3efa0bbf8.png 1024w" loading=lazy alt=image-20250102154438703 class=gallery-image data-flex-grow=215 data-flex-basis=517px></p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102154446772.png width=868 height=396 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102154446772_hu_e4e362338496b1c.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102154446772_hu_b5d93ee6f480a4e2.png 1024w" loading=lazy alt=image-20250102154446772 class=gallery-image data-flex-grow=219 data-flex-basis=526px></p><h3 id=å¤æ‚æ¨¡å‹kernelæ–¹æ³•-complex-models-and-kernel-methods>å¤æ‚æ¨¡å‹&amp;Kernelæ–¹æ³• Complex models and kernel methods</h3><p>è®¸å¤šå¤æ‚çš„æœºå™¨å­¦ä¹ æ¨¡å‹å®é™…ä¸Šå¯ä»¥è¢«è§†ä¸ºä¸€ä¸ªå˜æ¢åæ¥ä¸€ä¸ªç®€å•æ¨¡å‹ã€‚ Many
complex machine learning models can in fact be interpreted as a
transformation followed by a simple model.</p><ul><li><p>æˆ‘ä»¬çŸ¥é“å¦‚ä½•å˜æ¢æ•°æ®ï¼Œåªéœ€è¦å­¦ä¹ åœ¨ç›®æ ‡ç©ºé—´ä¸Šæ“ä½œçš„æ¨¡å‹ã€‚ We know how
to transform our data and only need to learn the model that operates
on the destination space.</p></li><li><p>æˆ‘ä»¬ä¸çŸ¥é“å˜æ¢ï¼Œå› æ­¤ä¹Ÿéœ€è¦å­¦ä¹ å®ƒã€‚ We don&rsquo;t know the transformation,
hence we need to learn it too.</p></li></ul><p>è¿™å¯èƒ½æ¶‰åŠé€šè¿‡éªŒè¯é€‰æ‹©æ­£ç¡®çš„å˜æ¢ï¼Œæˆ–é€šè¿‡è®­ç»ƒè°ƒæ•´ç»™å®šå˜æ¢çš„å‚æ•°ã€‚ This
can involve selecting the right transformation (via validation) or
tuning the parameters of a given transformation (via training).</p><p>æ ¸æ–¹æ³•ï¼Œå¦‚æ”¯æŒå‘é‡æœºï¼Œä½¿ç”¨æ‰€è°“çš„æ ¸å‡½æ•°éšå¼å®šä¹‰è¿™ç§å˜æ¢ã€‚ Kernel methods,
such as support vector machines, implicitly define such transformations
using so-called kernel functions.</p><h3 id=é™ç»´-dimensionality-reduction>é™ç»´ Dimensionality reduction</h3><p>å¯ä»¥ä½¿ç”¨PCAå…ˆè¿›è¡Œé™ç»´ï¼Œç„¶åæ„å»ºæ¨¡å‹ã€‚</p><h3 id=ç‰¹å¾é€‰æ‹©>ç‰¹å¾é€‰æ‹©</h3><p>ç‰¹å¾é€‰æ‹©æ˜¯ä¸€ç§å‡å°‘æ•°æ®é›†ç»´åº¦çš„æ–¹æ³•ï¼Œå®ƒå‡è®¾<strong>åªæœ‰åŸå§‹å±æ€§çš„ä¸€ä¸ªå­é›†æ˜¯ç›¸å…³çš„</strong>ã€‚
Feature selection is a method to reduce the dimensionality of a dataset
that assumes that only a subset of the original attributes are relevant.</p><p>ä¸ºäº†é€‰æ‹©æœ€ç›¸å…³çš„ç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦èƒ½å¤Ÿä¸ºä¸åŒçš„ç‰¹å¾å­é›†åˆ†é…ä¸€ä¸ªåˆ†æ•°ã€‚ To
select the most relevant features, we need to be able to assign a score
to different subsets of features.</p><ul><li>å¦‚æœæˆ‘ä»¬çš„æ•°æ®é›†æœ‰Mä¸ªå±æ€§ï¼Œæ€»å…±æœ‰2^M âˆ’
1ä¸ªå­é›†å¯ä»¥è€ƒè™‘ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬çš„æ•°æ®é›†æœ‰10ä¸ªå±æ€§ï¼Œæˆ‘ä»¬å¤§çº¦æœ‰1000ä¸ªé€‰é¡¹ï¼‰ã€‚
If our dataset has M attributes, there are a total of 2^M âˆ’ 1
subsets that we could consider (e.g., If our dataset has 10
attributes, we have roughly 1000 options).</li><li>åœ¨ç›‘ç£å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æˆ‘ä»¬çš„ç›®æ ‡æŒ‡æ ‡æ¥è¯„ä¼°ä¸€ä¸ªå±æ€§å­é›†çš„ç›¸å…³æ€§ã€‚
What do we mean by relevant? In supervised learning, we can use our
target metric to evaluate how relevant a subset of attributes is.</li><li>æˆ‘ä»¬ä»ç„¶éœ€è¦åœ¨æ¯ä¸ªç‰¹å¾å­é›†ä¸Šè®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚æœ€ç»ˆçš„ç›¸å…³æ€§è¿˜å–å†³äºæˆ‘ä»¬è®­ç»ƒæ¨¡å‹çš„èƒ½åŠ›ã€‚
We still need a model trained on each subset of features. The final
relevance will also depend on our ability to train a model.</li><li>ç‰¹å¾é€‰æ‹©å¯ä»¥è¢«çœ‹ä½œæ˜¯ä¸€ç§éªŒè¯å½¢å¼ï¼Œå…¶ä¸­æˆ‘ä»¬é€‰æ‹©ä½¿ç”¨ä¸åŒå±æ€§å­é›†çš„æ¨¡å‹ã€‚
Feature selection can be seen as a form of validation, where we
select models that use different subset of attributes.</li></ul><h4 id=filtering>Filtering</h4><p>æœ€ç®€å•çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•æ˜¯å•ç‹¬è€ƒè™‘æ¯ä¸ªå±æ€§ã€‚<br>The simplest approach to feature selection is to consider each attribute
individually.</p><p>å¯ä»¥é€šè¿‡æ‹Ÿåˆæ¨¡å‹å¹¶è·å–å…¶éªŒè¯æ€§èƒ½æ¥åˆ†é…åˆ†æ•°ã€‚<br>A score can be assigned by fitting a model and obtaining its validation
performance.</p><p>ç„¶åé€‰æ‹©æœ€ä½³ç»„ä»¶ã€‚<br>Then, the best components are selected.</p><p>ä½†æ˜¯å¯èƒ½æ•ˆæœä¸å¥½</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102155950561.png width=1183 height=334 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102155950561_hu_304eba5aedf621f6.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102155950561_hu_9a15f176f79fb27c.png 1024w" loading=lazy alt=image-20250102155950561 class=gallery-image data-flex-grow=354 data-flex-basis=850px></p><h4 id=wrapping>Wrapping</h4><p>å¦‚æœæˆ‘ä»¬æ€€ç–‘ç‰¹å¾ä¹‹é—´çš„äº¤äº’å¯èƒ½è‡³å…³é‡è¦ï¼Œæˆ‘ä»¬åˆ«æ— é€‰æ‹©ï¼Œåªèƒ½<strong>ä¸€èµ·è¯„ä¼°å®ƒä»¬</strong>ï¼Œè€Œä¸æ˜¯åˆ†å¼€è¯„ä¼°ã€‚
If we suspect that the interaction between features might be crucial, we
have no choice but to evaluate them together, rather than separately.</p><p>åŒ…è£…æ–¹æ³•é€šè¿‡ä»¥ä¸‹æ–¹å¼è€ƒè™‘é¢„æµ‹å˜é‡ä¹‹é—´å¯èƒ½çš„äº¤äº’ï¼š Wrapping approaches
consider possible interaction between predictors by:</p><ul><li>è®­ç»ƒå…·æœ‰<strong>ä¸åŒç‰¹å¾å­é›†</strong>çš„æ¨¡å‹ Training a model with different
subsets of features</li><li>ä½¿ç”¨<strong>éªŒè¯</strong>æ–¹æ³•è¯„ä¼°æ¯ä¸ªç”Ÿæˆçš„æ¨¡å‹ Evaluating each resulting model
by using validation approaches</li><li>é€‰æ‹©å…·æœ‰<strong>æœ€é«˜éªŒè¯æ€§èƒ½</strong>çš„å­é›† Picking the subset with the highest
validation performance.</li></ul><p>è´ªå©ªæœç´¢å¯ä»¥ç”¨æ¥å‡å°‘é€‰é¡¹çš„æ•°é‡ã€‚<br>Greedy search can be used to reduce the number of options.</p><h3 id=ç‰¹å¾æå–-feature-extraction>ç‰¹å¾æå– Feature extraction</h3><p>ç‰¹å¾æå–å¯ä»¥æ˜¾è‘—é™ä½æ•°æ®é›†çš„ç»´åº¦ï¼Œé€šè¿‡ä½¿ç”¨ä¸€äº›ç²¾å¿ƒè®¾è®¡çš„ç‰¹å¾æ¥æ€»ç»“æ•°æ®é›†ã€‚
Feature extraction can reduce dramatically the dimensionality of a
dataset summarising it using a few well-designed features.</p><h3 id=é›†æˆ-ensembles>é›†æˆ Ensembles</h3><p>é›†æˆæ–¹æ³•å…è®¸æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç»“åˆåŸºç¡€æ¨¡å‹ä¼˜åŠ¿çš„æ–°æ¨¡å‹ã€‚ Ensemble methods
allow us to create a new model that combines the strengths of base
models.</p><p>åŸºç¡€æ¨¡å‹éœ€è¦å°½å¯èƒ½å¤šæ ·åŒ–ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼åˆ›å»ºï¼š Base models need to be
as diverse as possible and can be created by:</p><ul><li><p>ä½¿ç”¨æ•°æ®çš„éšæœºå­é›†è®­ç»ƒä¸€ç³»åˆ—æ¨¡å‹ã€‚ Training a family of models with
random subsets of the data.</p></li><li><p>ä½¿ç”¨å±æ€§çš„éšæœºå­é›†è®­ç»ƒä¸åŒçš„æ¨¡å‹ã€‚ Training different models with
random subsets of attributes.</p></li><li><p>è®­ç»ƒå®Œå…¨ä¸åŒçš„æ¨¡å‹å®¶æ—ã€‚ Training different families of models
altogether.</p></li></ul><h4 id=bagging>Bagging</h4><p>Bootstrap æ˜¯ä¸€ç§ä»æ•°æ®é›†ä¸­æå–éšæœºæ ·æœ¬çš„ç»Ÿè®¡æ–¹æ³•ã€‚ Bootstrap is a
statistical method that extracts random samples from a dataset.</p><p>ç»™å®šä¸€ä¸ªè®­ç»ƒæ•°æ®é›†ï¼ŒBagging é€šè¿‡è‡ªåŠ©æ³•ç”Ÿæˆ K
ä¸ªå­æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨æ¯ä¸ªå­æ•°æ®é›†è®­ç»ƒ K ä¸ªç®€å•çš„åŸºç¡€æ¨¡å‹ã€‚ Given a training
dataset, bagging generates K sub-datasets by bootstrapping and trains K
simple base models with each sub-dataset.</p><p>æœ€ç»ˆçš„æ¨¡å‹ f(x) é€šè¿‡å¯¹åŸºç¡€æ¨¡å‹ fk(x) çš„é¢„æµ‹ç»“æœè¿›è¡Œå¹³å‡æˆ–æŠ•ç¥¨æ¥ç»„åˆã€‚
The final model f(x) combines the predictions of the base models fk(x)
by averaging or voting.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102160518817.png width=1071 height=336 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102160518817_hu_1cdf4c42cf65096d.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102160518817_hu_836157fc912de2c8.png 1024w" loading=lazy alt=image-20250102160518817 class=gallery-image data-flex-grow=318 data-flex-basis=765px></p><h4 id=decision-trees>Decision trees</h4><p>å†³ç­–æ ‘åˆ†ç±»å™¨é€šè¿‡ä»…ä½¿ç”¨ä¸€ä¸ªé¢„æµ‹å˜é‡å®æ–½ä¸€ç³»åˆ—åˆ†å‰²è§„åˆ™ï¼Œå°†é¢„æµ‹ç©ºé—´åˆ’åˆ†ä¸ºå¤šä¸ªå†³ç­–åŒºåŸŸã€‚<br>Decision tree classifiers partition the predictor space into multiple
decision regions by implementing sequences of splitting rules using one
predictor only.</p><p>è¿™å¯¼è‡´äº†ä¸€ç§å¯ä»¥è¡¨ç¤ºä¸ºæ ‘çš„ç®—æ³•ã€‚<br>This leads to an algorithm that can be represented as a tree.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102160527610.png width=1164 height=472 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102160527610_hu_b5c1afc32ca377e3.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102160527610_hu_c7fd6bb5d0ffca38.png 1024w" loading=lazy alt=image-20250102160527610 class=gallery-image data-flex-grow=246 data-flex-basis=591px></p><p>å†³ç­–æ ‘çš„æ„å»ºï¼š</p><p>åœ¨å†³ç­–æ ‘ä¸­ï¼Œ<strong>æ ¹èŠ‚ç‚¹</strong>å¯¹åº”äºæ•´ä¸ªæœªåˆ†å‰²çš„æ•°æ®é›†ï¼Œè€Œ<strong>å¶èŠ‚ç‚¹</strong>æ˜¯å…¶ä¸­ä¸€ä¸ªå†³ç­–åŒºåŸŸã€‚<br>In a decision tree, the root corresponds to the whole, unpartitioned
dataset and a leaf is one of the decision regions.</p><ul><li>ç›®æ ‡æ˜¯åˆ›å»º<strong>çº¯å‡€çš„å¶èŠ‚ç‚¹</strong>ï¼Œå³å°½å¯èƒ½åŒ…å«æ¥è‡ªåŒä¸€ç±»åˆ«çš„æ ·æœ¬ã€‚<br>The goal is to create pure leaves, i.e.Â containing as many samples
from the same class as possible.\</li><li>åœ¨åˆ†ç±»è¿‡ç¨‹ä¸­ï¼Œæ ·æœ¬è¢«åˆ†é…åˆ°å…¶æ‰€åœ¨å¶èŠ‚ç‚¹ä¸­çš„å¤šæ•°ç±»åˆ«ã€‚<br>During classification, a sample is assigned to the majority class in
the leaf where the sample is located.</li></ul><p>å†³ç­–æ ‘æ˜¯<strong>é€’å½’</strong>æ„å»ºçš„ï¼šä»æ ¹èŠ‚ç‚¹å¼€å§‹ï¼Œæˆ‘ä»¬é€’å½’åœ°<strong>å°†æ¯ä¸ªåŒºåŸŸåˆ†å‰²æˆä¸¤éƒ¨åˆ†</strong>ã€‚<br>Decision trees are built recursively: Starting from the root, we
recursively split each region into two.</p><ul><li>åˆ†å‰²æ˜¯<strong>è½´å¹³è¡Œçš„</strong>ï¼ˆä½¿ç”¨<strong>ä¸€ä¸ªé¢„æµ‹å˜é‡</strong>è¿›è¡Œå†³ç­–ï¼‰ã€‚<br>Splits are axis-parallel (decisions using one predictor).\</li><li>é€‰æ‹©çš„åˆ†å‰²æ–¹å¼ä½¿å¾—ç»“æœåŒºåŸŸçš„çº¯åº¦é«˜äºä»»ä½•å…¶ä»–åˆ†å‰²æ–¹å¼ã€‚<br>The chosen split is such that the purity of the resulting regions is
higher than any other split.\</li><li>å½“æ»¡è¶³ç»™å®šæ¡ä»¶æ—¶åœæ­¢ï¼Œä¾‹å¦‚åŒºåŸŸä¸­çš„æ ·æœ¬æ•°é‡ã€‚<br>We stop when a given criterion is met, such as the number of samples
in a region.</li></ul><p>å†³ç­–æ ‘æ— æ³•è§£å†³XORé—®é¢˜</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102160853573.png width=1177 height=640 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102160853573_hu_55c98f9d68c93ea5.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102160853573_hu_41808d7f599145e6.png 1024w" loading=lazy alt=image-20250102160853573 class=gallery-image data-flex-grow=183 data-flex-basis=441px></p><p>å†³ç­–æ ‘çš„é—®é¢˜ï¼š</p><p>æ ‘æ¨¡å‹ç®€å•ä¸”èƒ½è½»æ¾å¤„ç†æ•°å€¼å‹å’Œç±»åˆ«å‹é¢„æµ‹å˜é‡ã€‚ Trees are simple and can
handle easily both numerical and categorical predictors.</p><ul><li><p>ç„¶è€Œï¼Œæ ‘æ¨¡å‹å­˜åœ¨è®°å¿†è®­ç»ƒæ ·æœ¬çš„é£é™©ï¼Œå³è¿‡æ‹Ÿåˆã€‚ However, trees run
the risk of memorising training samples, i.e.Â overfitting.</p></li><li><p>å‰ªææŠ€æœ¯å’Œåœæ­¢å‡†åˆ™å¯ä»¥å¸®åŠ©é˜²æ­¢è¿‡æ‹Ÿåˆã€‚ Pruning techniques and stop
criteria can help to prevent this.</p></li><li><p>ä¸åŒçš„è®­ç»ƒæ•°æ®é›†å¯èƒ½å¯¼è‡´ä¸åŒçš„æ ‘ç»“æ„ã€‚ Different training datasets
can lead to a different tree structures.</p></li><li><p>æŸäº›åˆ†ç±»é—®é¢˜å¯ä»¥å¾ˆå®¹æ˜“åœ°è¡¨ç¤ºä¸ºæ ‘ï¼Œä½†æ ‘å¯èƒ½éš¾ä»¥å­¦ä¹ ï¼ˆä¾‹å¦‚å¼‚æˆ–é—®é¢˜ï¼‰ã€‚
Some classification problems can be easily represented as a tree,
but the tree might be hard to learn (e.g.Â XOR).</p></li></ul><p>ä½¿ç”¨éšæœºæ£®æ—é›†æˆå†³ç­–æ ‘å¯ä»¥è§£å†³è¿™äº›é—®é¢˜ã€‚</p><h4 id=éšæœºæ£®æ—>éšæœºæ£®æ—</h4><p>éšæœºæ£®æ—é€šè¿‡éšæœºåŒ–è®­ç»ƒæ ·æœ¬å’Œé¢„æµ‹å™¨æ¥è®­ç»ƒè®¸å¤šå•ç‹¬çš„æ ‘ã€‚é¢„æµ‹æ˜¯é€šè¿‡å¯¹å„ä¸ªé¢„æµ‹è¿›è¡Œå¹³å‡å¾—åˆ°çš„ã€‚<br>Random forests train many individual trees by randomising the training
samples and the predictors. Predictions are obtained by averaging the
individual predictions.</p><p>å®ƒä»¬é€šå¸¸å…·æœ‰å¾ˆé«˜çš„å‡†ç¡®æ€§ï¼Œä½†è®­ç»ƒæˆæœ¬è¾ƒé«˜ï¼Œå¹¶ä¸”æ¯”å•æ£µæ ‘æ›´éš¾è§£é‡Šã€‚<br>In general they have great accuracy, but can be expensive to train and
are harder to interpret than a single tree.</p><h4 id=boosting>Boosting</h4><p>Boosting
é‡‡ç”¨äº†ä¸€ç§ä¸åŒçš„æ–¹æ³•ï¼šå®ƒç”Ÿæˆä¸€ç³»åˆ—ç®€å•çš„åŸºç¡€æ¨¡å‹ï¼Œå…¶ä¸­æ¯ä¸ªåç»­æ¨¡å‹éƒ½ä¸“æ³¨äºå‰ä¸€ä¸ªæ¨¡å‹æ— æ³•æ­£ç¡®å¤„ç†çš„æ•°æ®æ ·æœ¬ã€‚
Boosting follows a different approach: it generates a sequence of simple
base models, where each successive model focuses on the samples that the
previous models could not handle properly.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102161212909.png width=951 height=496 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102161212909_hu_ca449c3cb3f1b751.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102161212909_hu_c5bed02912617c95.png 1024w" loading=lazy alt=image-20250102161212909 class=gallery-image data-flex-grow=191 data-flex-basis=460px></p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102161220671.png width=793 height=610 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102161220671_hu_9c4014082f88cab4.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102161220671_hu_8163cb58231bd060.png 1024w" loading=lazy alt=image-20250102161220671 class=gallery-image data-flex-grow=130 data-flex-basis=312px></p><h2 id=ç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ week-3-2>ç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ã€Week 3-2ã€‘</h2><h3 id=æ¨¡å¼ä¸ç»“æ„>æ¨¡å¼ä¸ç»“æ„</h3><p>æ¨¡å¼æ˜¯æˆ‘ä»¬æ•°æ®ä¸­çš„è§„å¾‹æ€§ï¼Œè€Œç»“æ„æ˜¯ç›®æ ‡ç¾¤ä½“ä¸­çš„è§„å¾‹æ€§ã€‚ Patterns are
regularities in our data, and structure is a regularity in our target
population.</p><p>æœºå™¨å­¦ä¹ é¡¹ç›®ä¾èµ–äºé€šè¿‡è¯†åˆ«æ•°æ®ä¸­çš„æ¨¡å¼æ¥å‘ç°æ½œåœ¨ç»“æ„ã€‚ Machine learning
projects rely on discovering the underlying structure by identifying
patterns in data.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102161448831.png width=1149 height=382 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102161448831_hu_a37a0e3fee25d8ab.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102161448831_hu_7e1fe60c56f9278a.png 1024w" loading=lazy alt=image-20250102161448831 class=gallery-image data-flex-grow=300 data-flex-basis=721px></p><h3 id=the-curse-of-dimensionality>The Curse of Dimensionality</h3><p>ç»´åº¦ç¾éš¾æ˜¯ä¸€ä¸ªè­¦å‘Šã€‚æ— å…³çš„å±æ€§ä¸ä¼šç›¸äº’æŠµæ¶ˆï¼Œå®ƒä»¬ä¼šè¡¨ç°ä¸ºè™šå‡çš„æ¨¡å¼ã€‚æ·»åŠ æ›´å¤šå±æ€§ï¼ˆä»¥é˜²ä¸‡ä¸€ï¼‰å®é™…ä¸Šå¯èƒ½å¯¼è‡´æ¨¡å‹æ€§èƒ½å˜å·®ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç‰¹å¾é€‰æ‹©æŠ€æœ¯æ¥é™ä½é—®é¢˜çš„ç»´åº¦ã€‚
The curse of dimensionality is a warning. Irrelevant attributes do not
cancel each other out, they show up as spurious patterns. Adding more
attributes (just in case) can actually result in worse-performing
models. We can use feature selection techniques to reduce the
dimensionality of the problem but first, let&rsquo;s use our domain knowledge.</p><h3 id=ç¥ç»ç½‘ç»œ>ç¥ç»ç½‘ç»œ</h3><p>ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§å—äººç±»ç¥ç»ç³»ç»Ÿå¯å‘è€Œè®¾è®¡çš„è®¡ç®—ç³»ç»Ÿï¼Œé€šå¸¸ä½œä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹å®¶æ—çš„ä¸€éƒ¨åˆ†ã€‚<br>A neural network is a computing system loosely inspired by the human
nervous system, commonly used as a family of Machine Learning models.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102161622247.png width=1065 height=531 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102161622247_hu_c47ba6c4b9d8c1ce.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102161622247_hu_a462b6e341c1e204.png 1024w" loading=lazy alt=image-20250102161622247 class=gallery-image data-flex-grow=200 data-flex-basis=481px></p><p>ä»è®¡ç®—è§’åº¦æ¥çœ‹ï¼Œç¥ç»ç½‘ç»œç”±ç›¸äº’è¿æ¥çš„å•ä½ç»„æˆï¼Œè¿™äº›å•ä½ï¼ˆæ¾æ•£åœ°ï¼‰æ¨¡ä»¿ç¥ç»å…ƒã€‚è¿™ç§æ¶æ„å…·æœ‰å¸å¼•åŠ›ï¼Œå› ä¸ºï¼š
From a computational angle, neural networks consist of interconnected
units that (loosely) mimic neurons. This architecture is appealing
since:</p><ul><li>ç¥ç»ç§‘å­¦è¡¨æ˜ï¼Œç”Ÿç‰©ç¥ç»ç½‘ç»œå¯ä»¥è§£å†³ä»»ä½•é—®é¢˜ã€‚ Neuroscience suggests
biological neural networks can solve any problem.</li><li>æ•°å­¦è¡¨æ˜ï¼Œäººå·¥ç¥ç»ç½‘ç»œå¯ä»¥å†ç°ä»»ä½•è¾“å…¥/è¾“å‡ºå…³ç³»ï¼Œåªè¦å®ƒä»¬è¶³å¤Ÿå¤æ‚ã€‚
Mathematics suggests artificial neural networks can reproduce any
input/output relationship, provided they are complex enough.</li><li>å› æ­¤ï¼Œç¥ç»ç½‘ç»œæ¨¡å‹å®¶æ—å¯ä»¥è¢«è§†ä¸ºä¸€ç§é€šç”¨æœºå™¨ã€‚ Hence, the family of
neural network models can be seen as a universal machine.</li></ul><h3 id=æ„ŸçŸ¥å™¨-perceptron>æ„ŸçŸ¥å™¨ perceptron</h3><p>æ„ŸçŸ¥å™¨æ˜¯ç¥ç»ç½‘ç»œçš„åŸºæœ¬å•å…ƒã€‚<br>The perceptron is the basic unit of a neural network.</p><p>å®ƒç”±ä¸€ä¸ªæƒé‡å‘é‡ w å’Œä¸€ä¸ªæ¿€æ´»å‡½æ•° h(â‹…) å®šä¹‰ï¼Œå°†æ‰©å±•å‘é‡ x æ˜ å°„åˆ°è¾“å‡º
aã€‚<br>It is defined by a weight vector w and an activation function h(â‹…) that
map an extended vector x to an output a.</p><p>ç³»æ•° w0 è¢«ç§°ä¸ºåç½®ã€‚<br>The coefficient w0 is known as the bias.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102161822071.png width=1018 height=487 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102161822071_hu_cdd791b9c791fced.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102161822071_hu_461f75fa1be6582d.png 1024w" loading=lazy alt=image-20250102161822071 class=gallery-image data-flex-grow=209 data-flex-basis=501px></p><p>æ¿€æ´»å‡½æ•°ä¸€èˆ¬æ˜¯éçº¿æ€§çš„ã€‚</p><h3 id=å±‚-layer>å±‚ Layer</h3><p>å±‚æ˜¯ä½¿ç”¨ç›¸åŒè¾“å…¥çš„æ„ŸçŸ¥å™¨çš„é›†åˆã€‚ A layer is a collection of perceptrons
that use the same input.</p><p>å±‚ä¸­çš„æ¯ä¸ªæ„ŸçŸ¥å™¨éƒ½ä¼šäº§ç”Ÿä¸€ä¸ªç‹¬ç«‹çš„è¾“å‡ºã€‚ Each perceptron within a layer
produces a separate output.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102161914678.png width=1009 height=484 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102161914678_hu_398695efb92de27a.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102161914678_hu_61deb36b57d71cd9.png 1024w" loading=lazy alt=image-20250102161914678 class=gallery-image data-flex-grow=208 data-flex-basis=500px></p><p>ç”± L ä¸ªæ„ŸçŸ¥å™¨å’Œ K ä¸ªè¾“å…¥ç»„æˆçš„ä¸€å±‚æœ‰ L Ã— (K + 1)ä¸ªæƒé‡ã€‚</p><h3 id=æ¶æ„-architecture>æ¶æ„ Architecture</h3><p>ç¥ç»ç½‘ç»œçš„æ¶æ„æè¿°äº†å„å±‚ä¹‹é—´çš„è¿æ¥æ–¹å¼ã€‚<br>The architecture of a neural network describes how layers are connected.</p><p>è¾“å…¥å±‚æ˜¯é¢„æµ‹å‘é‡ï¼Œéšè—å±‚ç”Ÿæˆå†…éƒ¨ç‰¹å¾ï¼Œè¾“å‡ºå±‚ç”Ÿæˆé¢„æµ‹ç»“æœã€‚<br>The input layer is the predictor vector, hidden layers produce internal
features, and the output layer produces the prediction.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102161956319.png width=922 height=381 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102161956319_hu_538a4bed7b5adf66.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102161956319_hu_4fe256cecd4f015d.png 1024w" loading=lazy alt=image-20250102161956319 class=gallery-image data-flex-grow=241 data-flex-basis=580px></p><h3 id=ç¥ç»ç½‘ç»œ-1>ç¥ç»ç½‘ç»œ</h3><ul><li><p>ç¥ç»ç½‘ç»œç”±æŒ‰æ¶æ„è¿æ¥çš„æ„ŸçŸ¥å™¨å±‚ç»„æˆã€‚ A neural network consists of
perceptrons arranged in layers that are connected according to an
architecture.</p></li><li><p>æ¯ä¸ªè¿æ¥æœ‰ä¸€ä¸ªå‚æ•°ï¼ˆè¿æ¥çš„æƒé‡ï¼‰ã€‚ There is one parameter per
connection (the connection&rsquo;s weight).</p></li><li><p>æ¯ä¸€å±‚ç”Ÿæˆä¸­é—´ç‰¹å¾ã€‚ Each layer produces intermediate features.</p></li><li><p>å±‚æ•°å†³å®šäº†ç¥ç»ç½‘ç»œçš„æ·±åº¦ï¼ˆå› æ­¤åŒºåˆ†äº†æµ…å±‚ç¥ç»ç½‘ç»œå’Œæ·±å±‚ç¥ç»ç½‘ç»œï¼‰ã€‚
The number of layers determines the depth of the neural network
(hence the distinction between shallow and deep neural networks).</p></li></ul><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102162502360.png width=831 height=322 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102162502360_hu_7ceafe7ef91c1d1a.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102162502360_hu_8edb18c3ad096cbb.png 1024w" loading=lazy alt=image-20250102162502360 class=gallery-image data-flex-grow=258 data-flex-basis=619px></p><h3 id=æ„ŸçŸ¥å™¨ä½œä¸ºçº¿æ€§åˆ†ç±»å™¨>æ„ŸçŸ¥å™¨ä½œä¸ºçº¿æ€§åˆ†ç±»å™¨</h3><p>ä½¿ç”¨é˜¶è·ƒå‡½æ•°ä½œä¸ºæ¿€æ´»å‡½æ•°ï¼Œåˆ™å¾—åˆ°çº¿æ€§åˆ†ç±»å™¨</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102162544118.png width=1075 height=468 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102162544118_hu_392164cd6be02ea2.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102162544118_hu_fd9725efb59d1586.png 1024w" loading=lazy class=gallery-image data-flex-grow=229 data-flex-basis=551px></p><h3 id=æ„ŸçŸ¥å™¨å®ç°é€»è¾‘é—¨åŠŸèƒ½>æ„ŸçŸ¥å™¨å®ç°é€»è¾‘é—¨åŠŸèƒ½</h3><p>æ¿€æ´»å‡½æ•°ä¸ºé˜¶è·ƒå‡½æ•°ã€‚</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102162709009.png width=1065 height=345 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102162709009_hu_67c4007d186b6e13.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102162709009_hu_bde78e2fbe189d4d.png 1024w" loading=lazy alt=image-20250102162709009 class=gallery-image data-flex-grow=308 data-flex-basis=740px></p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102162945140.png width=1122 height=436 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102162945140_hu_85c9723155f44348.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102162945140_hu_380e636733f3924.png 1024w" loading=lazy alt=image-20250102162945140 class=gallery-image data-flex-grow=257 data-flex-basis=617px></p><h3 id=æ„ŸçŸ¥å™¨å®ç°ç½‘æ ¼æ¨¡å¼æ£€æµ‹>æ„ŸçŸ¥å™¨å®ç°ç½‘æ ¼æ¨¡å¼æ£€æµ‹</h3><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102162919167.png width=1006 height=666 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102162919167_hu_766a29b8e12e6443.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102162919167_hu_ceff28823b7b50c3.png 1024w" loading=lazy alt=image-20250102162919167 class=gallery-image data-flex-grow=151 data-flex-basis=362px></p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102162932255.png width=1098 height=544 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102162932255_hu_5073ef9be75b869b.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102162932255_hu_560b010e7f238e5.png 1024w" loading=lazy alt=image-20250102162932255 class=gallery-image data-flex-grow=201 data-flex-basis=484px></p><h3 id=ç»“åˆçº¿æ€§åˆ†ç±»å™¨å’Œé€»è¾‘åŠŸèƒ½å¯ä»¥å¾—åˆ°>ç»“åˆçº¿æ€§åˆ†ç±»å™¨å’Œé€»è¾‘åŠŸèƒ½å¯ä»¥å¾—åˆ°ï¼š</h3><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102163026868.png width=880 height=511 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102163026868_hu_cc1b35a1a42eb287.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102163026868_hu_300009829c7fc724.png 1024w" loading=lazy alt=image-20250102163026868 class=gallery-image data-flex-grow=172 data-flex-basis=413px></p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102163040128.png width=1014 height=328 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102163040128_hu_a05e6024ad010a15.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102163040128_hu_98e396e6edf0e6ec.png 1024w" loading=lazy alt=image-20250102163040128 class=gallery-image data-flex-grow=309 data-flex-basis=741px></p><h3 id=ä»è®¡ç®—è§’åº¦çœ‹å¾…ç¥ç»ç½‘ç»œ>ä»è®¡ç®—è§’åº¦çœ‹å¾…ç¥ç»ç½‘ç»œ</h3><p>ä»è®¤çŸ¥è§’åº¦æ¥çœ‹ï¼Œå¤§å‹ç¥ç»ç½‘ç»œå…·æœ‰å¸å¼•åŠ›ï¼Œå› ä¸ºå®ƒä»¬ä¸ºæˆ‘ä»¬æä¾›äº†åˆ›å»ºæ–°çš„ã€æ—¥ç›Šå¤æ‚çš„æ¦‚å¿µæ‰€éœ€çš„<strong>çµæ´»æ€§</strong>ï¼Œè¿™äº›æ¦‚å¿µå¯èƒ½å¯¹åšå‡ºé¢„æµ‹è‡³å…³é‡è¦ã€‚<br>From a cognitive point of view, large neural networks are appealing, as
they give us the necessary flexibility to create new and increasingly
complex concepts that might be relevant to make a prediction.</p><p>ç„¶è€Œï¼Œæ›´é«˜çš„çµæ´»æ€§å¢åŠ äº†<strong>è¿‡æ‹Ÿåˆ</strong>çš„é£é™©ï¼ˆæˆ‘ä»¬å¯ä»¥å°†å…¶è§†ä¸ºç½‘ç»œåˆ›å»ºå’Œä½¿ç”¨æ— å…³æ¦‚å¿µï¼‰ã€‚<br>However, higher flexibility increases the risk of overfitting (which we
can see as a network creating and using irrelevant concepts).</p><p>å¤§é‡çš„å‚æ•°éœ€è¦è°ƒæ•´ï¼Œå› æ­¤<strong>è®¡ç®—éœ€æ±‚</strong>å¯èƒ½è¿‡é«˜ã€‚<br>A large number of parameters need to be tuned, therefore the
computational requirements might be too high.</p><p>å¯¹äº<strong>å¤æ‚çš„è¾“å…¥</strong>ï¼Œä¾‹å¦‚ç”±æ•°ç™¾ä¸‡åƒç´ ç»„æˆçš„å›¾ç‰‡ï¼Œè¿™ç§æƒ…å†µæ›´åŠ ä¸¥é‡ã€‚<br>For complex inputs, such as pictures consisting of millions of pixels,
this is even more severe.</p><h3 id=ç¥ç»ç½‘ç»œæˆæœ¬å‡½æ•°>ç¥ç»ç½‘ç»œæˆæœ¬å‡½æ•°</h3><p>ç»™å®šæ•°æ®é›†${\left(\boldsymbol{x}<em>i,y_i\right),1\leq i\leq N}$ï¼Œæ ‡ç­¾å–å€¼ä¸º0æˆ–1ï¼Œå¸¸ç”¨è´Ÿå¯¹æ•°ä¼¼ç„¶å‡½æ•°ï¼š
$$
l(\boldsymbol{W})=-\frac{1}{N}\sum</em>{n=1}^Ny_i\log\left[\hat{y}_i\right]+\left(1-y_i\right)\log\left[1-\hat{y}_i\right]
$$ å…¶ä¸­$\hat{y}<em>i=h</em>{\boldsymbol{W}}(x_i)$ã€‚å¯ä»¥æ‰©å±•åˆ°å¤šç±»åˆ†ç±»å™¨ã€‚</p><h3 id=æ¢¯åº¦ä¸‹é™ä¸åå‘ä¼ æ’­>æ¢¯åº¦ä¸‹é™ä¸åå‘ä¼ æ’­</h3><p>æ¢¯åº¦ä¸‹é™æ³•æ˜¯å¯»æ‰¾æˆæœ¬å‡½æ•° l(W) æœ€ä¼˜ç³»æ•°é›† W çš„é¦–é€‰æ–¹æ³•ã€‚ Gradient descent
is the method of choice to find the optimal set of coefficients W for
the cost function l(W).</p><p>è·å–æ¢¯åº¦å¾ˆå®¹æ˜“ï¼Œä½†è®¡ç®—æˆæœ¬å¯èƒ½å¾ˆé«˜ã€‚ Obtaining the gradient is easy, but
can be computationally expensive.</p><p>åå‘ä¼ æ’­æ˜¯ä¸€ç§è®¡ç®—æ¢¯åº¦çš„æœ‰æ•ˆç®—æ³•ã€‚ Back-propagation is an efficient
algorithm to compute the gradient.</p><p>ç„¶åï¼Œä¼˜åŒ–ç®—æ³•ä½¿ç”¨è¯¥æ¢¯åº¦æ¥æ›´æ–° Wã€‚ This gradient is then used by the
optimisation algorithm to update W.</p><p>åå‘ä¼ æ’­åˆ©ç”¨äº†å¾®ç§¯åˆ†ä¸­çš„é“¾å¼æ³•åˆ™ã€‚ Back-propagation exploits the chain
rule of calculus.</p><p>äº‹å®è¯æ˜ï¼Œè¦è®¡ç®—æŸä¸€å±‚çš„æ¢¯åº¦ï¼Œæˆ‘ä»¬åªéœ€è¦æ¥è‡ªä¸‹ä¸€å±‚çš„ä¿¡æ¯ã€‚ It turns out
that to compute the gradient in one layer, we just need information from
the next layer.</p><p>åå‘ä¼ æ’­ä»è¾“å‡ºå¼€å§‹ï¼šå®ƒè·å–æˆæœ¬å¹¶å‘åè®¡ç®—éšè—å•å…ƒçš„æ¢¯åº¦ã€‚
Back-propagation starts from the output: it obtains the cost and
proceeds backwards calculating the gradients of the hidden units.</p><h3 id=è®­ç»ƒæ¨¡å‹çš„æ³¨æ„äº‹é¡¹>è®­ç»ƒæ¨¡å‹çš„æ³¨æ„äº‹é¡¹</h3><ul><li>åˆå§‹åŒ–ï¼šå¦‚æœåˆå§‹æƒé‡ä¸ºé›¶ï¼Œåå‘ä¼ æ’­å°†æ— æ³•è¿›è¡Œã€‚åˆå§‹æƒé‡å€¼åº”ä¸ºéšæœºå€¼ã€‚<br>If the initial weights are zero, back-propagation fails. Initial
weight values should be random.\</li><li>è¿‡æ‹Ÿåˆï¼šç¥ç»ç½‘ç»œå¯èƒ½åŒ…å«æ•°ç™¾ä¸‡ä¸ªå‚æ•°ï¼Œä½¿ç”¨æ­£åˆ™åŒ–å’ŒåŸºäºéªŒè¯çš„æ—©åœæ¥é¿å…è¿‡æ‹Ÿåˆã€‚<br>Neural networks can have millions of parameters. Use regularisation
and validation-based early stop to avoid overfitting.\</li><li>æœ€å°å€¼ï¼šä»£ä»·å‡½æ•°å…·æœ‰å¤šä¸ªå±€éƒ¨æœ€å°å€¼ï¼Œä»ä¸åŒçš„éšæœºåˆå§‹å€¼é‡æ–°è®­ç»ƒã€‚<br>The cost function has multiple local minima. Retrain from different
random starting values.\</li><li>ç¼©æ”¾è¾“å…¥ï¼šè¾“å…¥å€¼çš„èŒƒå›´å¯èƒ½å½±å“æƒé‡çš„å€¼ï¼Œæ ‡å‡†åŒ–ä»¥ç¡®ä¿è¾“å…¥è¢«å¹³ç­‰å¯¹å¾…ã€‚<br>Range of input values can affect the values of the weights.
Standardise to ensure inputs are treated equally.\</li><li>æ¶æ„ï¼šä¸åŒçš„æ¶æ„é€‚ç”¨äºä¸åŒçš„é—®é¢˜ã€‚<br>Different architectures suit different problems.</li></ul><h3 id=è¿ç§»å­¦ä¹ >è¿ç§»å­¦ä¹ </h3><p>ä¸€ä¸ªå·²ç»æˆåŠŸè®­ç»ƒç”¨äºé—®é¢˜Açš„ç¥ç»ç½‘ç»œå¯ä»¥é‡ç”¨äºç›¸å…³é—®é¢˜Bï¼Œä¾‹å¦‚ï¼š A neural
network that has been successfully trained for problem A can be reused
for related problem B, for instance:</p><ul><li><p>æˆ‘ä»¬å¯ä»¥ä¿æŒæ—©æœŸé˜¶æ®µä¸å˜ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªå›ºå®šçš„è½¬æ¢é˜¶æ®µT(x)ã€‚ We can
leave the early stages unchanged, becoming a fixed transformation
stage T(x).</p></li><li><p>æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ–°æ•°æ®f(z)é‡æ–°è®­ç»ƒåæœŸé˜¶æ®µã€‚ We can retrain the late
stages using new data f(z).</p></li></ul><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102164533520.png width=825 height=183 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102164533520_hu_9f1849f460882088.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102164533520_hu_55d7d6fd94fc6dfe.png 1024w" loading=lazy alt=image-20250102164533520 class=gallery-image data-flex-grow=450 data-flex-basis=1081px></p><p>æˆ‘ä»¬æœ¬è´¨ä¸Šæ˜¯åœ¨è½¬ç§»ä¸€ä¸ªå·²ç»å­¦ä¹ åˆ°çš„å˜æ¢ï¼Œå¹¶å°†å…¶é‡æ–°ç”¨äºä¸åŒçš„é—®é¢˜ï¼š We
are in essence transferring an already learnt transformation and reusing
it for a different problem:</p><ul><li><p>æ— éœ€è®­ç»ƒ T(x)ï¼ˆé—®é¢˜ A å’Œé—®é¢˜ B çš„å‚æ•°ç›¸åŒï¼‰ã€‚ No need to train T(x)
(same parameters for problems A and B).</p></li><li><p>é—®é¢˜ B ä¸­ f(z) çš„æœ€ä¼˜å‚æ•°å°†æ¥è¿‘é—®é¢˜ A
ä¸­æ‰¾åˆ°çš„å‚æ•°ï¼ˆè®­ç»ƒæ—¶é—´æ›´çŸ­ï¼ï¼‰ã€‚ The optimal parameters of f(z) for
problem B will be close to the ones found for problem A (shorter
training time!).</p></li></ul><h3 id=å…¨è¿æ¥å±‚>å…¨è¿æ¥å±‚</h3><p>å…¨è¿æ¥å±‚ä¸­çš„æ¯ä¸ªæ„ŸçŸ¥å™¨éƒ½æ¥æ”¶æ¥è‡ªå‰ä¸€å±‚çš„æ‰€æœ‰è¾“å‡ºã€‚ Each perceptron in
the fully connected layer receives all the outputs from the previous
layer.</p><p>å…¨è¿æ¥å±‚å…·æœ‰å¤§é‡çš„å‚æ•°ï¼Œè®­ç»ƒå®ƒä»¬å¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ Fully connected layers
have a large number of parameters, and training them can be challenging.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102164634731.png width=865 height=312 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102164634731_hu_cdccc1be25fab44a.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102164634731_hu_ff8a5d7bebb1bfcc.png 1024w" loading=lazy alt=image-20250102164634731 class=gallery-image data-flex-grow=277 data-flex-basis=665px></p><h3 id=ç½‘æ ¼æ•°æ®ä¸­çš„ç­‰å˜æ€§-equivariance-in-grid-data>ç½‘æ ¼æ•°æ®ä¸­çš„ç­‰å˜æ€§ Equivariance in grid data</h3><p>å›¾åƒå’Œæ—¶é—´åºåˆ—æ˜¯ç”±ä¸å®šä¹‰ç©ºé—´å…³ç³»çš„è§„åˆ™ç½‘æ ¼ç›¸å…³è”çš„å„ä¸ªå±æ€§ç»„æˆçš„å¤æ‚æ•°æ®ç±»å‹ã€‚<br>Images and time series are complex data types consisting of individual
attributes associated to a regular grid defining a spatial relationship.</p><p>æŸäº›ç½‘æ ¼æ•°æ®è¡¨ç°å‡ºç­‰å˜æ€§ï¼Œå³ç›¸åŒçš„æ¨¡å¼å¯ä»¥å‡ºç°åœ¨ç½‘æ ¼çš„ä¸åŒä½ç½®ã€‚<br>Some grid data exhibit the equivariance property, according to which the
same pattern can be expected in different locations of the grid.</p><h3 id=å·ç§¯å±‚>å·ç§¯å±‚</h3><p>å·ç§¯å±‚æ–½åŠ äº†é¢å¤–çš„é™åˆ¶ï¼š Convolutional layers impose additional
restrictions:</p><ul><li><p>æ„ŸçŸ¥å™¨è¢«æ’åˆ—æˆä¸€ä¸ªç§°ä¸ºç‰¹å¾å›¾çš„ç½‘æ ¼ï¼Œ Perceptrons are arranged as a
grid known as a feature map,</p></li><li><p>ä¸“æ³¨äºè¾“å…¥ç½‘æ ¼ä¸­çš„ä¸åŒæœ‰é™åŒºåŸŸï¼Œ focus on different limited regions
in the input grid and</p></li><li><p>å¹¶å…±äº«å®ƒä»¬çš„å‚æ•°ï¼Œè¡¨ç¤ºä¸ºä¸€ä¸ªç§°ä¸ºæ ¸çš„ç½‘æ ¼ã€‚ share their parameters,
represented as a grid called kernel.</p></li></ul><p>ç‰¹å¾å›¾è¢«é«˜æ•ˆåœ°è®¡ç®—ä¸ºæ ¸ä¸è¾“å…¥çš„å·ç§¯ï¼Œ The feature map is efficiently
calculated as a convolution of the kernel</p><p>æˆ–è€…æ¢å¥è¯è¯´ï¼Œç”¨æ ¸è¿‡æ»¤è¾“å…¥ã€‚ and the input or in other words, filtering
the input with the kernel.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102165515151.png width=946 height=453 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102165515151_hu_a464c153bf03ab42.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102165515151_hu_afae0158df8307e.png 1024w" loading=lazy alt=image-20250102165515151 class=gallery-image data-flex-grow=208 data-flex-basis=501px></p><p>å·ç§¯å±‚å¯ä»¥æ‹¥æœ‰å¤šä¸ªç‰¹å¾å›¾ï¼Œæ¯ä¸ªç‰¹å¾å›¾éƒ½ä¸ä¸åŒçš„æ¦‚å¿µç›¸å…³è”ã€‚å®ƒä»¬å½¢æˆäº†ä¸€ç»„å †å çš„å›¾ã€‚
Convolutional layers can have several feature maps, each of which is
associated to a different concept. They form a stack of maps.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102165542654.png width=1135 height=436 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102165542654_hu_1ec8566ebab2b18.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102165542654_hu_313eaad745af92bf.png 1024w" loading=lazy alt=image-20250102165542654 class=gallery-image data-flex-grow=260 data-flex-basis=624px></p><ul><li><p>å·ç§¯æ ¸çš„ç»´åº¦ä¸º H Ã— W Ã— Dï¼Œå…¶ä¸­ H æ˜¯é«˜åº¦ï¼ŒW æ˜¯å®½åº¦ï¼ŒD
æ˜¯æ·±åº¦ï¼ˆè¾“å…¥ç‰¹å¾å›¾çš„æ•°é‡ï¼‰ã€‚<br>The dimensions of a kernel are H Ã— W Ã— D, where H is the height, W
is the width, and D is the depth (number of input feature maps).\</p></li><li><p>æ¯ä¸ªå·ç§¯æ ¸çš„æ€»æƒé‡æ•°ä¸º H Ã— W Ã— D + 1ï¼ˆåŒ…æ‹¬åç½®ï¼‰ã€‚<br>The total number of weights per kernel is H Ã— W Ã— D + 1 (including
the bias).</p></li><li><p>è®­ç»ƒå·ç§¯å±‚æ„å‘³ç€ä½¿ç”¨æ•°æ®æ¥è°ƒæ•´æ¯ä¸ªå·ç§¯æ ¸çš„æƒé‡ã€‚<br>Training a convolutional layer means using data to tune the weights
of each kernel.</p></li></ul><h3 id=æ± åŒ–å±‚>æ± åŒ–å±‚</h3><p>æ± åŒ–å±‚<strong>å‡å°‘äº†ç‰¹å¾å›¾çš„å¤§å°</strong>ã€‚<br>Pooling layers reduce the size of feature maps.</p><p>æ± åŒ–å±‚é€šè¿‡å°†ä¸€å®šåŒºåŸŸå†…çš„å€¼ç¼©å‡ä¸ºä¸€ä¸ªå•ä¸€æ•°å€¼æ¥å®šä¹‰ï¼Œå¹¶æ’å…¥åœ¨è¿ç»­çš„å·ç§¯å±‚ä¹‹é—´ã€‚<br>Pooling layers are defined by reducing a certain area to a single number
and are inserted between successive convolutional layers.</p><p>æ± åŒ–å±‚æœ‰ä¸¤ç§ç±»å‹ï¼š<br>They come in two flavors:</p><ul><li>æœ€å¤§æ± åŒ–ï¼šè¾“å‡ºæ˜¯æ»¤æ³¢å™¨åŒºåŸŸå†…æœ€å¤§çš„å€¼ã€‚<br>Max pooling: The output is the largest value within the filter
area.\</li><li>å¹³å‡æ± åŒ–ï¼šè¾“å‡ºæ˜¯æ»¤æ³¢å™¨åŒºåŸŸå†…å€¼çš„å¹³å‡å€¼ã€‚<br>Average pooling: The output is the average of the values within the
filter area.</li></ul><p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ± åŒ–å±‚ä¸éœ€è¦è®­ç»ƒï¼<br>Note that pooling layers do not need to be trained!</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102165731248.png width=969 height=300 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102165731248_hu_9df9c6bf45bed541.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102165731248_hu_a9dbd164ffcfdb16.png 1024w" loading=lazy alt=image-20250102165731248 class=gallery-image data-flex-grow=323 data-flex-basis=775px></p><h3 id=æ·±åº¦å­¦ä¹ æ¶æ„>æ·±åº¦å­¦ä¹ æ¶æ„</h3><p>æ·±åº¦ç¥ç»ç½‘ç»œå¹¶<strong>ä¸æ˜¯</strong>ä»»æ„å±‚çš„ä»»æ„åºåˆ—ã€‚ Deep neural networks are not
arbitrary sequences of arbitrary layers.</p><p>ç›¸åï¼Œå®ƒä»¬å…·æœ‰<strong>é€‚åˆç‰¹å®šç›®æ ‡çš„é¢„å®šä¹‰æ¶æ„</strong>ã€‚ On the contrary, they have
a predefined architecture that is suitable for a specific goal.</p><p>åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå¸¸è§çš„æ¶æ„æ˜¯ï¼š In classification, it is common to see
architectures in which:</p><ul><li>å‰å‡ å±‚å®šä¹‰äº†<strong>ä¸€äº›ç®€å•çš„æ¦‚å¿µ</strong>ï¼Œæœ€åå‡ å±‚å®šä¹‰äº†è®¸å¤š<strong>å¤æ‚çš„æ¦‚å¿µ</strong>ã€‚
The first layers define a few, simple concepts, the last layers
define many, complex concepts.</li><li>éšç€ç½‘ç»œåŠ æ·±ï¼Œç‰¹å¾å›¾é€æ¸<strong>ç¼©å°</strong>ã€‚ Feature maps shrink as we move
deeper into the network.</li></ul><p>ç›¸åŒçš„ä¸­é—´æ¦‚å¿µå¯ä»¥ç”¨äºä¸åŒçš„ç›®æ ‡ã€‚ The same intermediate concepts can be
useful for different goals.</p><p>æˆ‘ä»¬å¯ä»¥ä½¿ç”¨<strong>è¿ç§»å­¦ä¹ </strong>æ¥é‡ç”¨ç°æœ‰çš„è§£å†³æ–¹æ¡ˆã€‚ We can use transfer
learning to reuse existing solutions.</p><h2 id=ç»“æ„åˆ†æ-structure-analysisweek-3-4>ç»“æ„åˆ†æ Structure analysisã€Week 3-4ã€‘</h2><h3 id=æ— ç›‘ç£å­¦ä¹ >æ— ç›‘ç£å­¦ä¹ </h3><p>æ— ç›‘ç£å­¦ä¹ ä¸ä¼šå°†ä»»ä½•å±æ€§æå‡ä¸ºæ ‡ç­¾ç±»åˆ«ï¼šæ‰€æœ‰å±æ€§éƒ½è¢«å¹³ç­‰å¯¹å¾…ã€‚<br>Unsupervised learning does not elevate any attribute to the category of
label: all the attributes are treated equally.</p><p>æ— ç›‘ç£å­¦ä¹ çš„æœ¬è´¨å¯ä»¥å½’ç»“ä¸ºä¸€ä¸ªç®€å•çš„é—®é¢˜ï¼šæˆ‘çš„æ•°æ®<strong>åœ¨å“ªé‡Œ</strong>ï¼Ÿ<br>The essence of unsupervised learning is encapsulated in the simple
question: where is my data?</p><p>å±æ€§ç©ºé—´æ˜¯æ— é™çš„ä¸”å¤§éƒ¨åˆ†æ˜¯ç©ºçš„ï¼Œè€Œå¯¹è¿™ä¸ªé—®é¢˜çš„ç­”æ¡ˆå°†æ˜¯ä¸€ä¸ª<strong>æ¨¡å‹</strong>ï¼Œè¯¥æ¨¡å‹å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿè¯†åˆ«å‡ºæˆ‘ä»¬å¯èƒ½æœŸæœ›æ‰¾åˆ°æ ·æœ¬çš„åŒºåŸŸã€‚<br>The attribute space is infinite and mostly empty, and the answer to this
question will be a model that will allow us to identify the regions
where we could expect to find samples.</p><p>æœ‰ä¸¤ç§ä¸»è¦åšæ³•ï¼š</p><ul><li><strong>å¯†åº¦ä¼°è®¡</strong>ï¼šåˆ›å»ºæ¨¡å‹ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿé‡åŒ–åœ¨å±æ€§ç©ºé—´çš„æŸä¸ªåŒºåŸŸå†…æ‰¾åˆ°æ ·æœ¬çš„æ¦‚ç‡ï¼ˆ<strong>æ¦‚ç‡å¯†åº¦</strong>ï¼‰ã€‚<br>Density estimation: Creates models that allow us to quantify the
probability of finding a sample within a region of the attribute
space (probability density).\</li><li><strong>ç»“æ„åˆ†æ</strong>ï¼šåˆ›å»ºæ¨¡å‹ï¼Œè¯†åˆ«å±æ€§ç©ºé—´å†…æ ·æœ¬å¯†åº¦è¾ƒé«˜çš„åŒºåŸŸï¼ˆ<strong>èšç±»åˆ†æ</strong>ï¼‰æˆ–æ–¹å‘ï¼ˆ<strong>æˆåˆ†åˆ†æ</strong>ï¼‰ã€‚<br>Structure analysis: Creates models that identify regions within the
attribute space (cluster analysis) or directions (component
analysis) with a high density of samples.</li></ul><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102170825203.png width=933 height=373 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102170825203_hu_ce406c1a3fb87588.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102170825203_hu_929d6eb80e1348a0.png 1024w" loading=lazy alt=image-20250102170825203 class=gallery-image data-flex-grow=250 data-flex-basis=600px></p><h3 id=æ— ç›‘ç£å­¦ä¹ çš„ç”¨å¤„>æ— ç›‘ç£å­¦ä¹ çš„ç”¨å¤„</h3><ul><li>æ— ç›‘ç£å­¦ä¹ å¯ä»¥ç”¨äºä»¥åŸå‹æ ·æœ¬çš„å½¢å¼æä¾›å¯¹populationçš„æ€»ç»“ã€‚<br>Unsupervised learning can be used to provide summaries of a
population in the form of prototype samples.</li><li>æ— ç›‘ç£å­¦ä¹ ä¹Ÿå¯ä»¥ç”¨äºå‘ç°ç»“æ„ã€‚ Unsupervised learning can also be
used to discover structure.</li><li>æ— ç›‘ç£å­¦ä¹ å¯ç”¨äºæ„å»ºç±»åˆ«å¯†åº¦ï¼Œæè¿°åœ¨æŸä¸ªåŒºåŸŸä¸­æ‰¾åˆ°æ¥è‡ªç»™å®šç±»åˆ«çš„æ ·æœ¬çš„æ¦‚ç‡ã€‚<br>Unsupervised learning can be used to build class densities that
describe the probability of finding a sample from a given class in a
region.<ul><li>æ¦‚ç‡å¯†åº¦è¿˜å¯ç”¨äºè¯†åˆ«å¼‚å¸¸ï¼Œå³å¯èƒ½å±äºä¸åŒæ€»ä½“çš„æ ·æœ¬ã€‚<br>A probability density is also useful to identify anomalies,
i.e.Â samples that are likely to belong to a different
population.</li></ul></li></ul><h3 id=èšç±»åˆ†æ>èšç±»åˆ†æ</h3><p>èšç±»æ˜¯ä¸€ç±»æ— ç›‘ç£å­¦ä¹ ç®—æ³•ï¼Œç”¨äºå°†æ•°æ®é›†çš„ç»“æ„æè¿°ä¸ºç›¸ä¼¼æ ·æœ¬çš„ç»„æˆ–ç°‡ã€‚
Clustering is a family of unsupervised learning algorithms that describe
the structure of a dataset as groups, or clusters, of similar samples.</p><h3 id=similarity-as-proximity>Similarity as proximity</h3><p>èšç±»å¯ä»¥å®šä¹‰ä¸ºå½¼æ­¤æ¥è¿‘çš„æ ·æœ¬ç»„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨<strong>æ¥è¿‘åº¦</strong>ä½œä¸ºç›¸ä¼¼æ€§çš„æ¦‚å¿µã€‚
Clusters can be defined as groups of samples that are close to one
another. In this case, we use proximity as our notion of similarity.</p><p>ä½¿ç”¨è·ç¦»ä½œä¸ºç›¸ä¼¼åº¦çš„åº¦é‡ï¼ŒåŒä¸€ç°‡ä¸­çš„æ ·æœ¬åº”è¯¥å½¼æ­¤æ¥è¿‘ï¼Œè€Œä¸åŒç°‡ä¸­çš„æ ·æœ¬åº”è¯¥ç›¸è·è¾ƒè¿œã€‚
Using distance as our notion of similarity, samples within the same
cluster should be close to one another and samples from different
clusters should be far apart.</p><p>ä¾‹ï¼šæŒ‡æ ‡ï¼Œå¹³æ–¹è·ç¦»</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102171505898.png width=864 height=444 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102171505898_hu_fc5539fd5d623cca.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102171505898_hu_243080648b02ce55.png 1024w" loading=lazy alt=image-20250102171505898 class=gallery-image data-flex-grow=194 data-flex-basis=467px></p><h3 id=è´¨é‡æŒ‡æ ‡>è´¨é‡æŒ‡æ ‡</h3><p>ä½¿ç”¨ç°‡é—´æ•£å¸ƒå’Œç°‡å†…æ•£å¸ƒã€‚</p><p>æœ€ä¼˜çš„èšç±»æ‹¥æœ‰æœ€ä½çš„ç°‡å†…æ•£å¸ƒï¼Œæœ€å¤§çš„ç°‡é—´åˆ†å¸ƒã€‚</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102171812569.png width=1087 height=678 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102171812569_hu_d9dd00e85f8c7302.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102171812569_hu_561af37804c3415a.png 1024w" loading=lazy alt=image-20250102171812569 class=gallery-image data-flex-grow=160 data-flex-basis=384px></p><p>ä½¿ç”¨èšç±»åŸå‹ï¼ˆå¦‚èšç±»ä¸­å¿ƒï¼‰æ˜¯æè¿°èšç±»çš„ä¸€ç§ç®€å•æ–¹æ³•ã€‚<br>A simple way to describe a cluster is by using cluster prototypes, such
as the centre of a cluster.</p><h3 id=k-means-èšç±»>K-means èšç±»</h3><p>ç°‡å†…æ ·æœ¬æ•£å¸ƒæŒ‡æ ‡ï¼š</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102172326414.png width=1080 height=487 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102172326414_hu_b483c43504c1e705.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102172326414_hu_b91b9ebc74f59ac3.png 1024w" loading=lazy alt=image-20250102172326414 class=gallery-image data-flex-grow=221 data-flex-basis=532px></p><p>K-means å°†æ•°æ®é›†åˆ’åˆ†ä¸º K ä¸ªç”±å…¶å‡å€¼è¡¨ç¤ºçš„ç°‡ï¼Œå¹¶æŒ‰ä»¥ä¸‹æ­¥éª¤è¿­ä»£è¿›è¡Œï¼š<br>K-means partitions a dataset into K clusters represented by their mean
and proceeds iteratively as follows:</p><ul><li>åŸå‹è¢«è·å–ä¸ºæ¯ä¸ªç°‡çš„ä¸­å¿ƒï¼ˆæˆ–å‡å€¼ï¼‰ã€‚<br>Prototypes are obtained as the centre (or mean) of each cluster.\</li><li>æ ·æœ¬è¢«é‡æ–°åˆ†é…åˆ°å…·æœ‰æœ€è¿‘åŸå‹çš„ç°‡ã€‚<br>Samples are re-assigned to the cluster with the closest prototype.</li></ul><p>éšç€ K-means
ç®—æ³•çš„è¿›è¡Œï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°æ ·æœ¬è¢«é‡æ–°åˆ†é…åˆ°ä¸åŒçš„ç°‡ï¼Œç›´åˆ°è¾¾åˆ°æŸä¸ªç¨³å®šè§£ï¼Œæ­¤æ—¶æ²¡æœ‰æ ·æœ¬è¢«é‡æ–°åˆ†é…ã€‚<br>As the K-means algorithm proceeds, we will see samples been reassigned
to different clusters until at some point we reach a stable solution,
where no sample is reassigned.</p><p>æœ€ç»ˆè§£æ˜¯<strong>å±€éƒ¨æœ€ä¼˜è§£</strong>ï¼Œä¸ä¸€å®šæ˜¯å…¨å±€æœ€ä¼˜è§£ã€‚<br>The final solution is a local optimum, not necessarily the global one.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102172541620.png width=1026 height=586 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102172541620_hu_ab9803f1ae7567ca.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102172541620_hu_7a229c2f00f0a39c.png 1024w" loading=lazy alt=image-20250102172541620 class=gallery-image data-flex-grow=175 data-flex-basis=420px></p><h3 id=è‚˜éƒ¨æ³•-elbow-method>è‚˜éƒ¨æ³• Elbow method</h3><p>ä½¿ç”¨Kmeansçš„æ—¶å€™éœ€è¦æŒ‡å®šä¸€ä¸ªè¶…å‚æ•°ç°‡æ•°é‡Kï¼Œä½†æ˜¯æœ‰äº›æ—¶å€™æˆ‘ä»¬ä¸çŸ¥é“è¿™ä¸ªKæ˜¯å¤šå°‘ã€‚</p><p>éªŒè¯ç­–ç•¥å¯ä»¥å»ºè®®ä¸€ä¸ªåˆé€‚çš„è¶…å‚æ•°Kå€¼ã€‚ Validation strategies can suggest
a suitable value for the hyperparameter K.</p><p>ç„¶è€Œï¼Œé€‰æ‹©äº§ç”Ÿæœ€ä½I(C0)çš„Kå€¼å¹¶ä¸å¥æ•ˆï¼Œå› ä¸ºéšç€èšç±»æ•°é‡çš„å¢åŠ ï¼ŒI(C0)æ€»æ˜¯ä¼šé™ä½ã€‚
However, choosing the value of K producing the lowest I(C0) would not
work, as I(C0) always decreases as the number of clusters increase.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102172823205.png width=1050 height=565 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102172823205_hu_16b3147ae6157d0e.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102172823205_hu_2311875aeae75e0c.png 1024w" loading=lazy alt=image-20250102172823205 class=gallery-image data-flex-grow=185 data-flex-basis=446px></p><p>å‡è®¾çœŸå®çš„èšç±»æ•°é‡ä¸º KTã€‚å¯¹äº K > KTï¼Œæˆ‘ä»¬åº”é¢„æœŸè´¨é‡çš„æå‡é€Ÿåº¦ä¼šæ¯” K &lt;
KT æ—¶æ›´æ…¢ï¼Œå› ä¸ºæˆ‘ä»¬å°†æ‹†åˆ†çœŸå®çš„èšç±»ã€‚ Assume the true number of clusters
is KT. For K > KT, we should expect the increase in quality to be
slower than for K &lt; KT, as we will be splitting true clusters.</p><p>çœŸå®çš„èšç±»æ•°é‡å¯ä»¥é€šè¿‡è§‚å¯Ÿ K å€¼è¶…è¿‡æŸä¸ªç‚¹åè´¨é‡æå‡é€Ÿåº¦å‡ç¼“æ¥è¯†åˆ«ã€‚ The
true number of clusters can be identified by observing the value of K
beyond which the improvement slows down.</p><h3 id=éå‡¸èšç±»-non-convex-clusters>éå‡¸èšç±» Non-convex clusters</h3><p>Kmeansäº§ç”Ÿçš„ç°‡æ˜¯åœ†å½¢çš„ï¼Œåœ¨éå‡¸æ ·æœ¬ä¸Šçš„æ•ˆæœä¸å¥½ã€‚</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102173015848.png width=1057 height=591 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102173015848_hu_b53f606d5a836a26.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102173015848_hu_a9ef79ed5f0e43a6.png 1024w" loading=lazy alt=image-20250102173015848 class=gallery-image data-flex-grow=178 data-flex-basis=429px></p><p>åœ¨<strong>éå‡¸</strong>èšç±»ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»ä¸€ä¸ªæ ·æœ¬åˆ°å¦ä¸€ä¸ªæ ·æœ¬çš„å°è·³è·ƒåˆ°è¾¾ä»»ä½•æ ·æœ¬ã€‚<br>In non-convex clustering, we can reach any sample by taking small jumps
from sample to sample.</p><p>éå‡¸åœºæ™¯æå‡ºäº†ä¸€ç§ä¸åŒçš„èšç±»æ¦‚å¿µï¼Œå³æ ·æœ¬æ˜¯<strong>è¿æ¥</strong>çš„ï¼Œè€Œä¸ä»…ä»…æ˜¯æ¥è¿‘çš„ï¼šå¦‚æœæˆ‘ä¸ä½ ç›¸ä¼¼ï¼Œä½ ä¸ä»–ä»¬ç›¸ä¼¼ï¼Œé‚£ä¹ˆæˆ‘ä¹Ÿä¸ä»–ä»¬ç›¸ä¼¼ã€‚<br>Non-convex scenarios suggest a different notion of cluster as a group of
samples that are connected, rather than simply close: if I am similar to
you, and you are similar to them, I am similar to them too.</p><p>è¿™ç§å°†èšç±»è§†ä¸ºä¸€ç»„<strong>è¿æ¥</strong>æ ·æœ¬çš„æ¦‚å¿µæ˜¯è®¸å¤šèšç±»ç®—æ³•çš„åŸºç¡€ï¼Œä¾‹å¦‚DBSCANï¼ˆåŸºäºå¯†åº¦çš„å™ªå£°åº”ç”¨ç©ºé—´èšç±»ï¼‰ã€‚<br>This notion of cluster as a group of connected samples is behind many
clustering algorithms, such as DBSCAN (density-based spatial clustering
of applications with noise).</p><h3 id=dbscan>DBSCAN</h3><p>DBSCANå±äºåŸºäºå¯†åº¦çš„ç®—æ³•å®¶æ—ï¼Œå…¶ä¸­ä½¿ç”¨æ¯ä¸ªæ ·æœ¬å‘¨å›´æ ·æœ¬å¯†åº¦çš„ä¼°è®¡å°†æ•°æ®é›†åˆ’åˆ†ä¸ºèšç±»ã€‚DBSCAN
belongs to the family of density-based algorithms, where an estimation
of the density of samples around each sample is used to partition the
dataset into clusters.</p><p>DBSCANå®šä¹‰äº†ä¸¤ä¸ªé‡ï¼ŒåŠå¾„rå’Œé˜ˆå€¼tã€‚é¦–å…ˆè®¡ç®—æ¯ä¸ªæ ·æœ¬å‘¨å›´åŠå¾„rçš„é‚»åŸŸå†…çš„æ ·æœ¬æ•°é‡ï¼ˆä¸åŒ…æ‹¬è‡ªèº«ï¼‰ä½œä¸ºå¯†åº¦ã€‚ç„¶åï¼Œè¯†åˆ«å‡ºä¸‰ç§ç±»å‹çš„æ ·æœ¬ï¼š
DBSCAN defines two quantities, a radius r and a threshold t. A density
is first calculated as the number of samples in a neighbourhood of
radius r around each sample (excluding itself). Then, three types of
samples are identified:</p><ul><li>æ ¸å¿ƒï¼šå…¶å¯†åº¦ç­‰äºæˆ–é«˜äºé˜ˆå€¼tã€‚ Core: its density is equal or higher
than the threshold t.</li><li>è¾¹ç•Œï¼šå…¶å¯†åº¦ä½äºé˜ˆå€¼tï¼Œä½†å…¶é‚»åŸŸå†…åŒ…å«ä¸€ä¸ªæ ¸å¿ƒæ ·æœ¬ã€‚ Border: its
density is lower than the threshold t, but contains a core sample
within its neighbourhood.</li><li>ç¦»ç¾¤å€¼ï¼šä»»ä½•å…¶ä»–æ ·æœ¬ã€‚ Outlier: Any other sample.</li></ul><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102173338431.png width=739 height=379 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102173338431_hu_664f5176afd93b7d.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102173338431_hu_e853c58f49b27169.png 1024w" loading=lazy alt=image-20250102173338431 class=gallery-image data-flex-grow=194 data-flex-basis=467px></p><p>DBSCANç®—æ³•æµç¨‹å¦‚ä¸‹ï¼š<br>The DBSCAN algorithm proceeds as follows:</p><ul><li>è¯†åˆ«æ ¸å¿ƒç‚¹ã€è¾¹ç•Œç‚¹å’Œç¦»ç¾¤ç‚¹ã€‚<br>Identify core, border, and outlier samples.\</li><li>å½¼æ­¤åœ¨é‚»åŸŸå†…çš„æ ¸å¿ƒç‚¹å¯¹ä¼šè¢«è¿æ¥ã€‚è¿æ¥çš„æ ¸å¿ƒç‚¹å½¢æˆç°‡çš„ä¸»å¹²ã€‚<br>Pairs of core samples that are within each other&rsquo;s neighbourhood are
connected. Connected core samples form the backbone of a cluster.\</li><li>è¾¹ç•Œç‚¹è¢«åˆ†é…åˆ°å…¶é‚»åŸŸå†…æ ¸å¿ƒç‚¹æ•°é‡æœ€å¤šçš„ç°‡ã€‚<br>Border samples are assigned to the cluster that has more core
samples in the neighbourhood of the border sample.\</li><li>ç¦»ç¾¤ç‚¹ä¸åˆ†é…åˆ°ä»»ä½•ç°‡ã€‚<br>Outlier samples are not assigned to any cluster.</li></ul><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102173433241.png width=1129 height=505 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102173433241_hu_dcdc29c251385c13.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102173433241_hu_966d10f85d53c792.png 1024w" loading=lazy alt=image-20250102173433241 class=gallery-image data-flex-grow=223 data-flex-basis=536px></p><h3 id=å±‚æ¬¡èšç±»-hierarchical-clustering>å±‚æ¬¡èšç±» Hierarchical clustering</h3><p>ç»™å®šä¸€ä¸ªåŒ…å«Nä¸ªæ ·æœ¬çš„æ•°æ®é›†ï¼Œå­˜åœ¨ä¸¤ç§å¹³å‡¡çš„èšç±»è§£å†³æ–¹æ¡ˆï¼šä¸€ä¸ªåŒ…å«æ‰€æœ‰æ ·æœ¬çš„å•ä¸€ç°‡ï¼Œä»¥åŠæ¯ä¸ªæ ·æœ¬è‡ªæˆä¸€ä¸ªç°‡çš„è§£å†³æ–¹æ¡ˆã€‚
Given a dataset consisting of N samples, there exist two trivial
clustering solutions: one single cluster that includes all the samples,
and the solution where each sample is a cluster on its own.</p><p>K-meansç”ŸæˆKä¸ªç°‡ï¼Œä½†æˆ‘ä»¬éœ€è¦åœ¨1 â‰¤ K â‰¤ Nä¹‹é—´é€‰æ‹©Kã€‚ K-means produces K
clusters, but we need to choose K within 1 â‰¤ K â‰¤ N.</p><p>åœ¨DBSCANä¸­ï¼Œç°‡æ˜¯è‡ªåŠ¨å‘ç°çš„ï¼Œä½†æœ€ç»ˆçš„ç°‡æ•°é‡å–å†³äºåŠå¾„rå’Œé˜ˆå€¼tçš„å€¼ã€‚ In
DBSCAN clusters are discovered automatically, but the final number of
clusters depends on the values of the radius r and the threshold value
t.</p><p>è¿™ç§æ¨¡ç³Šæ€§æœ€ç»ˆæ­ç¤ºäº†<strong>æ•°æ®é›†çš„ç»“æ„å¯ä»¥åœ¨ä¸åŒå±‚æ¬¡ä¸Šæ¢ç´¢</strong>ï¼Œä»è€Œæ­ç¤ºä¸åŒçš„ç‰¹æ€§ã€‚
This ambiguity ultimately reveals that the structure of a dataset can be
explored at different levels that expose different properties.</p><p><strong>å±‚æ¬¡èšç±»æ˜¯ä¸€ç³»åˆ—é€šè¿‡é€æ­¥æ„å»ºã€ä¸åŒå±‚æ¬¡ã€‘èšç±»å®‰æ’çš„èšç±»æ–¹æ³•ã€‚</strong><br>Hierarchical clustering is a family of clustering approaches that
proceed by progressively building clustering arrangements at different
levels.</p><p>ç”Ÿæˆçš„èšç±»å®‰æ’é›†åˆæ˜¯å±‚æ¬¡åŒ–çš„ï¼Œå› ä¸ºä¸€ä¸ªå±‚æ¬¡ä¸­çš„èšç±»åŒ…å«æ¥è‡ªä¸‹ä¸€å±‚æ¬¡ä¸­ä¸€ä¸ªæˆ–å¤šä¸ªèšç±»çš„æ‰€æœ‰æ ·æœ¬ã€‚<br>The resulting collection of clustering arrangements is hierarchical in
the sense that a cluster in one level contains all the samples from one
or more clusters in the level below.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102173752383.png width=1024 height=459 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102173752383_hu_1235df835df47e71.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102173752383_hu_2113acff30ff041d.png 1024w" loading=lazy alt=image-20250102173752383 class=gallery-image data-flex-grow=223 data-flex-basis=535px></p><p>èšç±»åœ¨ä¸åŒå±‚æ¬¡ä¹‹é—´å…³ç³»çš„è¡¨ç¤ºç§°ä¸ºæ ‘çŠ¶å›¾ã€‚ The representation of the
relationship between clusters at different levels is called a
dendrogram.</p><ul><li>åœ¨åº•éƒ¨ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½æ˜¯ä¸€ä¸ªå•ç‹¬çš„èšç±»ã€‚ At the bottom, each sample is
one cluster.</li><li>åœ¨é¡¶éƒ¨ï¼Œæ•´ä¸ªæ•°æ®é›†å½¢æˆä¸€ä¸ªèšç±»ã€‚ At the top, the whole dataset forms
one cluster.</li></ul><p>æ„å»ºæ ‘çŠ¶å›¾æœ‰ä¸¤ç§åŸºæœ¬ç­–ç•¥ï¼š There exist two basic strategies to build a
dendrogram:</p><ul><li><strong>åˆ†è£‚æ³•</strong>æˆ–è‡ªä¸Šè€Œä¸‹æ³•ä»æ ‘çŠ¶å›¾çš„é¡¶éƒ¨å¼€å§‹åˆ†å‰²èšç±»ï¼Œç›´åˆ°åº•éƒ¨å±‚æ¬¡ã€‚ The
divisive or top-down approach splits clusters starting from the top
of the dendrogram and stops at the bottom level.</li><li><strong>å‡èšæ³•</strong>æˆ–è‡ªä¸‹è€Œä¸Šæ³•ä»åº•éƒ¨å¼€å§‹åˆå¹¶ä¸¤ä¸ªèšç±»ï¼Œç›´åˆ°è¾¾åˆ°é¡¶éƒ¨å±‚æ¬¡ã€‚ The
agglomerative or bottom-up approach merges two clusters, starting
from the bottom until we reach the top level.</li></ul><p>å†³å®šåº”å½“æ€æ ·åˆ‡åˆ†/åˆå¹¶èšç±»çš„é€‰é¡¹ï¼š</p><p><strong>å•é“¾æ³•</strong>ï¼šä½¿ç”¨ä¸¤ä¸ªç°‡ä¸­æœ€è¿‘çš„ä¸¤ä¸ªæ ·æœ¬ä¹‹é—´çš„è·ç¦»ã€‚è¿™ç§æ–¹æ³•ä¼šäº§ç”Ÿä»»æ„å½¢çŠ¶çš„ç°‡ã€‚<br>Single linkage: uses the distance between the two closest samples from
two clusters. This option results in clusters of arbitrary shapes.</p><p><strong>å…¨é“¾æ³•</strong>ï¼šä½¿ç”¨ä¸¤ä¸ªç°‡ä¸­æœ€è¿œçš„ä¸¤ä¸ªæ ·æœ¬ä¹‹é—´çš„è·ç¦»ã€‚è¿™ç§æ–¹æ³•å€¾å‘äºäº§ç”Ÿçƒå½¢ç°‡ã€‚<br>Complete linkage: uses the distance between the two further samples from
each pair of clusters. This choice produces clusters that tend to have a
spherical shape.</p><p><strong>ç»„å¹³å‡æ³•</strong>ï¼šä½¿ç”¨ä¸¤ä¸ªç°‡ä¸­æ ·æœ¬ä¹‹é—´çš„å¹³å‡è·ç¦»ã€‚è¿™ç§æ–¹æ³•ä¹Ÿä¼šäº§ç”Ÿçƒå½¢ç°‡ï¼Œä½†å¯¹å¼‚å¸¸å€¼æ›´å…·é²æ£’æ€§ã€‚<br>Group average: uses the average distance between samples in two clusters
and also produces spherical shapes, although they are more robust to
outliers.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102174015128.png width=1062 height=741 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102174015128_hu_f12f57ccab09371b.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102174015128_hu_dcb6c755e5a8ca32.png 1024w" loading=lazy alt=image-20250102174015128 class=gallery-image data-flex-grow=143 data-flex-basis=343px></p><h3 id=ç»„åˆ†åˆ†æ-component-analysis>ç»„åˆ†åˆ†æ component analysis</h3><p>ç»„ä»¶åˆ†æä½¿æˆ‘ä»¬èƒ½å¤Ÿè¯†åˆ«æ•°æ®åœ¨ç©ºé—´ä¸­å¯¹é½çš„æ–¹å‘ã€‚ Component analysis allows
us to identify the directions in the space that our data are aligned
with.</p><p>è¿™å¯¹äºè½¬æ¢æ•°æ®é›†ã€æ¸…ç†æ•°æ®å’Œé™ä½å…¶ç»´åº¦éå¸¸æœ‰ç”¨ã€‚ This can be useful to
transform our dataset, clean it and reduce its dimensionality.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102174058244.png width=1053 height=414 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102174058244_hu_485a17c4a6d54da7.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102174058244_hu_b7827f1e9c5254f3.png 1024w" loading=lazy alt=image-20250102174058244 class=gallery-image data-flex-grow=254 data-flex-basis=610px></p><h2 id=å¯†åº¦ä¼°è®¡-density-estimation-week-4-2>å¯†åº¦ä¼°è®¡ Density estimation ã€week 4-2ã€‘</h2><h3 id=æ•°æ®åˆ†å¸ƒ>æ•°æ®åˆ†å¸ƒ</h3><p>æ•°æ®é›†æ˜¯åˆ†å¸ƒåœ¨å±æ€§ç©ºé—´å†…çš„æ ·æœ¬é›†åˆï¼Œä½†å¹¶æ²¡æœ‰å æ®æ•´ä¸ªç©ºé—´ã€‚</p><ul><li><p>æˆ‘çš„å¤§å¤šæ•°æ ·æœ¬åœ¨å“ªé‡Œï¼Ÿ Where are most of my samples?</p></li><li><p>æˆ‘åº”è¯¥åœ¨è¿™ä¸ªå±æ€§ç©ºé—´çš„åŒºåŸŸä¸­æœŸå¾…ä¸€ä¸ªæ ·æœ¬å—ï¼Ÿ Should I expect a
sample in this region of the attribute space?</p></li><li><p>åœ¨è¿™ä¸ªåŒºåŸŸæ‰¾åˆ°æ ·æœ¬çš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Ÿ What is the probability that I will
find a sample in this region?</p></li><li><p>ç»™å®šä¸€ä¸ªæ¦‚ç‡ï¼Œæˆ‘å°†åœ¨å“ªé‡Œæ‰¾åˆ°ä¸‹ä¸€ä¸ªæ ·æœ¬ï¼Ÿ Given a probability, where
will I find my next sample?</p></li></ul><h3 id=åˆ‡åˆ†å¹¶è®¡æ•°>åˆ‡åˆ†å¹¶è®¡æ•°</h3><p>å°†ç©ºé—´åˆ’åˆ†ä¸ºè¾ƒå°çš„åŒºåŸŸå¹¶<strong>è®¡ç®—</strong>æ¯ä¸ªåŒºåŸŸå†…çš„æ ·æœ¬æ•°é‡ï¼Œæ˜¯æè¿°<strong>è§‚å¯Ÿåˆ°çš„åˆ†å¸ƒ</strong>çš„ä¸€ç§ç®€å•æ–¹æ³•ã€‚<br>Partitioning the space into smaller regions and counting the number of
samples within each region is a simple way of describing the observed
distribution.</p><p>è¿™ä¹Ÿä¸ºæˆ‘ä»¬æä¾›äº†ä»åŒä¸€æ€»ä½“ä¸­æå–æ›´å¤šæ ·æœ¬æ—¶é¢„æœŸç»“æœçš„æŒ‡ç¤ºï¼Œå³<strong>çœŸå®åˆ†å¸ƒ</strong>ã€‚<br>It also gives an indication of what to expect if we extract more samples
from the same population, i.e.Â the true distribution.</p><p>è®¡æ•°æœ¬èº«ä¸ä¼šæä¾›æœ‰ç”¨çš„<strong>å®šé‡</strong>ç­”æ¡ˆï¼Œä½†æˆ‘ä»¬å¯ä»¥å°†å…¶è½¬æ¢ä¸º<strong>æ¯”ç‡</strong>ã€‚<br>Counts will not give useful quantitative answers, but we can transform
them into rates.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102174824199.png width=1036 height=397 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102174824199_hu_bed07f0919573187.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102174824199_hu_a1c784f7e3730b9b.png 1024w" loading=lazy alt=image-20250102174824199 class=gallery-image data-flex-grow=260 data-flex-basis=626px></p><h3 id=æ¦‚ç‡å¯†åº¦>æ¦‚ç‡å¯†åº¦</h3><p>æ¦‚ç‡å¯†åº¦æ˜¯æè¿°æˆ‘ä»¬æ•°æ®çœŸå®åˆ†å¸ƒçš„æ¨¡å‹ã€‚<br>Probability densities are models that describe the underlying true
distribution of our data.</p><h3 id=å¯†åº¦ä¼°è®¡>å¯†åº¦ä¼°è®¡</h3><p>åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨<strong>æ•°æ®</strong>æ¥æ„å»ºæ¦‚ç‡å¯†åº¦ï¼Œè¿™ä¸ªä»»åŠ¡ç§°ä¸º<strong>å¯†åº¦ä¼°è®¡</strong>ã€‚<br>In machine learning, we use data to build probability densities, and
this task is called density estimation.</p><p>æˆ‘ä»¬å¯ä»¥æ„å»ºè€ƒè™‘æ‰€æœ‰å±æ€§çš„æ¦‚ç‡å¯†åº¦ï¼Œä¹Ÿå¯ä»¥æ„å»ºè€ƒè™‘éƒ¨åˆ†å±æ€§çš„æ¦‚ç‡å¯†åº¦ï¼Œåè€…ç§°ä¸ºè¾¹ç¼˜æ¦‚ç‡å¯†åº¦ã€‚<br>We can build a probability density that considers all the attributes, or
probability densities that consider a subset of the attributes, which we
call marginal probability densities.</p><p>åœ¨å…·æœ‰ä¸¤ä¸ªå±æ€§ $x_1$ å’Œ $x_2$ çš„æ•°æ®é›†ä¸­ï¼Œæ¦‚ç‡å¯†åº¦çš„ç®€åŒ–æ•°å­¦è¡¨ç¤ºå¦‚ä¸‹ï¼š<br>In a dataset with two attributes $x_1$ and $x_2$, the simplified
mathematical notation for the probability densities is as follows:</p><ul><li><p>æ¦‚ç‡å¯†åº¦è¡¨ç¤ºä¸º $p(x_1, x_2)$ï¼Œæˆ–è€…ä½¿ç”¨å‘é‡è¡¨ç¤ºæ³• $p(\mathbf{x})$ã€‚<br>The probability density is denoted by $p(x_1, x_2)$ or, using vector
notation, $p(\mathbf{x})$.</p></li><li><p>è¾¹ç¼˜æ¦‚ç‡å¯†åº¦è¡¨ç¤ºä¸º $p(x_1)$ å’Œ $p(x_2)$ã€‚<br>The marginal probability densities are denoted $p(x_1)$ and
$p(x_2)$.</p></li></ul><h3 id=éå‚æ•°æ–¹æ³•çš„å¯†åº¦ä¼°è®¡>éå‚æ•°æ–¹æ³•çš„å¯†åº¦ä¼°è®¡</h3><p>éå‚æ•°æ–¹æ³•ä¸æŒ‡å®šæ¦‚ç‡å¯†åº¦çš„å½¢çŠ¶ã€‚<br>Non-parametric methods do not specify the shape of the probability
density.</p><h4 id=ç›´æ–¹å›¾-histogram>ç›´æ–¹å›¾ histogram</h4><p><strong>ç›´æ–¹å›¾</strong>æ˜¯æœ€ç®€å•ä¸”æœ€è‘—åçš„éå‚æ•°å¯†åº¦ä¼°è®¡æ–¹æ³•ã€‚<br>The <strong>histogram</strong> is the simplest and best known non-parametric method
for density estimation.</p><p>ç›´æ–¹å›¾é€šè¿‡å°†ç‰¹å¾ç©ºé—´åˆ’åˆ†ä¸ºå¤§å°ç›¸ç­‰çš„åŒºåŸŸï¼ˆç§°ä¸º"<strong>ç®±</strong>"ï¼‰æ¥æ„å»ºã€‚<br>A histogram is built by dividing the feature space into equal-sized
regions called <strong>bins</strong>.</p><p>å¯†åº¦é€šè¿‡è½åœ¨æ¯ä¸ªç®±ä¸­çš„æ ·æœ¬æ¯”ä¾‹æ¥è¿‘ä¼¼ä¼°è®¡ã€‚<br>The density is approximated by the fraction of samples that fall within
each bin.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102175333719.png width=1066 height=765 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102175333719_hu_17052327e84b7ad3.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102175333719_hu_3eca12980429b7a6.png 1024w" loading=lazy alt=image-20250102175333719 class=gallery-image data-flex-grow=139 data-flex-basis=334px></p><h4 id=æ ¸æ–¹æ³•-kernel-method>æ ¸æ–¹æ³• Kernel method</h4><p>Kernel methods proceed by building an individual density around each
sample first and then combining all the densities together. Individual
densities have the same shape (the kernel), for instance a Gaussian.
æ ¸æ–¹æ³•é¦–å…ˆåœ¨æ¯ä¸ªæ ·æœ¬å‘¨å›´æ„å»ºä¸€ä¸ª<strong>å•ç‹¬çš„å¯†åº¦</strong>ï¼Œç„¶åå°†æ‰€æœ‰å¯†åº¦<strong>ç»“åˆåœ¨ä¸€èµ·</strong>ã€‚æ¯ä¸ªå¯†åº¦å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ï¼ˆå³<strong>æ ¸</strong>ï¼‰ï¼Œä¾‹å¦‚é«˜æ–¯åˆ†å¸ƒã€‚</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102175446295.png width=1093 height=405 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102175446295_hu_4cd82e4a4ceb9d69.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102175446295_hu_514b4e0719e07c16.png 1024w" loading=lazy alt=image-20250102175446295 class=gallery-image data-flex-grow=269 data-flex-basis=647px></p><p>éå‚æ•°æ–¹æ³•æ€»ç»“ï¼š</p><ul><li><p>éå‚æ•°å¯†åº¦ä¼°è®¡æ–¹æ³•ä¸å‡è®¾æ¦‚ç‡å¯†åº¦çš„å…·ä½“å½¢çŠ¶ã€‚<br>Non-parametric methods for density estimation do not assume any
specific shape for the probability density.\</p></li><li><p>å®ƒä»¬åŒ…å«éœ€è¦æŒ‡å®šçš„è¶…å‚æ•°ï¼Œä¾‹å¦‚ç®±ä½“å¤§å°æˆ–æ ¸ç±»å‹ã€‚<br>They contain hyperparameters that need to be specified, such as the
bin size or the type of kernel.</p></li><li><p>ç›´æ–¹å›¾æä¾›ç¦»æ•£çš„æ¦‚ç‡å¯†åº¦ï¼Œè€Œæ ¸æ–¹æ³•ç”Ÿæˆå¹³æ»‘çš„æ¦‚ç‡å¯†åº¦ã€‚<br>The histogram provides a discrete probability density, whereas
kernel methods produce a smooth one.\</p></li><li><p>æ³¨æ„ç»´åº¦ç¾éš¾ï¼šç›´æ–¹å›¾å¯èƒ½æœ€ç»ˆæ¯ä¸ªç®±ä½“ä¸­åªæœ‰ä¸€ä¸ªæ ·æœ¬ï¼›æ ¸æ–¹æ³•ä¸­çš„ä¸ªä½“åˆ†å¸ƒå¯èƒ½æœ€ç»ˆå½¼æ­¤å­¤ç«‹ã€‚<br>Beware of the curse of dimensionality: histograms can end up having
one sample per bin; individual distributions in kernel methods might
end up being isolated from one another.</p></li></ul><h3 id=å‚æ•°æ–¹æ³•çš„å¯†åº¦ä¼°è®¡>å‚æ•°æ–¹æ³•çš„å¯†åº¦ä¼°è®¡</h3><p>å‚æ•°åŒ–æ–¹æ³•æŒ‡å®šäº†æ¦‚ç‡å¯†åº¦çš„å½¢çŠ¶ã€‚ Parametric approaches specify the shape
of the probability density.</p><p>å¯†åº¦ä¼°è®¡çš„é—®é¢˜åœ¨äºä¼°è®¡å…¶å‚æ•°ã€‚ The problem of density estimation
consists of estimating its parameters.</p><p>æœ‰è®¸å¤šå¯ç”¨çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼š There are many available models, including:</p><ul><li><p>é«˜æ–¯åˆ†å¸ƒï¼Œé€šå¸¸è¡¨ç¤ºä¸º N (Î¼, Î£)ã€‚ The Gaussian distribution, usually
denoted by N (Î¼, Î£).</p></li><li><p>å¯¹æ•°æ­£æ€åˆ†å¸ƒã€‚ Log-normal distribution.</p></li><li><p>å‡åŒ€åˆ†å¸ƒã€‚ Uniform distribution.</p></li><li><p>ä¼½é©¬åˆ†å¸ƒã€‚ Gamma distribution.</p></li></ul><h4 id=é«˜æ–¯åˆ†å¸ƒ>é«˜æ–¯åˆ†å¸ƒ</h4><p>é«˜æ–¯åˆ†å¸ƒæˆ–æ­£æ€åˆ†å¸ƒ N (Î¼, Ïƒ) ç”±ä¸¤ä¸ªå‚æ•° Î¼ å’Œ Ïƒ
å®šä¹‰ï¼Œåˆ†åˆ«æè¿°å…¶ä½ç½®å’Œå®½åº¦ï¼ˆå‡å€¼å’Œæ ‡å‡†å·®ï¼‰ã€‚<br>The Gaussian or normal distribution N (Î¼, Ïƒ) is defined by two
parameters Î¼ and Ïƒ describing its location and width.</p><p>$p(x_1)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x_1-\mu}{\sigma}\right)^2}$</p><h4 id=ä¸­å¿ƒæé™å®šç†-central-limit-theorem-clt>ä¸­å¿ƒæé™å®šç† Central Limit Theorem, CLT</h4><p>ä¸­å¤®æé™å®šç†ï¼ˆCLTï¼‰æŒ‡å‡ºï¼Œå¤§é‡ç‹¬ç«‹éšæœºé‡çš„å’Œæœä»é«˜æ–¯åˆ†å¸ƒã€‚<br>The Central Limit Theorem (CLT) states that the sum of a large number of
independent, random quantities has a Gaussian distribution.</p><p>å¦‚æœä½ çš„æ•°æ®æ˜¯é«˜æ–¯åˆ†å¸ƒçš„ï¼Œé‚£ä¹ˆå°±æ²¡æœ‰ä»€ä¹ˆå¯å‘ç°çš„äº†ã€‚<br>If your data is Gaussian, there is little to discover.</p><h4 id=å¤šå…ƒé«˜æ–¯åˆ†å¸ƒ>å¤šå…ƒé«˜æ–¯åˆ†å¸ƒ</h4><p>é«˜æ–¯åˆ†å¸ƒå¯ä»¥æ‰©å±•åˆ°äºŒç»´ã€ä¸‰ç»´&mldr;å±æ€§ç©ºé—´ï¼š The Gaussian distribution can
be extended to 2D, 3D&mldr; attribute spaces:</p><p>$p(\boldsymbol{x})=\frac{1}{\sqrt{(2\pi)^k|\boldsymbol{\Sigma}|}}e^{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})}$</p><p>$\boldsymbol x=[x_1,\ldots,x_P]^{T^{\prime}}$ åŒ…å«æ‰€æœ‰å±æ€§ã€‚<br>$\boldsymbol x=[x_1,\ldots,x_P]^{T^{\prime}}$ contains all the
attributes.</p><p>å‡å€¼ $\boldsymbol\mu$ å’Œåæ–¹å·®çŸ©é˜µ $\Sigma$ æè¿°äº†åˆ†å¸ƒçš„ä½ç½®å’Œå½¢çŠ¶ã€‚<br>The mean $\boldsymbol\mu$ and covariance matrix $\Sigma$ describe the
position and shape of the distribution.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102180043162.png width=1108 height=462 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102180043162_hu_a28c6da1118c9eea.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102180043162_hu_92da57cb5941d7e9.png 1024w" loading=lazy alt=image-20250102180043162 class=gallery-image data-flex-grow=239 data-flex-basis=575px></p><p>ç»™å®šä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒ p(x1, x2)ï¼Œä»¥ä¸‹é™ˆè¿°æ˜¯ç­‰ä»·çš„ï¼š Given a Gaussian
distribution p(x1, x2), the following statements are equivalent:</p><ul><li>å±æ€§ x1 å’Œ x2 æ˜¯<strong>ç‹¬ç«‹</strong>çš„ï¼ˆä¾‹å¦‚ï¼Œæˆ‘ä»¬æ— æ³•æ ¹æ®ä¸€ä¸ªé¢„æµ‹å¦ä¸€ä¸ªçš„å€¼ï¼‰ã€‚
Attributes x1 and x2 are independent (e.g., we cannot predict the
value of one based on the other).</li><li>åæ–¹å·®çŸ©é˜µ Î£ æ˜¯<strong>å¯¹è§’</strong>çŸ©é˜µã€‚ The covariance matrix Î£ is diagonal.</li><li>p(x1, x2) å¯ä»¥è¡¨ç¤ºä¸º<strong>è¾¹é™…å¯†åº¦ p(x1) å’Œ p(x2)
çš„ä¹˜ç§¯</strong>ï¼Œè€Œå®ƒä»¬æœ¬èº«ä¹Ÿæ˜¯é«˜æ–¯åˆ†å¸ƒã€‚ p(x1, x2) can be obtained as the
product of the marginal densities p(x1) and p(x2), which are
themselves Gaussian.</li></ul><p>è¿™ä¸€æ€§è´¨å¯ä»¥æ‰©å±•åˆ°æ›´é«˜ç»´çš„å±æ€§ç©ºé—´ã€‚</p><h3 id=é«˜æ–¯åˆ†å¸ƒä¼°è®¡>é«˜æ–¯åˆ†å¸ƒä¼°è®¡</h3><p>ç»™å®šä¸€ä¸ªåŒ…å«Nä¸ªæ ·æœ¬xiçš„æ•°æ®é›†ï¼Œå¯ä»¥ä½¿ç”¨<strong>æœ€å¤§ä¼¼ç„¶</strong>æ–¹æ³•æ¥ä¼°è®¡é«˜æ–¯åˆ†å¸ƒçš„å‚æ•°ã€‚<br>Given a dataset consisting of N samples xi, the parameters of a Gaussian
distribution can be estimated using maximum likelihood approaches.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102180521563.png width=1020 height=370 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102180521563_hu_6ac9b6bb3f27e980.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102180521563_hu_dabad478d82f9fad.png 1024w" loading=lazy alt=image-20250102180521563 class=gallery-image data-flex-grow=275 data-flex-basis=661px></p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102180535031.png width=1105 height=403 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102180535031_hu_39e2c2378d6fa618.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102180535031_hu_a3d5425b6a2fc3a0.png 1024w" loading=lazy alt=image-20250102180535031 class=gallery-image data-flex-grow=274 data-flex-basis=658px></p><p>é«˜ç»´åº¦æ€»æ˜¯å­˜åœ¨é—®é¢˜å¹¶å¯¼è‡´è¿‡æ‹Ÿåˆã€‚<br>High dimensionality is always problematic and leads to overfitting.</p><p>å¯¹åæ–¹å·®çŸ©é˜µçš„çº¦æŸå’Œæ­£åˆ™åŒ–æŠ€æœ¯å¯ç”¨äºç¨³å®šè§£ã€‚<br>Constraints on the covariance matrix and regularisation techniques can
be used to stabilise the solution.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102180622388.png width=1090 height=483 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102180622388_hu_dad0b90b52fbe3f0.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102180622388_hu_4ae8efd9d14cf949.png 1024w" loading=lazy alt=image-20250102180622388 class=gallery-image data-flex-grow=225 data-flex-basis=541px></p><h3 id=æ··åˆæ¨¡å‹-mixture-models>æ··åˆæ¨¡å‹ Mixture models</h3><p>æ•°æ®é›†å¯èƒ½è¡¨ç°å‡ºå¤šä¸ªæ¨¡å¼ï¼ˆç°‡ï¼‰ï¼Œè¿™ç§æƒ…å†µä¸‹å•ä¸€çš„é«˜æ–¯å¯†åº¦å‡½æ•°å¹¶ä¸é€‚ç”¨ã€‚
Datasets can exhibit more than one mode (clumps) for which single
Gaussian densities are not suitable.</p><p>åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ··åˆå¯†åº¦æ¨¡å‹å¦‚é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰æ˜¯ä¸€ä¸ªæ–¹ä¾¿çš„é€‰æ‹©ã€‚ In
such cases, mixture densities such as Gaussian Mixture Models (GMM)
constitute a convenient choice.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102180701508.png width=1065 height=283 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102180701508_hu_74120181462d92f1.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102180701508_hu_1333191101231ad6.png 1024w" loading=lazy alt=image-20250102180701508 class=gallery-image data-flex-grow=376 data-flex-basis=903px></p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102180719562.png width=906 height=426 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102180719562_hu_1885906d40d5514b.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102180719562_hu_4c5af90da431d9ec.png 1024w" loading=lazy alt=image-20250102180719562 class=gallery-image data-flex-grow=212 data-flex-basis=510px></p><p>æœŸæœ›æœ€å¤§åŒ–ï¼ˆEMï¼‰ç®—æ³•æ˜¯ä¸€ç§è¿­ä»£è¿‡ç¨‹ï¼Œç”¨äºå°†é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰æ‹Ÿåˆåˆ°æ•°æ®é›†ï¼Œç±»ä¼¼äºKå‡å€¼ç®—æ³•ã€‚
The Expectation-Maximization (EM) algorithm is an iterative process to
fit a Gaussian Mixture Model (GMM) to a dataset, similar to the K-means
algorithm.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102180741245.png width=999 height=576 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102180741245_hu_f3babf3a1cae4a5d.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102180741245_hu_1d815b3eb2472724.png 1024w" loading=lazy alt=image-20250102180741245 class=gallery-image data-flex-grow=173 data-flex-basis=416px></p><h3 id=å™ªå£°ä¸ç¦»ç¾¤å€¼å¼‚å¸¸å€¼-noise-and-outliers>å™ªå£°ä¸ç¦»ç¾¤å€¼ï¼ˆå¼‚å¸¸å€¼ï¼‰ Noise and outliers</h3><p>æ ·æœ¬ä»åŒä¸€æ€»ä½“ä¸­æå–æ—¶ï¼Œæ€»ä¼šè¡¨ç°å‡ºä¸€å®šç¨‹åº¦çš„<strong>éšæœºæ€§</strong>ï¼Œå¹¶<strong>åç¦»æ½œåœ¨çš„æ¨¡å¼</strong>ã€‚è¿™ç§åç¦»è¢«ç§°ä¸º<strong>å™ªå£°</strong>ã€‚
Samples extracted from the same population will always exhibit some
level of randomness and deviate from the underlying pattern. Such
deviations are known as noise.</p><p>æœ‰æ—¶ï¼Œæ ·æœ¬å¯èƒ½éå¸¸ä¸åŒï¼Œä»¥è‡³äºæˆ‘ä»¬æ€€ç–‘å…¶åç¦»ä¸ä»…ä»…æ˜¯ç”±äºå™ªå£°ã€‚æˆ‘ä»¬ç§°è¿™äº›æ ·æœ¬ä¸º<strong>å¼‚å¸¸å€¼æˆ–å¼‚å¸¸ç‚¹</strong>ã€‚
Sometimes a sample can be so different that we doubt its deviation is
just due to noise. We call these samples outliers or anomalies.</p><p>å¼‚å¸¸å€¼æ˜¯å±äºå®Œå…¨ä¸åŒæ€»ä½“çš„æ ·æœ¬ã€‚ Outliers are samples that belong to a
different population altogether.</p><p>æ£€æµ‹å¼‚å¸¸å€¼<strong>å¯¹è®¸å¤šåº”ç”¨ï¼ˆä¾‹å¦‚æ¬ºè¯ˆæ£€æµ‹æˆ–ç½‘ç»œå®‰å…¨ï¼‰éå¸¸é‡è¦</strong>ã€‚ Detecting
outliers is important for many applications (for instance, fraud
detection or cybersecurity).</p><p>å¦‚æœåœ¨æ¨¡å‹è®­ç»ƒä¸­ä½¿ç”¨å¼‚å¸¸å€¼ï¼Œå®ƒä»¬ä¹Ÿå¯èƒ½äº§ç”Ÿè´Ÿé¢å½±å“ã€‚ Outliers can also
have a negative impact if used during model training.</p><p>ä¸¤ç±»å¼‚å¸¸å€¼æ–¹æ³•ï¼š</p><ul><li><strong>å¼‚å¸¸æ£€æµ‹ç®—æ³•</strong>æ—¨åœ¨è¯†åˆ«å¼‚å¸¸å€¼ã€‚<br>Anomaly detection algorithms aim at identifying outliers.<ul><li>å¼‚å¸¸å€¼æ£€æµ‹ä¹‹åä¼šæ ¹æ®åº”ç”¨é‡‡å–ç›¸åº”çš„è¡ŒåŠ¨ï¼Œä¾‹å¦‚ç§»é™¤ã€‚<br>Outlier detection is followed by actions that depend on the
application, for instance, removal.\</li></ul></li><li><strong>é²æ£’æ€§</strong>æ˜¯ä¸€ç§è®¾è®¡éœ€æ±‚ï¼Œç”¨äºå‡è½»å¼‚å¸¸å€¼çš„å½±å“ã€‚<br>Robustness is a design requirement that mitigates the impact of
outliers.<ul><li>ä¾‹å¦‚ï¼Œä¸åŒçš„æˆæœ¬å‡½æ•°å¯ä»¥ä»¥ä¸åŒçš„æ–¹å¼è€ƒè™‘å¤§åå·®çš„æˆæœ¬ã€‚<br>For instance, different cost functions can account differently
for the cost associated to large deviations.</li></ul></li></ul><h3 id=åŸºæœ¬å¼‚å¸¸å€¼æ£€æµ‹ç®—æ³•>åŸºæœ¬å¼‚å¸¸å€¼æ£€æµ‹ç®—æ³•</h3><p>å¼‚å¸¸æ£€æµ‹ç®—æ³•çš„ä¸»è¦æ€æƒ³æ˜¯é‡åŒ–è§‚å¯Ÿåˆ°ä¸æ€»ä½“æ¨¡å¼æœ‰ä¸€å®šè·ç¦»çš„æ ·æœ¬çš„æ¦‚ç‡ã€‚å¦‚æœè¿™ä¸ªæ¦‚ç‡å¾ˆä½ï¼Œåˆ™è¯¥æ ·æœ¬ä¸ºå¼‚å¸¸ã€‚
The main idea behind an anomaly detection algorithm is to quantify the
probability of observing samples some distance away from the general
pattern. If this probability is low, the sample is an anomaly.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102181114651.png width=1093 height=291 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102181114651_hu_d5c498a467193fbd.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102181114651_hu_94d148b15686670.png 1024w" loading=lazy alt=image-20250102181114651 class=gallery-image data-flex-grow=375 data-flex-basis=901px></p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102181122138.png width=1087 height=487 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102181122138_hu_be690f572c17af00.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102181122138_hu_ffc7df3bfedad81e.png 1024w" loading=lazy alt=image-20250102181122138 class=gallery-image data-flex-grow=223 data-flex-basis=535px></p><h3 id=ç±»åˆ«å¯†åº¦ä¼°è®¡åˆ†ç±»å™¨>ç±»åˆ«å¯†åº¦ä¼°è®¡åˆ†ç±»å™¨</h3><p>åˆ†ç±»å™¨åº”ç”¨è´å¶æ–¯è§„åˆ™å°†åéªŒæ¦‚ç‡è½¬åŒ–ä¸ºå…ˆéªŒæ¦‚ç‡å’Œç±»åˆ«å¯†åº¦ã€‚ Classifiers
that apply Bayes rule turn posterior probabilities into priors and class
densities.</p><p>ç±»åˆ«å¯†åº¦ p(x|C) æè¿°äº†æ¯ä¸ªç±»åˆ« C åœ¨é¢„æµ‹ç©ºé—´ä¸­æ ·æœ¬çš„åˆ†å¸ƒã€‚ A class
density p(x|C) describes the distribution of samples in the predictor
space for each class C.</p><p>ç±»åˆ«å¯†åº¦é€šè¿‡å¯†åº¦ä¼°è®¡æ–¹æ³•è·å¾—ã€‚ Class densities are obtained using
density estimation methods.</p><p>æˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªç±»åˆ«åˆ†åˆ«æ‹Ÿåˆä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒã€‚ We need to fit a probability
distribution for each class separately.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102181208227.png width=1068 height=439 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102181208227_hu_60b9381185bf1697.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102181208227_hu_a6be40d07107303f.png 1024w" loading=lazy alt=image-20250102181208227 class=gallery-image data-flex-grow=243 data-flex-basis=583px></p><h3 id=åº”ç”¨ç±»åˆ«å¯†åº¦åˆ°æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨>åº”ç”¨ç±»åˆ«å¯†åº¦åˆ°æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨</h3><p>é«˜æ–¯åˆ†å¸ƒæ˜¯ç±»å¯†åº¦æœ€æµè¡Œçš„é€‰æ‹©ã€‚ Gaussian distributions are the most
popular choice for class densities.</p><p>åœ¨é«˜ç»´åœºæ™¯ä¸­ï¼Œå‚æ•°çš„æ€»æ•°éå¸¸å¤§ï¼šå¯¹äºPä¸ªé¢„æµ‹å˜é‡ï¼Œæˆ‘ä»¬æœ‰Pï¼ˆå‡å€¼ï¼‰+
PÂ²ï¼ˆåæ–¹å·®ï¼‰ä¸ªå‚æ•°ã€‚ In high-dimensional scenarios, the total number of
parameters is very large: for P predictors, we have P (mean) + PÂ²
(covariance) parameters.</p><p>æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨åšå‡ºäº†ï¼ˆæœ´ç´ ï¼‰å‡è®¾ï¼Œå³é¢„æµ‹å˜é‡æ˜¯<strong>ç‹¬ç«‹</strong>çš„ï¼Œå› æ­¤ä¸€ä¸ªPç»´é«˜æ–¯åˆ†å¸ƒå¯ä»¥è¡¨ç¤ºä¸ºå®ƒçš„Pä¸ª<strong>è¾¹ç¼˜åˆ†å¸ƒ</strong>çš„ä¹˜ç§¯ã€‚
Naive Bayes classifiers make the (naive) assumption that predictors are
independent, hence a P-dimensional Gaussian distribution can be
expressed as the product of its P marginal distributions.</p><p>ç”±äºè¿™ä¸ªé¢å¤–çš„çº¦æŸï¼Œæˆ‘ä»¬éœ€è¦è·å¾—Pï¼ˆå‡å€¼ï¼‰+
Pï¼ˆæ–¹å·®ï¼‰ä¸ªå‚æ•°ï¼Œè¿™é™ä½äº†è¿‡æ‹Ÿåˆçš„é£é™©ã€‚ As a result of this additional
constraint, we need to obtain P (means) + P (variances) parameters,
which reduces the risk of overfitting.</p><h3 id=åº”ç”¨ç±»åˆ«å¯†åº¦åˆ°èšç±»>åº”ç”¨ç±»åˆ«å¯†åº¦åˆ°èšç±»</h3><p>K-means å¯ä»¥è¢«è§†ä¸º GMM
æ‹Ÿåˆçš„ä¸€ç§ç‰ˆæœ¬ï¼Œå…¶ä¸­é«˜æ–¯åˆ†å¸ƒå…·æœ‰ç›¸åŒçš„å¯¹è§’åæ–¹å·®çŸ©é˜µï¼Œå› æ­¤èšç±»å€¾å‘äºå‘ˆçƒå½¢ã€‚<br>K-means can be seen as a version of GMM fitting, where the Gaussian
distributions have the same diagonal covariance matrix and hence
clusters tend to be spherical.</p><p>GMM å¯ä»¥ä½œä¸ºä¸€ç§èšç±»æ–¹æ³•ï¼Œç”Ÿæˆæ¤­çƒå½¢çš„èšç±»ã€‚é¦–å…ˆæˆ‘ä»¬æ‹Ÿåˆ K
ä¸ªé«˜æ–¯å¯†åº¦ï¼Œç„¶åå°†æ¯ä¸ªæ ·æœ¬åˆ†é…åˆ°æœ€å¯èƒ½çš„å¯†åº¦ã€‚<br>GMM can be used as a clustering method that produces ellipsoidal
clusters. First we fit K Gaussian densities and then we assign each
sample to the most likely density.</p><p><img src=/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102181353614.png width=751 height=370 srcset="/blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102181353614_hu_62a2f60e7ff69e.png 480w, /blog/p/2025-ml-note/assets/%E7%AC%94%E8%AE%B0.assets/image-20250102181353614_hu_de2b26033436786.png 1024w" loading=lazy alt=image-20250102181353614 class=gallery-image data-flex-grow=202 data-flex-basis=487px></p></section><footer class=article-footer><section class=article-tags><a href=/blog/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>æœºå™¨å­¦ä¹ </a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>é™¤éå¦æœ‰å£°æ˜ï¼Œæœ¬ç«™æ‰€æœ‰å†…å®¹ä»¥ CC BY-NC-SA 4.0 åè®®å…±äº«</span></section><section class=article-lastmod><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<span>æœ€åæ›´æ–°äº Jan 29, 2025 00:00 UTC</span></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>ç›¸å…³æ–‡ç« </h2><div class=related-content><div class="flex article-list--tile"><article><a href=/blog/p/ipn-w3/><div class=article-details><h2 class=article-title>äº’è”ç½‘åè®® - Week3æ¦‚å¿µæ•´ç†</h2></div></a></article><article><a href=/blog/p/ipn-w1/><div class=article-details><h2 class=article-title>äº’è”ç½‘åè®® - è¯¾ç¨‹æ¦‚è¦ & Week1æ¦‚å¿µæ•´ç†</h2></div></a></article><article><a href=/blog/p/java-w1/><div class=article-details><h2 class=article-title>JAVAé«˜çº§è¯­è¨€ç¨‹åºè®¾è®¡ - Week1æ¦‚å¿µæ•´ç†</h2></div></a></article><article><a href=/blog/p/db-hw/><div class=article-details><h2 class=article-title>æ•°æ®åº“ - ä½œä¸šé¢˜ç›® & å®éªŒæŠ¥å‘Šæ•´ç†</h2></div></a></article><article><a href=/blog/p/2023-em-question-1/><div class=article-details><h2 class=article-title>ä¼ä¸šç®¡ç† ç¬¬ä¸€é¢˜ è¯¾ä»¶æ•´ç†</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2023 -
2025 KatMelon's Blog</section><section class=powerby>ä½¿ç”¨ <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> æ„å»º<br>ä¸»é¢˜ <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> ç”± <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> è®¾è®¡</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/blog/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>